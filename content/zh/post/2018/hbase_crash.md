+++
title = "è®°ä¸€æ¬¡Hbase å®•æœºåŸå› åˆ†æ"
date = 2018-04-04T19:54:00+08:00
lastmod = 2022-02-24T22:58:14+08:00
tags = ["hbase"]
draft = false
toc = true
+++

## <span class="section-num">1</span> èƒŒæ™¯ {#èƒŒæ™¯}

åœ¨2018å¹´4æœˆ4å·æ—©ä¸Šï¼Œä¸šåŠ¡æ–¹ååº”Hbase è¯»è¶…æ—¶ï¼Œæ— æ³•è¯»å–å½“å‰æ•°æ®ã€‚ç„¶åå‘ç°æµ‹è¯•ç¯å¢ƒçš„
Hbase region server å…¨éƒ¨å®•æœºï¼Œå·²ç»æ— å¯ç”¨Region Server. å› ä¸ºå…¬å¸çš„æœºå™¨çš„Ip å’ŒHost
ä¸ä¾¿åœ¨åšæ–‡å±•ç¤ºï¼Œæ‰€ä»¥æˆ‘ä¼šç”¨ï¼š

```nil
192.168.2.1: node-master
192.168.2.2: node1
192.168.2.3: node2
```

æ¥ä»£æ›¿


## <span class="section-num">2</span> Region Server å®•æœºåŸå› åˆ†æ {#region-server-å®•æœºåŸå› åˆ†æ}

ç»æŸ¥çœ‹æ—¥å¿—ï¼Œå‘ç°ä¸‰å°éƒ¨ç½²äº†Hbase çš„æœåŠ¡å™¨ï¼Œåˆ†åˆ«æ˜¯`node-master 192.168.2.1`, `node1 192.168.2.2`,=node2 192.168.2.3=. node1 æœºå™¨åœ¨2018-03-13 14:47:55 æ”¶åˆ°äº†Shutdown Message, åœäº†Region Server. node-masterè¿™å°æœºå™¨åœ¨2018-03-20
10:13:07æ”¶åˆ°äº†Shutdown Message, åœæ‰äº†Region Server.

ä¹Ÿå°±æ˜¯è¯´åœ¨3æœˆä¸‹æ—¬åˆ°æ˜¨å¤©ï¼ŒHbase ä¸€ç›´åªæœ‰ä¸€å°Region Server åœ¨è¿è¡Œã€‚è€Œåœ¨æ˜¨å¤©ï¼Œ2018-04-03 23:19:35, å‰©ä¸‹çš„æœ€åä¸€å°æœºå™¨ä¹Ÿæ”¶åˆ°äº†Shutdown Message, å› æ­¤æŠŠå‰©ä¸‹çš„æœ€åä¸€å°Region Server åœæ‰ï¼Œæµ‹è¯• ç¯å¢ƒçš„Hbase å…¨éƒ¨ä¸‹çº¿ã€‚é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆè¿™ä¸‰å°æœåŠ¡å™¨ä¼šæ”¶åˆ°Shutdown Message å‘¢ï¼Ÿ


### <span class="section-num">2.1</span> node1 {#node1}

å…ˆä» node1è¿™å°æœºå™¨å¼€å§‹åˆ†æï¼Œå…³äº Region Server é€€å‡ºçš„æ—¥å¿—æ˜¾ç¤ºå¦‚ä¸‹ï¼š

```log
2018-03-13 14:47:49,665 INFO  [main-SendThread(node-master:2181)] zookeeper.ClientCnxn:
Unable to reconnect to ZooKeeper service, session 0x161d6c1ae910001 has expired, closing socket connection
2018-03-13 14:47:49,706 FATAL [main-EventThread] regionserver.HRegionServer: ABORTING region server node1,60020,1519732610839:
regionserver:60020-0x161d6c1ae910001, quorum=node-master:2181,node1:2181,node2:2181,
baseZNode=/hbase regionserver:60020-0x161d6c1ae910001 received expired from ZooKeeper, aborting
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:700)
    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:611)
    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2018-03-13 14:47:49,718 FATAL [main-EventThread] regionserver.HRegionServer: RegionServer abort:
loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
2018-03-13 14:47:50,705 WARN  [DataStreamer for file /hbase-nemo/WALs/node1,60020,1519732610839/node1%2C60020%2C1519732610839.default.1520922158622 block BP-1296874721-192.168.2.1-1519712987003:blk_1073743994_3170] hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException):
No lease on /hbase-nemo/oldWALs/node1%2C60020%2C1519732610839.default.1520922158622 (inode 18837):
File is not open for writing. [Lease.  Holder: DFSClient_NONMAPREDUCE_551822027_1, pendingcreates: 1]
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3612)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode(FSNamesystem.java:3516)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode(NameNodeRpcServer.java:711)
    at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getAdditionalDatanode(AuthorizationProviderProxyClientProtocol.java:229)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolServerSideTranslatorPB.java:508)
    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

    at org.apache.hadoop.ipc.Client.call(Client.java:1471)
    at org.apache.hadoop.ipc.Client.call(Client.java:1408)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
    at com.sun.proxy.$Proxy16.getAdditionalDatanode(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolTranslatorPB.java:429)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
    at com.sun.proxy.$Proxy17.getAdditionalDatanode(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
    at com.sun.proxy.$Proxy18.getAdditionalDatanode(Unknown Source)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1228)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1404)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
2018-03-13 14:47:53,803 FATAL [regionserver/node1/192.168.2.2:60020] regionserver.HRegionServer: ABORTING region server node1,60020,1519732610839: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing node1,60020,1519732610839 as dead server
```

ä» 14:47:49 å¼€å§‹ï¼Œ Hbase æ²¡æ³•å’Œ Zookeeper é€šä¿¡ï¼Œè¿æ¥æ—¶é—´è¶…æ—¶ã€‚ç¿»æŸ¥ Zookeeper çš„æ—¥å¿—ï¼Œå‘ç°Zookeeper çš„æ—¥å¿—æœ‰å¦‚ä¸‹å†…å®¹ï¼š

```log
2018-03-13 14:47:46,926 [myid:1] - INFO  [QuorumPeer[myid=1]/0.0.0.0:2181:ZooKeeperServer@588]
- Invalid session 0x161d6c1ae910002 for client /192.168.2.2:51611, probably expired
2018-03-13 14:47:49,612 [myid:1] - INFO  [QuorumPeer[myid=1]/0.0.0.0:2181:ZooKeeperServer@588]
- Invalid session 0x161d6c1ae910001 for client /192.168.2.2:51612, probably expired
```

è¯´æ˜Hbase å’Œ ZooKeeper çš„é€šä¿¡çš„ç¡®å»äº†é—®é¢˜ã€‚è¿æ¥å‡ºé—®é¢˜ä»¥åï¼Œé›†ç¾¤å°±ä¼šè®¤ä¸º è¿™ä¸ª
Hbase çš„èŠ‚ç‚¹å‡ºäº†æ•…éšœï¼Œå®•æœºï¼Œç„¶åå°±æŠŠè¿™ä¸ªèŠ‚ç‚¹å½“ä½œ `DeadNode`, è¿™ä¸ªèŠ‚ç‚¹çš„
RegionServer å°±ä¸‹çº¿äº†ã€‚


### <span class="section-num">2.2</span> node-master {#node-master}

ç°åœ¨å†æ¥çœ‹çœ‹node-masterè¿™å°æœºå™¨çš„æ—¥å¿—

```log
2018-03-20 10:12:19,986 INFO  [main-SendThread(node-master:2181)] zookeeper.ClientCnxn:
Unable to read additional data from server sessionid 0x361d65049260001, likely server has closed socket, closing socket connection and attempting reconnect
2018-03-20 10:12:20,841 INFO  [main-SendThread(node1:2181)] zookeeper.ClientCnxn:
Opening socket connection to server node1/192.168.2.2:2181. Will not attempt to authenticate using SASL (unknown error)
2018-03-20 10:12:43,747 INFO  [regionserver/node-master/192.168.2.1:60020-SendThread(node1:2181)] zookeeper.ClientCnxn:
Client session timed out, have not heard from server in 60019ms for sessionid 0x161d65049590000, closing socket connection and attempting reconnect
2018-03-20 10:12:44,574 INFO  [regionserver/node-master/192.168.2.1:60020-SendThread(node-master:2181)] zookeeper.ClientCnxn:
Opening socket connection to server node-master/192.168.2.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-03-20 10:12:44,575 INFO  [regionserver/node-master/192.168.2.1:60020-SendThread(node-master:2181)] zookeeper.ClientCnxn:
Socket connection established, initiating session, client: /192.168.2.1:58042, server: node-master/192.168.2.1:2181
2018-03-20 10:12:44,577 INFO  [regionserver/node-master/192.168.2.1:60020-SendThread(node-master:2181)] zookeeper.ClientCnxn:
Session establishment complete on server node-master/192.168.2.1:2181, sessionid = 0x161d65049590000, negotiated timeout = 90000
2018-03-20 10:12:49,625 INFO  [main-SendThread(node1:2181)] zookeeper.ClientCnxn:
Socket connection established, initiating session, client: /192.168.2.1:46815, server: node1/192.168.2.2:2181
2018-03-20 10:12:53,258 WARN  [ResponseProcessor for block BP-1296874721-192.168.2.1-1519712987003:
blk_1073747108_6286] hdfs.DFSClient: Slow ReadProcessor read fields took 70070ms (threshold=30000ms);
ack: seqno: -2 reply: 0 reply: 1 downstreamAckTimeNanos: 0, targets: [DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.2:50010,DS-4eb97418-f0a1-45a7-b335-83f77e4d6a7b,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]
2018-03-20 10:12:53,259 WARN  [ResponseProcessor for block BP-1296874721-192.168.2.1-1519712987003:blk_1073747108_6286] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1296874721-192.168.2.1-1519712987003:blk_1073747108_6286
java.io.IOException: Bad response ERROR for block BP-1296874721-192.168.2.1-1519712987003:blk_1073747108_6286 from datanode DatanodeInfoWithStorage[192.168.2.2:50010,DS-4eb97418-f0a1-45a7-b335-83f77e4d6a7b,DISK]
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:1002)
2018-03-20 10:12:53,259 WARN  [DataStreamer for file /hbase-nemo/WALs/node-master,60020,1519720160721/node-master%2C60020%2C1519720160721.default.1521509628323 block BP-1296874721-192.168.2.1-1519712987003:blk_1073747108_6286] hdfs.DFSClient: Error Recovery for block BP-1296874721-192.168.2.1-1519712987003:blk_1073747108_6286 in pipeline DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.2:50010,DS-4eb97418-f0a1-45a7-b335-83f77e4d6a7b,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]: bad datanode DatanodeInfoWithStorage[192.168.2.2:50010,DS-4eb97418-f0a1-45a7-b335-83f77e4d6a7b,DISK]
2018-03-20 10:12:53,264 WARN  [DataStreamer for file /hbase-nemo/WALs/node-master,60020,1519720160721/node-master%2C60020%2C1519720160721.default.1521509628323 block BP-1296874721-192.168.2.1-1519712987003:blk_1073747108_6286] hdfs.DFSClient: DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]], original=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:1162)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1236)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1404)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
2018-03-20 10:12:53,265 WARN  [sync.4] hdfs.DFSClient: Error while syncing
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]], original=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:1162)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1236)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1404)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
2018-03-20 10:12:53,266 ERROR [sync.4] wal.FSHLog: Error syncing, request close of WAL
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]], original=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:1162)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1236)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1404)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
2018-03-20 10:12:53,266 INFO  [sync.4] wal.FSHLog: Slow sync cost: 474 ms, current pipeline: [DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]
2018-03-20 10:13:05,816 INFO  [regionserver/node-master/192.168.2.1:60020.logRoller] wal.FSHLog: Slow sync cost: 12546 ms, current pipeline: [DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]
2018-03-20 10:13:05,817 ERROR [sync.0] wal.FSHLog: Error syncing, request close of WAL
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]], original=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:1162)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1236)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1404)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
2018-03-20 10:13:05,817 ERROR [regionserver/node-master/192.168.2.1:60020.logRoller] wal.FSHLog: Failed close of WAL writer hdfs://node-master:19000/hbase-nemo/WALs/node-master,60020,1519720160721/node-master%2C60020%2C1519720160721.default.1521509628323, unflushedEntries=1
org.apache.hadoop.hbase.regionserver.wal.FailedSyncBeforeLogCloseException: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]], original=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SafePointZigZagLatch.waitSafePoint(FSHLog.java:1615)
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog.replaceWriter(FSHLog.java:833)
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:699)
    at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:148)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try.
(Nodes: current=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK],
DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]],
original=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK],
DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]).
The current failed datanode replacement policy is DEFAULT, and a client may configure this
via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:1162)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1236)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1404)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
2018-03-20 10:13:05,818 FATAL [regionserver/node-master/192.168.2.1:60020.logRoller]
regionserver.HRegionServer: ABORTING region server node-master,60020,1519720160721: Failed log close in log roller
org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException: hdfs://node-master:19000/hbase-nemo/WALs/node-master,60020,1519720160721/node-master%2C60020%2C1519720160721.default.1521509628323, unflushedEntries=1
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog.replaceWriter(FSHLog.java:882)
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:699)
    at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:148)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hbase.regionserver.wal.FailedSyncBeforeLogCloseException: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]], original=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SafePointZigZagLatch.waitSafePoint(FSHLog.java:1615)
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog.replaceWriter(FSHLog.java:833)
    ... 3 more
Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]], original=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:1162)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1236)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1404)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
2018-03-20 10:13:05,818 FATAL [regionserver/node-master/192.168.2.1:60020.logRoller] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2018-03-20 10:13:05,997 INFO  [regionserver/node-master/192.168.2.1:60020.logRoller] regionserver.HRegionServer: Dump of metrics as JSON on abort:
```

ä»ä¸Šé¢çš„æ—¥å¿—å¯ä»¥çœ‹åˆ° node-masterä¸node1æœºå™¨é€šä¿¡ï¼Œè·å–node1 çš„å“åº”å¤±è´¥ï¼Œè®¤ä¸ºnode1 æ˜¯ bad DataNodeï¼Œæ¥ç€é›†ç¾¤æƒ³è¦æŠŠå‡ºç°é—®é¢˜çš„DataNode ä¸‹æ‰ï¼Œå´å‘ç°æ²¡æœ‰å¤šä½™DataNode æ¥æ›¿æ¢ï¼Œ ç´§æ¥ç€åœ¨Syncing æ—¶å‡ºé”™ï¼Œå…³é—­ WAL å¤±è´¥, æœ€åå°±åœæ‰äº†Region Server. æ¯”è¾ƒå…³é”®çš„æ—¶æœºå¦‚ä¸‹ï¼š

```log
2018-03-20 10:12:53,265 WARN  [sync.4] hdfs.DFSClient: Error while syncing

2018-03-20 10:12:53,266 ERROR [sync.4] wal.FSHLog: Error syncing, request close of WAL

2018-03-20 10:13:05,817 ERROR [sync.0] wal.FSHLog: Error syncing, request close of WAL

2018-03-20 10:13:06,397 ERROR [regionserver/node-master/192.168.2.1:60020] regionserver.HRegionServer: Shutdown / close of WAL failed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]], original=[DatanodeInfoWithStorage[192.168.2.1:50010,DS-84998b22-8294-44ed-90fd-9c1a78d0f558,DISK], DatanodeInfoWithStorage[192.168.2.3:50010,DS-d94668c9-66f4-40f6-b38f-83f14b26c2b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
```

æœŸé—´HDFS åŒæ­¥å‡ºé”™ï¼Œå°è¯•å…³é—­WAL, å¤±è´¥ã€‚å¤±è´¥çš„åŸå› æ˜¯æ— æ³•ç”¨å¥åº·çš„èŠ‚ç‚¹æ›¿æ¢å‡ºäº†é—®é¢˜çš„èŠ‚ç‚¹ï¼Œ åº”è¯¥æ˜¯å¥åº·çš„èŠ‚ç‚¹æ•°å¤ªå°‘äº†ã€‚æœ€ååœ¨å¤šæ¬¡å°è¯•å…³é—­WALéƒ½å› ä¸ºIOException å¤±è´¥ä¹‹åï¼Œ
RegionServer ä¸‹çº¿ã€‚åªæ˜¯ä¸ºä»€ä¹ˆå°è¯•å…³é—­WAL å¤±è´¥éœ€è¦å…³é—­Region Server ä¾ç„¶å­˜ç–‘ã€‚


### <span class="section-num">2.3</span> node2 {#node2}

node2 æ˜¯Hbase é›†ç¾¤æœ€åä¸€å°æœºå™¨ï¼Œå½“node2 å€’ä¸‹äº†ï¼ŒHbase å°±çœŸçš„å®Œå…¨å®•æœºäº†ã€‚

```log
2018-04-03 23:19:33,472 FATAL [regionserver/node2/192.168.2.3:60020.logRoller] regionserver.LogRoller: Aborting
java.io.IOException: cannot get log writer
    at org.apache.hadoop.hbase.wal.DefaultWALProvider.createWriter(DefaultWALProvider.java:365)
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog.createWriterInstance(FSHLog.java:724)
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:689)
    at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:148)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create file/hbase-nemo/WALs/node2,60020,1519732668326/node2%2C60020%2C1519732668326.default.1522768773233. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1418)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2674)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2561)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:593)
    at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.create(AuthorizationProviderProxyClientProtocol.java:111)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:393)
    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

    at org.apache.hadoop.ipc.Client.call(Client.java:1471)
    at org.apache.hadoop.ipc.Client.call(Client.java:1408)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
    at com.sun.proxy.$Proxy16.create(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
    at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
    at com.sun.proxy.$Proxy17.create(Unknown Source)
    at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
    at com.sun.proxy.$Proxy18.create(Unknown Source)
    at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1897)
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1698)
    at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:450)
    at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:446)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.createNonRecursive(DistributedFileSystem.java:446)
    at org.apache.hadoop.fs.FileSystem.createNonRecursive(FileSystem.java:1124)
    at org.apache.hadoop.fs.FileSystem.createNonRecursive(FileSystem.java:1100)
    at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.init(ProtobufLogWriter.java:90)
    at org.apache.hadoop.hbase.wal.DefaultWALProvider.createWriter(DefaultWALProvider.java:361)
    ... 4 more
2018-04-03 23:19:33,501 FATAL [regionserver/node2/192.168.2.3:60020.logRoller] regionserver.HRegionServer: ABORTING region server node2,60020,1519732668326: IOE in log roller
java.io.IOException: cannot get log writer
    at org.apache.hadoop.hbase.wal.DefaultWALProvider.createWriter(DefaultWALProvider.java:365)
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog.createWriterInstance(FSHLog.java:724)
    at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:689)
    at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:148)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create file/hbase-nemo/WALs/node2,60020,1519732668326/node2%2C60020%2C1519732668326.default.1522768773233. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1418)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2674)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2561)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:593)
    at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.create(AuthorizationProviderProxyClientProtocol.java:111)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:393)
    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)

    at org.apache.hadoop.ipc.Client.call(Client.java:1471)
    at org.apache.hadoop.ipc.Client.call(Client.java:1408)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
    at com.sun.proxy.$Proxy16.create(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)
    at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
    at com.sun.proxy.$Proxy17.create(Unknown Source)
    at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
    at com.sun.proxy.$Proxy18.create(Unknown Source)
    at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1897)
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1738)
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1698)
    at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:450)
    at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:446)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.createNonRecursive(DistributedFileSystem.java:446)
    at org.apache.hadoop.fs.FileSystem.createNonRecursive(FileSystem.java:1124)
    at org.apache.hadoop.fs.FileSystem.createNonRecursive(FileSystem.java:1100)
    at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.init(ProtobufLogWriter.java:90)
    at org.apache.hadoop.hbase.wal.DefaultWALProvider.createWriter(DefaultWALProvider.java:361)
    ... 4 more
```

å¯ä»¥çœ‹åˆ°ä¸Šé¢çš„æ—¥å¿—å‡ºç°IO å‡ºç°å¼‚å¸¸ï¼Œæ— æ³•è·å– `log writer`:

```nil
2018-04-03 23:19:33,472 FATAL [regionserver/node2/192.168.2.3:60020.logRoller] regionserver.LogRoller: Aborting
java.io.IOException: cannot get log writer

2018-04-03 23:19:33,501 FATAL [regionserver/node2/192.168.2.3:60020.logRoller] regionserver.HRegionServer: ABORTING region server node2,60020,1519732668326: IOE in log roller
java.io.IOException: cannot get log writer
```

è€Œæ— æ³•è·å– `log writer`, å¯¹æ—¥å¿—è¿›è¡Œå†™å…¥çš„åŸå› æ˜¯ï¼š

```nil
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create file/hbase-nemo/WALs/node2,60020,1519732668326/node2%2C60020%2C1519732668326.default.1522768773233. Name node is in safe mode.
Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
```

NameNode è¿›å…¥äº†safe-mode, å…³äºsafe-mode çš„æè¿°ï¼š
&gt;During start up the NameNode loads the file system state from the fsimage and the edits log file. It then waits for DataNodes to report their blocks so that it does not prematurely start replicating the blocks though enough replicas already exist in the cluster. During this time NameNode stays in Safemode. Safemode for the NameNode is essentially a read-only mode for the HDFS cluster, where it does not allow any modifications to file system or blocks. Normally the NameNode leaves Safemode automatically after the DataNodes have reported that most file system blocks are available. If required, HDFS could be placed in Safemode explicitly using bin/hadoop dfsadmin -safemode command. NameNode front page shows whether Safemode is on or off. A more detailed description and configuration is maintained as JavaDoc for setSafeMode().

NameNode è¿›å…¥safe-mode çš„åŸå› æ˜¯å› ä¸º `node-master`è¿™å°Master æœºå™¨çš„ç£ç›˜è¢«åº”ç”¨æ—¥å¿—æ‰“æ»¡äº†ï¼Œå¯¼ è‡´ NameNode è¿›å…¥äº†åªè¯»çš„ `safe-mode`. å› ä¸ºNameNode è¿›å…¥readonly çš„safe-mode å°±æ—  æ³•å†™å…¥æ—¥å¿—, æ‰€ä»¥ Hbase åœ¨å‡ºç°å¼‚å¸¸ä¹‹åï¼Œå°±å¼€å§‹æŠŠHbase çš„ä¿¡æ¯ dump å‡ºæ¥ï¼Œå¹¶å…³é—­
Region Server, å¯¼è‡´æ•´ä¸ªHbase é›†ç¾¤å®•æœºã€‚

å¯¹äº`node2` Region Server ä¸‹çº¿çš„åŸå› ï¼ŒçŒœæµ‹æ˜¯ NameNode æœåŠ¡å™¨çš„ç£ç›˜ç”¨å®Œï¼Œå¯¼è‡´NameNode è¿›å…¥read-only çš„safe-mode, åˆå› ä¸ºHbase å­˜å‚¨çš„æ ¸å¿ƒä¹‹ä¸€æ˜¯WAL(write-ahead-log, é¢„å†™æ—¥å¿—),è¾ƒé•¿æ—¶é—´æ— æ³•å†™å…¥æ—¥å¿—ï¼Œæœ€ç»ˆå¯¼è‡´ Region Server ä¸‹çº¿ã€‚


## <span class="section-num">3</span> åˆ†æå°ç»“ {#åˆ†æå°ç»“}

ç»è¿‡è¿™æ ·çš„ä¸€ç¿»æ’æŸ¥ï¼Œå¯ä»¥å¾—å‡ºç»“è®ºï¼Œæœ€å¼€å§‹ `node1` å› ä¸ºHbase å’Œ ZooKeeper çš„é€šä¿¡å‡ºç°é—®é¢˜ï¼Œ è¢«è®¤ä¸ºæ˜¯é—®é¢˜èŠ‚ç‚¹ï¼Œä¸‹çº¿äº†Region Serverï¼›

ä¸€ä¸ªæ˜ŸæœŸä¹‹åï¼Œ`node-master`è¿™å°æœºå™¨åœ¨åŒæ­¥çš„æ—¶å€™ å‡ºç°é—®é¢˜ï¼Œæƒ³è¦å…³é—­WAL, ä½†æ˜¯å´å› ä¸ºæ²¡æœ‰å……è¶³çš„å¥åº·èŠ‚ç‚¹æ¥æ›¿æ¢å‡ºç°é—®é¢˜çš„`node1`, å¯¼è‡´å…³é—­ WAL å¤±è´¥ï¼Œä¹Ÿä¸‹çº¿äº†Region Server. `node2` è¿™å°æœºå™¨å› ä¸ºä½œä¸º NameNode çš„`node-master`æœåŠ¡å™¨çš„ç£ç›˜ç”¨ å®Œï¼Œå¯¼è‡´NameNode è¿›å…¥read-only çš„safe-mode, åˆå› ä¸ºHbase å­˜å‚¨çš„æ ¸å¿ƒä¹‹ä¸€æ˜¯
WAL(write-ahead-log, é¢„å†™æ—¥å¿—),è¾ƒé•¿æ—¶é—´æ— æ³•å†™å…¥æ—¥å¿—ï¼Œæœ€ç»ˆå¯¼è‡´ Region Server ä¸‹çº¿ã€‚


## <span class="section-num">4</span> å…¶ä»– {#å…¶ä»–}

è¿˜æœ‰ä¸€ä¸ªå…³é”®ç‚¹æ˜¯ä¸ºä»€ä¹ˆHbase å’ŒZookeeper çš„è¿æ¥è¶…æ—¶ï¼ŒZookeeper çš„æ—¥å¿—åªæ˜¯ç®€å•åœ°è¯´æ˜ï¼š

```nil
2018-03-13 14:47:46,926 [myid:1] - INFO  [QuorumPeer[myid=1]/0.0.0.0:2181:ZooKeeperServer@588] - Invalid session 0x161d6c1ae910002 for client /192.168.2.2:51611, probably expired
2018-03-13 14:47:49,612 [myid:1] - INFO  [QuorumPeer[myid=1]/0.0.0.0:2181:ZooKeeperServer@588] - Invalid session 0x161d6c1ae910001 for client /192.168.2.2:51612, probably expired
```

ä¸ºä»€ä¹ˆ session ä¼šæ— æ•ˆï¼Œæ—¥å¿—å¹¶æ²¡æœ‰ç»™å‡ºè¯´æ˜ï¼Œä¸ªäººçŒœæµ‹å¯èƒ½æ˜¯å› ä¸ºåœ¨éƒ¨ç½²äº† Hbase/Zookeeper çš„æœåŠ¡å™¨ä¸Šè¿˜éƒ¨ç½²äº†åº”ç”¨ã€‚

åº”ç”¨æˆ–è€…æ˜¯Hbase å¯¼è‡´çš„é•¿GC å¯¼è‡´ZooKeeper åœé¡¿ï¼Œå¹¶ä¸”å¯¼è‡´session è¶…æ—¶æ— æ•ˆã€‚


## <span class="section-num">5</span> ç»“è¯­ {#ç»“è¯­}

å’ŒåŒäº‹äº¤æµä¹‹åï¼Œè§‰å¾—ä»¥ä¸Šçš„åˆ†æåªæ˜¯åŸºäºæ—¥å¿—çš„çŒœæµ‹ï¼Œå¯èƒ½Hbase å®•æœºçš„åŸå› æ­£å¦‚æˆ‘æ‰€è¯´ï¼Œ æˆ–è€…å¦æœ‰åŸå› ï¼Œæ‰€ä»¥ç°åœ¨æœ€å…³é”®çš„æªæ–½æ˜¯åŠ ä¸Šå¯¹Hbase çš„å„ç§ç›‘æ§ã€‚

åœ¨Hbase å®•æœºçš„æ—¶å€™ï¼Œ å‚è€ƒæ—¥å¿—å’Œè¯¦ç»†çš„ç›‘æ§ï¼Œæ¯”å¦‚è¿æ¥æ•°ï¼ŒCPU ä½¿ç”¨ç‡ï¼Œå†…å­˜ï¼Œé›†ç¾¤è´Ÿè½½æƒ…å†µï¼Œæ¯ä¸ªèŠ‚ç‚¹æƒ…å†µã€‚ä¸ç„¶å†é‡åˆ°ä¸€æ¬¡å®•æœºï¼Œè¿˜æ˜¯åªèƒ½çœ‹æ—¥å¿—ï¼ŒçŒœåŸå› ã€‚

è¯åˆ†ä¸¤å¤´ï¼Œç°åœ¨çš„åˆ†æä¸»è¦æ˜¯åŸºäºHbase å’ŒZooKeeper çš„æ—¥å¿—è¿›è¡Œåˆ†æï¼Œç®€è€Œè¨€ä¹‹å°±æ˜¯ææ—¥ å¿—ï¼ŒæŸ¥çœ‹ä¿¡æ¯; ææ—¥å¿—ï¼ŒæŸ¥çœ‹ä¿¡æ¯ï¼›é€šè¿‡å·¥å…·æ‰¾å‡ºæ—¥å¿—ä¸­éšè—çš„å…³é”®æ—¶æœºï¼Œç„¶åå¯¹æ—¶æœºå‰åå‘ç”Ÿçš„äº‹æƒ…è¿›è¡Œåˆ†æï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªæœ‰è¶£çš„è¿‡ç¨‹ã€‚

åªæ˜¯ä»1G å¤šçš„æ—¥å¿—é‡Œé¢æ‰¾å‡ºæƒ³è¦çš„å†…å®¹ï¼Œä¹Ÿä¸æ˜¯ä¸€ä¸ªå®¹æ˜“çš„è¿‡ç¨‹ã€‚

<div center class="qr-container">
<img src="/ox-hugo/qrcode_gh_e06d750e626f_1.jpg" alt="qrcode_gh_e06d750e626f_1.jpg" width="160px" height="160px" center="t" class="qr-container" />
å…¬å·åŒæ­¥æ›´æ–°ï¼Œæ¬¢è¿å…³æ³¨ğŸ‘»
</div>

