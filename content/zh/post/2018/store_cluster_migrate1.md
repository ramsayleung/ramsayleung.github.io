+++
title = "è®°å­˜å‚¨é›†ç¾¤çš„ä¸€æ¬¡è¿ç§»è¿‡ç¨‹ï¼ˆä¸Šï¼‰"
date = 2018-03-05T17:39:00+08:00
lastmod = 2022-02-24T22:25:27+08:00
tags = ["mysql", "hbase"]
categories = ["hbase"]
draft = false
toc = true
showQuote = true
+++

æ­å»ºå’Œé…ç½® Hadoop, Zookeeper, Hbase


## <span class="section-num">1</span> å‰è¨€ {#å‰è¨€}

æœ€è¿‘è´Ÿè´£å…¬å¸æµ‹è¯•ç¯å¢ƒçš„è¿ç§»ï¼Œä¸»è¦åŒ…æ‹¬ Hbase+Mysql å­˜å‚¨é›†ç¾¤çš„è¿ç§»ï¼Œæ¶ˆæ¯é˜Ÿåˆ—ï¼Œç¼“å­˜ç»„ä»¶çš„è¿ç§», è€Œæˆ‘æ‰“ç®—è¯´è¯´å­˜å‚¨é›†ç¾¤çš„è¿ç§»ã€‚å› ä¸ºå…¬å¸çš„æœºå™¨çš„Ip å’ŒHost ä¸ä¾¿åœ¨åšæ–‡å±•ç¤ºï¼Œæ‰€ä»¥æˆ‘ä¼šç”¨ï¼š

```cfg
192.168.2.1: node-master
192.168.2.2: node1
192.168.2.3: node2
```

æ¥ä»£æ›¿å…¬å¸çš„æœºå™¨å’ŒåŸŸåã€‚


## <span class="section-num">2</span> æ­å»ºæ–°ç¯å¢ƒ {#æ­å»ºæ–°ç¯å¢ƒ}


### <span class="section-num">2.1</span> Hadoop æ­å»ºæµç¨‹ {#hadoop-æ­å»ºæµç¨‹}


#### <span class="section-num">2.1.1</span> Hadoop é›†ç¾¤çš„æ¶æ„ {#hadoop-é›†ç¾¤çš„æ¶æ„}

åœ¨é…ç½® Hadoop çš„ä¸»ä»èŠ‚ç‚¹(master/slave)ä¹‹å‰ï¼Œå…ˆæ¥äº†è§£ä¸€ä¸‹ Hadoop é›†ç¾¤çš„ç»„ä»¶ä½œç”¨ï¼›

-   `master` èŠ‚ç‚¹è´Ÿè´£æ‹…ä»»ç®¡ç†åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä»¥åŠè¿›è¡Œç›¸åº”çš„èµ„æºè°ƒåº¦çš„è§’è‰²:
    -   NameNode: ç®¡ç†åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿå¹¶ä¸”æ„ŸçŸ¥æ•°æ®å—åœ¨é›†ç¾¤çš„å­˜å‚¨ä½ç½®
    -   ResourceManager: ç®¡ç† `YARN` ä»»åŠ¡ï¼Œå¹¶ä¸”è´Ÿè´£åœ¨ `slave` èŠ‚ç‚¹è°ƒåº¦å’Œå¤„ç†
-   `slave` èŠ‚ç‚¹è´Ÿè´£æ‹…ä»»å­˜å‚¨çœŸå®çš„æ•°æ®å¹¶ä¸”æä¾›è¿ç®— `YARN` ä»»åŠ¡çš„èƒ½åŠ›çš„è§’è‰²ï¼š
    -   DataNode: è´Ÿè´£ç‰©ç†å­˜å‚¨çœŸå®çš„æ•°æ®
    -   NodeManager: ç®¡ç†åœ¨è¯¥èŠ‚ç‚¹ `YARN` ä»»åŠ¡çš„å…·ä½“æ‰§è¡Œã€‚

`master` å’Œ `slave` çš„è§’è‰²ä¸ä¸€å®šåƒä¸Šé¢åˆ’åˆ†å¾—æ³¾æ¸­åˆ†æ˜ï¼Œæ¯”å¦‚ `master` èŠ‚ç‚¹ä¹Ÿå¯ä»¥æ˜¯ `dataNode`,è¿™ä¸ªå°±çœ‹å…·ä½“é…ç½®äº†ã€‚


#### <span class="section-num">2.1.2</span> é…ç½®JDK {#é…ç½®jdk}

Hadoop é›†ç¾¤éœ€è¦JAVA ç¯å¢ƒï¼Œè€ŒLinux çš„å‘è¡Œç‰ˆæœ¬ä¸€èˆ¬éƒ½æ˜¯é»˜è®¤å¸¦æœ‰JDK çš„ï¼Œåªæ˜¯OpenJDK è€Œä¸æ˜¯ Oracle JDK, å¦‚æœéœ€è¦ä¿®æ”¹JDK çš„ç‰ˆæœ¬ï¼Œå¯ä»¥è‡ªè¡Œä¿®æ”¹ï¼Œç½‘ä¸Šå·²ç»æœ‰å¾ˆå¤šå®‰è£…JDK çš„æ•™ç¨‹ï¼Œæˆ‘å°±ä¸ä¸€ä¸€è®²è§£äº†ã€‚


#### <span class="section-num">2.1.3</span> ä¿®æ”¹host {#ä¿®æ”¹host}

å› ä¸ºéœ€è¦ä¸åŒçš„æœºå™¨ä¹‹é—´é€šä¿¡ï¼Œæ‰€ä»¥éœ€è¦å…ˆé…ç½®å¥½Ip å’ŒåŸŸåçš„æ˜ å°„ã€‚ä¿®æ”¹æ¯å°æœºå™¨çš„ `/etc/hosts` æ–‡ä»¶ï¼ŒåŠ ä¸Šä»¥ä¸‹å†…å®¹ï¼š

```nil
192.168.2.1: nodw-master
192.168.2.2: node1
192.168.2.3: node2
```


#### <span class="section-num">2.1.4</span> æ–°å»º hadoop ç”¨æˆ· {#æ–°å»º-hadoop-ç”¨æˆ·}

è™½è¯´æˆ‘å¯ä»¥ç”¨æˆ‘è‡ªå·±çš„ç™»å½•åæ¥é…ç½®å’Œè¿è¡Œ hadoop, ä½†æ˜¯å‡ºäºå®‰å…¨çš„è€ƒè™‘ï¼Œè¿˜æ˜¯åœ¨æ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨æ¥è¿è¡Œ hadoop é›†ç¾¤çš„ç”¨æˆ·æ¯”è¾ƒå¥½ã€‚

```shell
useradd hadoop
passwd hadoop
```


#### <span class="section-num">2.1.5</span> SSH å…å¯†ç ç™»å½• {#ssh-å…å¯†ç ç™»å½•}

å› ä¸ºåœ¨ Hadoop é›†ç¾¤ä¸­ï¼Œ `node-master` èŠ‚ç‚¹ä¼šé€šè¿‡SSHè¿æ¥å’Œå…¶ä»–èŠ‚ç‚¹è¿›è¡Œé€šä¿¡ï¼Œæ‰€ä»¥éœ€è¦ä¸º
Hadoop é›†ç¾¤é…ç½®å…å¯†ç æ ¡éªŒçš„é€šä¿¡ã€‚é¦–å…ˆä»¥ `hadoop` ç”¨æˆ·èº«ä»½ç™»å½•åˆ° `node-master` èŠ‚ç‚¹ï¼Œç„¶åç”Ÿæˆ SSHçš„å…¬ç§é’¥ï¼š

```shell
ssh-keygen -b 4096
```

ç„¶åæŠŠå…¬é’¥å¤åˆ¶åˆ°å…¶ä»–çš„èŠ‚ç‚¹ï¼Œå¦‚æœä½ æƒ³è¦æŠŠ `node-master`ä¹Ÿå½“ä½œ`dataNote`çš„è¯ï¼Œå°±éœ€è¦æŠŠå…¬é’¥ä¹Ÿå¤åˆ¶åˆ°
`master`èŠ‚ç‚¹ï¼š

```nil
ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@node-master
ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@node1
ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@node2
```

è°¨è®°ï¼šå¤åˆ¶çš„æ˜¯â€å…¬é’¥â€,ä¸æ˜¯â€ç§é’¥â€.


#### <span class="section-num">2.1.6</span> å®‰è£…hadoop 1. ä¸‹è½½hadoop å®‰è£…åŒ…ï¼Œä»¥ `hadoop`ç™»å½•`node-master`ï¼Œé‡‡ç”¨ wgetå‘½ä»¤ä¸‹è½½ï¼š {#å®‰è£…hadoop-1-dot-ä¸‹è½½hadoop-å®‰è£…åŒ…-ä»¥-hadoop-ç™»å½•-node-master-é‡‡ç”¨-wgetå‘½ä»¤ä¸‹è½½}

wget <http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.7.1.tar.gz>

1.  åˆ›å»ºä¸€ä¸ªhadoopç›®å½•ï¼Œå°†å„ä¸ªç»„ä»¶éƒ½å®‰è£…åœ¨è¿™ä¸ªç›®å½•ã€‚

<!--listend-->

```nil
mdkir ~/hadoop
tar -zxvf hadoop-2.6.0-cdh5.7.1.tar.gz -C ~/hadoop
```


#### <span class="section-num">2.1.7</span> ä¿®æ”¹é…ç½®æ–‡ä»¶ {#ä¿®æ”¹é…ç½®æ–‡ä»¶}

æ‰€æœ‰ä¿®æ”¹çš„ hadoopé…ç½®æ–‡ä»¶éƒ½ä½äº ~/hadoop/etc/hadoop/ ç›®å½•

```nil
cd ~/hadoop/etc/hadoop
```

<!--list-separator-->

1.  è®¾ç½® NameNode ä½ç½®

    `vim core-site.xml`: ä¿®æ”¹æˆä»¥ä¸‹å†…å®¹ï¼š

    ```xml
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node-master:19000</value>
      </property>
      <property>
        <name>fs.trash.interval</name>
        <value>10080</value>
      </property>
      <property>
        <name>fs.trash.checkpoint.interval</name>
        <value>10080</value>
      </property>
    </configuration>
    ```

<!--list-separator-->

2.  è®¾ç½® HDFS è·¯å¾„

    `vim hdfs-site.xml`,ä¿®æ”¹å†…å®¹ä¸ºï¼š

    ```xml
    <configuration>
      <property>
        <name>dfs.replication</name>
        <value>1</value>
      </property>
      <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/hadoop/data/temp</value>
      </property>
      <property>
        <name>dfs.namenode.http-address</name>
        <value>node-master:50070</value>
      </property>
      <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node1:50090</value>
      </property>
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.data.dir</name>
        <value>/home/hadoop/hadoop/data/hdfs</value>
      </property>
    </configuration>
    ```

<!--list-separator-->

3.  å°† YARN è®¾ç½®æˆä»»åŠ¡è°ƒåº¦å™¨(Job Scheduler)

    ```shell
    cp mapred-site.xml.template mapred-site.xml
    ```

    ç„¶åä¿®æ”¹é…ç½®ï¼Œå°† `yarn` è®¾ç½®æˆ `MapReduce` æ“ä½œçš„é»˜è®¤æ¡†æ¶ï¼š
    `vim mapred-site.xml`:

    ```xml
    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.address</name>
        <value>node-master:10020</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>node-master:19888</value>
      </property>
    </configuration>
    ```

<!--list-separator-->

4.  é…ç½® YARN

    `vim yarn-site.xml`:

    ```xml
    <configuration>
      <property>
        <name>yarn.acl.enable</name>
        <value>0</value>
      </property>

      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>node-master</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>
    </configuration>
    ```

<!--list-separator-->

5.  é…ç½® master èŠ‚ç‚¹

    å› ä¸ºå…¬å¸çš„æœºå™¨å†…å­˜è¾ƒå¤§ï¼Œå…¼ä¹‹æœºå™¨æ•°ä¸å¤šï¼Œæ‰€ä»¥æˆ‘å°±æŠŠä¸¤å°æœºå™¨éƒ½éƒ½å½“ä½œ `master`:
    `vim masters`ä¿®æ”¹æ–‡ä»¶ä¸ºï¼š

    ```cfg
    node-master
    node1
    ```

<!--list-separator-->

6.  é…ç½® slave èŠ‚ç‚¹

    æŠŠå…¨éƒ¨èŠ‚ç‚¹éƒ½å½“ä½œæ•°æ®å‡ ç‚¹(dataNode):
    `vim slaves`:

    ```cfg
    node-master
    node1
    node2
    ```

<!--list-separator-->

7.  ä¿®æ”¹ Hadoop ç¯å¢ƒå˜é‡é…ç½®

    è¿™ä¸ªé…ç½®æ˜¯å¦ä¿®æ”¹å°±è§†æƒ…å†µè€Œå®šäº†ã€‚
    `vim hadoop-env.sh`:

    ```sh
    #å› ä¸ºsshçš„ç«¯å£ä¸æ˜¯é»˜è®¤çš„22,éœ€è¦é‡æ–°æŒ‡å®š
    export HADOOP_SSH_OPTS="-p 9922"
    #å¦‚æœæŠ¥é”™java_homeæ‰¾ä¸åˆ°ï¼Œå¯åœ¨è¿™é‡Œé‡æ–°æŒ‡å®š
    #export JAVA_HOME=/usr/java/jdk1.8.0_161/
    ```

<!--list-separator-->

8.  åœ¨å…¶å®ƒçš„èŠ‚ç‚¹å®‰è£…å¹¶ä¸”è§£å‹ï¼Œä¸è¦ä¿®æ”¹é…ç½®æ–‡ä»¶

<!--list-separator-->

9.  å°†é…ç½®æ–‡ä»¶åŒæ­¥åˆ°slaveä¸»æœº

    ```shell
    scp -r -P 9922 /home/hadoop/hadoop/hadoop-2.6.0-cdh5.7.1/etc/hadoop/  node1:/home/hadoop/hadoop/hadoop-2.6.0-cdh5.7.1/etc
    scp -r -P 9922 /home/hadoop/hadoop/hadoop-2.6.0-cdh5.7.1/etc/hadoop/  node2:/home/hadoop/hadoop/hadoop-2.6.0-cdh5.7.1/etc
    ```

<!--list-separator-->

10.  ä¿®æ”¹ç¯å¢ƒå˜é‡ï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½è¦é…ç½®ï¼‰

    ç¼–è¾‘ `.bash_profile`ï¼š
    `vim ~/.bash_profile`
    åŠ å…¥ï¼š

    ```shell
    export JAVA_HOME=/usr/java/jdk1.8.0_161/
    export JRE_HOME=/usr/java/jdk1.8.0_161/jre
    export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH
    export HADOOP_HOME=/home/hadoop/hadoop/hadoop-2.6.0-cdh5.7.1
    export HBASE_HOME=/home/hadoop/hadoop/hbase-1.2.0-cdh5.7.1
    export HADOOP_MAPRED_HOME=${HADOOP_HOME}
    export HADOOP_COMMON_HOME=${HADOOP_HOME}
    export HADOOP_HDFS_HOME=${HADOOP_HOME}
    export YARN_HOME=${HADOOP_HOME}
    export HADOOP_YARN_HOME=${HADOOP_HOME}
    export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
    export HDFS_CONF_DIR=${HADOOP_HOME}/etc/hadoop
    export YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop
    PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$HADOOP_HOME/bin
    export PATH
    ```

    ç„¶ååŠ è½½ `~/.bash_profile`:
    `source ~/.bash_profile`

<!--list-separator-->

11.  æ ¼å¼åŒ– HDFS

    å°±åƒå…¶å®ƒçš„æ–‡ä»¶ç³»ç»Ÿé‚£æ ·ï¼Œåœ¨ä½¿ç”¨ä¹‹å‰éœ€è¦æ ¼å¼åŒ–ï¼ŒHDFS è¿™ä¸ªåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä¹Ÿä¸ä¾‹å¤–ã€‚åœ¨ `node-master`,è¿è¡Œï¼š

    ```nil
    hdfs namenode -format
    ```

    é‚£ä¹ˆï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œ Hadoop å°±å·²ç»å®‰è£…å’Œé…ç½®å¥½äº†ã€‚


#### <span class="section-num">2.1.8</span> è¿è¡Œå’Œç›‘æ§ HDFS {#è¿è¡Œå’Œç›‘æ§-hdfs}

<!--list-separator-->

1.  å¯åŠ¨HDFS

    åœ¨ `node-master` çš„ `/home/hadoop/hadoop/sbin/` ç›®å½•è¿è¡Œä¸‹é¢çš„å‘½ä»¤ä»¥å¯åŠ¨ HDFS:

    ```nil
    ./start-dfs.sh
    ```

    ç„¶åå°±ä¼šå¯åŠ¨ `NameNode` å’Œ `SecondaryNameNode`, ç„¶åç»§ç»­å¯åŠ¨ `DataNode`.

<!--list-separator-->

2.  éªŒè¯HDFS

    å¯ä»¥åœ¨å„ä¸ªèŠ‚ç‚¹é€šè¿‡ `jps` æ£€æŸ¥HDFS çš„è¿è¡ŒçŠ¶æ€ã€‚æ¯”å¦‚åœ¨ `node-master` è¿è¡Œ `jps`:

    ```cfg
    12243 NameNode
    2677 ResourceManager
    19593 Jps
    15036 DataNode
    ```

    `node1`:

    ```cfg
    30464 DataNode
    13094 Jps
    28589 SecondaryNameNode
    ```

<!--list-separator-->

3.  åœæ­¢HDFS

    åœ¨ `node-master` çš„ `/home/hadoop/hadoop/sbin/` ç›®å½•è¿è¡Œä¸‹é¢çš„å‘½ä»¤ä»¥åœæ­¢HDFS:

    ```nil
    ./stop-dfs.sh
    ```

<!--list-separator-->

4.  ç›‘æ§HDFS

    å¦‚æœä½ åœ¨å¯åŠ¨ HDFS ä¹‹åï¼Œæƒ³è¦è·å–å…³äº HDFS çš„è¯¦ç»†ä¿¡æ¯ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hdfs dfsadmin -report` å‘½ä»¤ï¼Œ ä¾‹å¦‚åœ¨ `node-master` è¿è¡Œ `hdfs dfsadmin -resport`, è¾“å‡ºå¦‚ä¸‹ï¼š

    ```cfg
    Configured Capacity: 1201169780736 (1.09 TB)
    Present Capacity: 1129442681745 (1.03 TB)
    DFS Remaining: 1129442358161 (1.03 TB)
    DFS Used: 323584 (316 KB)
    DFS Used%: 0.00%
    Under replicated blocks: 0
    Blocks with corrupt replicas: 0
    Missing blocks: 0
    Missing blocks (with replication factor 1): 0

    -------------------------------------------------
    Live datanodes (3):

    Name: 192.168.2.3:50010 (node2)
    Hostname: node2
    Decommission Status : Normal
    Configured Capacity: 400389926912 (372.89 GB)
    DFS Used: 102400 (100 KB)
    Non DFS Used: 24759164513 (23.06 GB)
    DFS Remaining: 375630659999 (349.83 GB)
    DFS Used%: 0.00%
    DFS Remaining%: 93.82%
    Configured Cache Capacity: 0 (0 B)
    Cache Used: 0 (0 B)
    Cache Remaining: 0 (0 B)
    Cache Used%: 100.00%
    Cache Remaining%: 0.00%
    Xceivers: 11
    Last contact: Mon Mar 05 14:05:38 CST 2018


    Name: 192.168.2.2:50010 (node1)
    Hostname: node1
    Decommission Status : Normal
    Configured Capacity: 400389926912 (372.89 GB)
    DFS Used: 77824 (76 KB)
    Non DFS Used: 23483977479 (21.87 GB)
    DFS Remaining: 376905871609 (351.02 GB)
    DFS Used%: 0.00%
    DFS Remaining%: 94.13%
    Configured Cache Capacity: 0 (0 B)
    Cache Used: 0 (0 B)
    Cache Remaining: 0 (0 B)
    Cache Used%: 100.00%
    Cache Remaining%: 0.00%
    Xceivers: 7
    Last contact: Mon Mar 05 14:05:38 CST 2018


    Name: 192.168.2.1:50010 (node-master)
    Hostname: node-master
    Decommission Status : Normal
    Configured Capacity: 400389926912 (372.89 GB)
    DFS Used: 143360 (140 KB)
    Non DFS Used: 23483956999 (21.87 GB)
    DFS Remaining: 376905826553 (351.02 GB)
    DFS Used%: 0.00%
    DFS Remaining%: 94.13%
    Configured Cache Capacity: 0 (0 B)
    Cache Used: 0 (0 B)
    Cache Remaining: 0 (0 B)
    Cache Used%: 100.00%
    Cache Remaining%: 0.00%
    Xceivers: 7
    Last contact: Mon Mar 05 14:05:39 CST 2018
    ```

    æˆ–è€…å¯ä»¥ä½¿ç”¨æ›´åŠ å‹å¥½çš„ Web ç®¡ç†ç•Œé¢ï¼Œåœ¨æµè§ˆå™¨è¾“å…¥ï¼š <http://node-master-ip:50070>, ç„¶åä½ å°±å¯ä»¥çœ‹åˆ°å¦‚ä¸‹çš„ç›‘æ§ç•Œé¢ï¼š
    ![](https://imgur.com/jmIXxRy.jpg)

<!--list-separator-->

5.  ä½¿ç”¨ HDFS

    æ—¢ç„¶ HDFS å¯ä»¥è·‘èµ·æ¥äº†ï¼Œç°åœ¨å°±éœ€è¦æ·»åŠ ä¸€ç‚¹æ•°æ®ä»¥æµ‹è¯• HDFS äº†ã€‚ åœ¨HDFS çš„æ ¹ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ª `test` ç›®å½•ï¼š

    ```sh
    hdfs dfs -mkdir /test
    ```

    ç„¶ååœ¨æœ¬åœ°åˆ›å»ºä¸€ä¸ª `helloworld` æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

    ```sh
    Bye world!
    ```

    æ¥ç€æŠŠ `helloworld` æ–‡ä»¶æ”¾ç½®åˆ°HDFSçš„ `/test` ç›®å½•ä¸‹ï¼š

    ```sh
    hdfs dfs -put helloworld /test
    ```

    æœ€åæŸ¥çœ‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼š

    ```shell
    hdfs dfs -ls /test
    ```


### <span class="section-num">2.2</span> å°ç»“ {#å°ç»“}

è‡³æ­¤ï¼Œå¦‚æœä¸€åˆ‡é¡ºåˆ©çš„è¯ï¼Œ é‚£ä¹ˆHadoop é›†ç¾¤å°±è¿è¡Œèµ·æ¥äº†ã€‚å› ä¸ºæˆ‘æ˜¯éœ€è¦ç”¨Hbase ä½œå­˜å‚¨é›†ç¾¤ï¼Œæš‚ä¸éœ€ç”¨`Yarn` ä½œè®¡ç®—ï¼Œæ‰€ä»¥æˆ‘å°±æ²¡æœ‰ä»‹ç»å¯åŠ¨ `Yarn` çš„ æµç¨‹äº†ã€‚


### <span class="section-num">2.3</span> ZooKeeper æ­å»ºæµç¨‹ {#zookeeper-æ­å»ºæµç¨‹}

å› ä¸ºéœ€è¦ç”¨ ZooKeeper æ¥ç®¡ç†é›†ç¾¤ï¼Œæ‰€ä»¥ä¹Ÿéœ€è¦å®‰è£… ZooKeeper. è€Œ ZooKeeper çš„å®‰è£…å’Œ é…ç½®ä¹Ÿæ˜¯ç”¨ `hadoop` ç”¨æˆ·è¿›è¡Œæ“ä½œçš„ã€‚


#### <span class="section-num">2.3.1</span> å®‰è£…zookeeper (æ¯ä¸ªèŠ‚ç‚¹åŒæ ·çš„æ“ä½œ) 1. ä¸‹è½½zookeeperå®‰è£…åŒ…ï¼Œç™»å½•ä¸»æœºï¼Œé‡‡ç”¨wgetå‘½ä»¤ä¸‹è½½ï¼š {#å®‰è£…zookeeper--æ¯ä¸ªèŠ‚ç‚¹åŒæ ·çš„æ“ä½œ--1-dot-ä¸‹è½½zookeeperå®‰è£…åŒ…-ç™»å½•ä¸»æœº-é‡‡ç”¨wgetå‘½ä»¤ä¸‹è½½}

wget <http://archive.cloudera.com/cdh5/cdh/5/zookeeper-3.4.5-cdh5.7.1.tar.gz>

1.  è§£å‹å®‰è£…åˆ°hadoopç›®å½•ï¼Œå°†å„ä¸ªç»„ä»¶éƒ½å®‰è£…åœ¨è¿™ä¸ªç›®å½•ã€‚

<!--listend-->

```shell
tar -zxvf zookeeper-3.4.5-cdh5.7.1.tar.gz -C ~/hadoop
```


#### <span class="section-num">2.3.2</span> é…ç½® ZooKeeper {#é…ç½®-zookeeper}

ä¿®æ”¹zoo.cfg (æ‰€æœ‰æœºå™¨ä¸€æ ·çš„é…ç½®): `vim /home/hadoop/hadoop/zookeeper-3.4.5-cdh5.7.1/conf/zoo.cfg`, é…ç½®æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š

```cfg
# The number of milliseconds of each tick
tickTime=2000
maxSessionTimeout=300000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just
# example sakes.
dataDir=/home/hadoop/hadoop/zookeeper-3.4.5-cdh5.7.1/data
# the port at which the clients will connect
clientPort=2181
#
# Be sure to read the maintenance section of the
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
server.1=node-master:2888:3888
server.2=node1:2888:3888
server.3=node2:2888:3888
```

<!--list-separator-->

1.  åˆ›å»ºmyidæ–‡ä»¶ ï¼ˆä¸åŒä¸»æœºä¸åŒæ•°å­—ï¼‰

    ```sh
    cd {data_dir} # æŒ‰ç…§ä¸Šé¢çš„é…ç½®ï¼Œå°±åº”è¯¥æ˜¯/home/hadoop/hadoop/zookeeper-3.4.5-cdh5.7.1/data
    vim myid
    ```

    myidçš„æ•°å­—è¦ä¸zoo.cfgé…ç½®çš„ä¸€ä¸€å¯¹åº”ã€‚å³è¦å¯¹åº”ï¼š

    ```cfg
    server.1=node-master:2888:3888
    server.2=node1:2888:3888
    server.3=node2:2888:3888
    ```

    ä¹Ÿå°±æ˜¯ `node-master` çš„ `myid`æ˜¯`1`, `node1`çš„ `myid`æ˜¯ `2`,ä¾æ¬¡ç±»æ¨ã€‚éœ€è¦æ³¨æ„çš„ æ˜¯ï¼Œæ•°å­—å‰åéƒ½ä¸èƒ½æœ‰ç©ºæ ¼ï¼

<!--list-separator-->

2.  å¯åŠ¨ ZooKeeper

    åœ¨ `node-master` çš„ `/home/hadoop/hadoop/zookeeper-3.4.5-cdh5.7.1` ç›®å½•ï¼Œè¿è¡Œä»¥ ä¸‹å‘½ä»¤ï¼š

    ```shell
    sh bin/zkServer.sh start
    ```

    å…¶å®ƒç›¸åº”çš„å‘½ä»¤å¦‚ä¸‹ï¼š

    -   å¯åŠ¨ZKæœåŠ¡: `sh bin/zkServer.sh start`
    -   æŸ¥çœ‹ZKæœåŠ¡çŠ¶æ€: `sh bin/zkServer.sh status`
    -   åœæ­¢ZKæœåŠ¡: `sh bin/zkServer.sh stop`
    -   é‡å¯ZKæœåŠ¡: `sh bin/zkServer.sh restart`

<!--list-separator-->

3.  éªŒè¯ ZooKeeper

    å¯ä»¥é€šè¿‡è°ƒç”¨ `jps` æˆ–è€… `bin/zkCli.sh` æ¥éªŒè¯ Zookeeper çš„è¿è¡Œæƒ…å†µï¼š

    ```shell
    ./zkCli.sh -server 192.168.2.1:2181
    ```


### <span class="section-num">2.4</span> HBase æ­å»ºæµç¨‹ {#hbase-æ­å»ºæµç¨‹}


#### <span class="section-num">2.4.1</span> å®‰è£…Hbase {#å®‰è£…hbase}

1.  ä¸‹è½½hbase å®‰è£…åŒ…ï¼Œç™»å½•ä¸»æœºï¼Œé‡‡ç”¨wgetå‘½ä»¤ä¸‹è½½ï¼š
    wget <http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.7.1.tar.gz>
    2ã€è§£å‹å®‰è£…åˆ°hadoopç›®å½•ï¼ˆ3å°ä¸»æœºåŒæ ·æ“ä½œï¼‰

<!--listend-->

```sh
tar -zxvf hbase-1.2.0-cdh5.7.1.tar.gz -C ~/hadoop
```


#### <span class="section-num">2.4.2</span> ä¿®æ”¹hbaseçš„é…ç½®æ–‡ä»¶(æ‰€æœ‰ä¸»æœºä¸€æ ·çš„é…ç½®) {#ä¿®æ”¹hbaseçš„é…ç½®æ–‡ä»¶--æ‰€æœ‰ä¸»æœºä¸€æ ·çš„é…ç½®}

ä¿®æ”¹ `hbase-1.2.0-cdh5.7.1/conf` ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼š 1. ä¿®æ”¹ `hbase-site.xml` å†…å®¹å¦‚ä¸‹ï¼š

```xml
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://node-master:19000/hbase-${user.name}</value>
  </property>
  <property>
    <name>hbase.master</name>
    <value>node-master</value>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>
  <property>
    <name>hbase.tmp.dir</name>
    <value>/home/hadoop/hadoop/data/hbase-${user.name}</value>
  </property>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>node-master, node1, node2</value>
  </property>
</configuration>
```

1.  ä¿®æ”¹ `hbase-env.sh`:

<!--listend-->

```shell
export JAVA_HOME=/usr/java/jdk1.8.0_161/ (æ— æ³•è¯†åˆ«ç³»ç»Ÿçš„ç¯å¢ƒå˜é‡ï¼Œåœ¨è¿™é‡Œç›´æ¥æŒ‡å®š)
export HBASE_MANAGES_ZK=false  ï¼ˆå…³é—­è‡ªå¸¦çš„zookeeperï¼‰
export HBASE_SSH_OPTS="-p 9922" (ç«¯å£å·ä¸æ˜¯é»˜è®¤çš„22ï¼Œè¦æ”¹ä¸º9922)
```

1.  ä¿®æ”¹ =regionservers=(æŒ‡å®šregionserversçš„ä¸»æœºåœ°å€):

<!--listend-->

```shell
node-master
node1
node2
```


#### <span class="section-num">2.4.3</span> å¯åŠ¨ Hbase {#å¯åŠ¨-hbase}

åœ¨ `node-master` çš„ `/home/hadoop/hadoop/hbase-1.2.0-cdh5.7.1/bin` ç›®å½•ä¸‹è¿è¡Œï¼š

```sh
start-hbase.sh
```


#### <span class="section-num">2.4.4</span> ç›‘æ§ Hbase {#ç›‘æ§-hbase}

å¯ä»¥åœ¨æµè§ˆå™¨æŸ¥çœ‹ HBase é›†ç¾¤çš„ä¿¡æ¯ï¼š

-   HMaster web ç®¡ç†ä¿¡æ¯ï¼š<http://192.168.2.1:60010/master-status>
-   HregionServer ç®¡ç†ä¿¡æ¯ï¼š<http://192.168.2.1:60030/> <http://192.168.2.2:60030/>

<http://192.168.2.3:60030/>


## <span class="section-num">3</span> ç»“è¯­ {#ç»“è¯­}

æ•´ä¸ª Hbase é›†ç¾¤åº”è¯¥æ­å»ºå®Œäº†ï¼Œå…³äºMysql æ­å»ºçš„æ–‡ç« å°±å¤ªå¤šäº†ï¼Œæˆ‘ä¹Ÿä¸èµ˜è¨€äº†ã€‚è‡³æ­¤ï¼Œæ‰€æœ‰å­˜å‚¨çš„ç»„ä»¶å°±å·²ç»å®‰è£…å®Œæ¯•å¹¶å·²å¯ç”¨ï¼Œä½†æ˜¯éƒ½æ˜¯æ²¡æœ‰æ•°æ®çš„ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦åšçš„æ˜¯å¦‚ä½•å°†æ—§çš„æµ‹è¯•ç¯å¢ƒçš„æ•°æ®è¿ç§»åˆ°æ–°çš„æµ‹è¯•ç¯å¢ƒã€‚è€ƒè™‘åˆ°è¿™ç¯‡å†…å®¹å·²ç»å¾ˆé•¿äº†ï¼Œå‰©ä¸‹çš„å†…å®¹æˆ‘å°±å¦å¤–å†™ä¸€ç¯‡åšæ–‡äº†ã€‚


## <span class="section-num">4</span> å‚è€ƒ {#å‚è€ƒ}

-   <http://blog.csdn.net/u010824591/article/details/51174099>
-   <https://yq.aliyun.com/articles/26415>
-   <https://linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/>

<div center class="qr-container">
<img src="/ox-hugo/qrcode_gh_e06d750e626f_1.jpg" alt="qrcode_gh_e06d750e626f_1.jpg" width="160px" height="160px" center="t" class="qr-container" />
å…¬å·åŒæ­¥æ›´æ–°ï¼Œæ¬¢è¿å…³æ³¨ğŸ‘»
</div>

