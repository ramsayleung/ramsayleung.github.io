<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>从京东"窃取"150+万条数据 | 菠萝油与天光墟</title>
<meta name=keywords content="python,crawler"><meta name=description content="An spider to crawl jindong item and comments"><meta name=author content="Ramsay Leung"><link rel=canonical href=https://ramsayleung.github.io/zh/post/2017/jd_spider/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.756d16b9d1a19b0d7274ca93e0428bf24ca10b144104611c6875be78291ca07b.css integrity="sha256-dW0WudGhmw1ydMqT4EKL8kyhCxRBBGEcaHW+eCkcoHs=" rel="preload stylesheet" as=style><link rel=icon href=https://ramsayleung.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ramsayleung.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ramsayleung.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ramsayleung.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ramsayleung.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://ramsayleung.github.io/zh/post/2017/jd_spider/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-9MG65HQHEL"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-9MG65HQHEL")}</script><meta property="og:url" content="https://ramsayleung.github.io/zh/post/2017/jd_spider/"><meta property="og:site_name" content="菠萝油与天光墟"><meta property="og:title" content='从京东"窃取"150+万条数据'><meta property="og:description" content="An spider to crawl jindong item and comments"><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-06-21T00:00:00+08:00"><meta property="article:modified_time" content="2022-02-24T15:46:43+08:00"><meta property="article:tag" content="Python"><meta property="article:tag" content="Crawler"><meta property="og:image" content="https://ramsayleung.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ramsayleung.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content='从京东"窃取"150+万条数据'><meta name=twitter:description content="An spider to crawl jindong item and comments"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://ramsayleung.github.io/zh/post/"},{"@type":"ListItem","position":2,"name":"从京东\"窃取\"150+万条数据","item":"https://ramsayleung.github.io/zh/post/2017/jd_spider/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"从京东\"窃取\"150+万条数据","name":"从京东\u0022窃取\u0022150\u002b万条数据","description":"An spider to crawl jindong item and comments","keywords":["python","crawler"],"articleBody":"我最近编写了两只京东商品和评论的分布式爬虫来进行数据分析，现在就来分享一下。\n1 爬取策略 众所周知，爬虫比较难爬取的就是动态生成的网页，因为需要解析 JS, 其中比较典型的例子就是淘宝，天猫，京东，QQ 空间等。\n所以在我爬取京东网站的时候，首先需要确定的就是爬取策略。因为我想要爬取的是商品的信息以及相应的评论，并没有爬取特定的商品的需求。所以在分析京东的网页的 url 的时候, 决定使用类似全站爬取的策略。 分析如图：\n可以看出，京东不同的商品类别是对应不同的子域名的，例如 book 对应的是图书， mvd 对应的是音像， shouji 对应的是手机等。\n因为我使用的是获取 标签里面的 url 值，然后迭代爬取的策略。所以要把爬取的 url 限定在域名为jd.com 范围内，不然就有可能会出现无限广度。\n此外，有相当多的页面是不会包含商品信息的；例如： help.jd.com, doc.jd.com 等，因此使用 jd.com 这个域名范围实在太大了，所以把所需的子域名都添加到一个 list :\n1 2 3 4 jd_subdomain = [\"jiadian\", \"shouji\", \"wt\", \"shuma\", \"diannao\", \"bg\", \"channel\", \"jipiao\", \"hotel\", \"trip\", \"ish\", \"book\", \"e\", \"health\", \"baby\", \"toy\", \"nong\", \"jiu\", \"fresh\", \"china\", \"che\", \"list\"] 2 提取数据 在确定了爬取策略之后，爬虫就可以不断地进行工作了。那么爬虫怎么知道什么时候才是商品信息的页面呢？再来分析一下京东的商品页面：\n从上面的信息可以看出，每个商品的页面都是以 item.jd.com/xxxxxxx.html 的形式存 在的；而 xxxxxxx 就是该商品的 sku-id. 所以只需对 url 进行解析，子域名为 item 即商品页面，就可以进行爬取。\n页面提取使用 Xpath 即可，也无需赘言。不过，需要注 意的是对商品而言，非常重要的价格就不是可以通过爬取 HTML 页面得到的。\n因为价格是经常变动的，所以是异步向后台请求的。对于这些异步请求的数据，打开控制台，然后刷新，就可以看到一堆的 JS 文件，然后寻找相应的请求带有 “money 或者price” 之类关 键字的 JS 文件，应该就能找到。\n如果还没办法找出来的话，Firefox 上有一个 user-agent-switcher 的扩展，然后通过这个扩展把自己的浏览器伪装成 IE6, 相信所有 花俏的 JS 都会没了, 只剩下那些不可或缺的 JS, 这样结果应该一目了然了，这么看来 IE6 还是有用滴。最终找到的URL 如下 https://p.3.cn/prices/mgets?callback=jQuery6646724\u0026type=1\u0026area=19_1601_3633_0.137875165\u0026pdtk=9D4RIAHY317A3bZnQNapD7ip5Dg%252F6NXiIXt90Ahk0if2Yyh39PZQCuDBlhN%252FxOch3MpwWpHICu4P%250AVcgcOm11GQ%253D%253D\u0026pduid=14966417675252009727775\u0026pdpin=%25E5%2585%2591%25E9%2587%2591%25E8%25BE%25B0%25E6%2589%258B\u0026pdbp=0\u0026skuIds=J_3356012\u0026ext=10000000\u0026source=item-pc\n不得不说，URL 实在是太长了。\n根据经验，大部分的参数应该都是没什么用的，应该可以去掉的，所以在浏览器就一个个参数去掉，然后试试请求是否成功，如果成功，说明此参数无关重要，最后简化成： http://p.3.cn/prices/mgets?pduid={}\u0026skuIds=J_{} sku_id 即商品页面的 URL中包含的数字，而 pduid 则是一随机整数而已，用 random.randint(1, 100000000) 函数解决。\n3 商品评论 商品的评论也是以 sku-id 为参数通过异步的方式进行请求的，构造请求的方法跟价格类 似，也不需过多赘述。\n只是想要吐嘈一下的是，京东的评论是只能一页页向后翻的，不能跳转。还有一点就是，即使某样商品有 10+w 条评论，最多也只是返回 100页的数据。 略坑\n4 反爬虫策略 商品的爬取策略以及提取策略都确定了，一只爬虫就基本成型了。但是一般比较大型的网站都有反爬虫措施的。所以道高一尺，魔高一丈，爬虫也要有对应的反反爬虫策略\n4.1 禁用 cookie 通过禁用 cookie, 服务器就无法根据 cookie 判断出爬虫是否访问过网站\n4.2 轮转 user-agent 一般的爬虫都会使用浏览器的 user-agent 来模拟浏览器以欺骗服务器 (当然，如果你是一只什么 user-agent都不用耿直的小爬虫，我也无话可说).\n为了提高突破反爬虫策略的成功率，可以定义多个 user-agent, 然后每次请求都随机选择 user-agent。\n4.3 伪装成搜索引擎 要说最著名的爬虫是谁？肯定是搜索引擎，它本质上也是爬虫，而且是非常强大的爬虫。\n而且这些爬虫可以光明正大地去爬取各式网站，相信各式网站也很乐意被它爬。\n那么， 现在可以通过修改 user-agent 伪装成搜索引擎，然后再结合上面的轮转 user-agent,\n伪装成各式搜索引擎：\n1 2 3 4 5 6 7 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)', 'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)', 'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)', 'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)', 'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)', 'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)', 'ia_archiver (+http://www.alexa.com/site/help/webmasters; crawler@alexa.com)', 4.4 代理 IP 虽说可以伪装成搜索引擎，但是因为 http 请求是建立在三次握手之上的，爬虫的 IP 还是会被记录下来的，如果同一个 IP 访问得太频繁，那基本就可以确定是一只爬虫了，然后就把它的 IP 封掉，温和一点的就会叫你输入验证码，不然就返回 403.\n对待这种情况，就需要使用代理 IP 了。\n只是代理 IP 都有不同程度的延迟，并且免费的 IP 大多不能用，所以这是不得而为之了\n5 扩展成分布式爬虫 一台机器的爬虫可能爬取一个网站可能需要 100 天，而且带宽也到达瓶颈了，那么是否可以提高爬取效率呢？\n那就用 100台机器，1天应该就能爬取完 (当然，现实并非如此美好).\n这个就涉及到分布式的爬虫的问题。而不同的分布式爬虫有不同的实现方法，而我选择了 scrapy 和 redis 整合的 scrapy-redis 来实现分布式，URL 的去重以及调度都有了相应的实现了，也无需额外的操心\n6 爬虫监控 既然爬虫从单机变成了分布式，新的问题随之而来：如何监控分布式爬虫呢？在单机的时候，最简单的监控 – 直接将爬虫的日志信息输出到终端即可。\n但是对于分布式爬虫，这样的做法显然不现实。我最终选择使用 graphite 这个监控工具。\n6.1 scrapy-graphite 参考 Github上 distributed_crawler 的代码，将单机版本的 scrapy-graphite 扩展成基于分布式的 graphite 监控程序，并且实现对 python3 的支持。\n6.2 docker 但是 graphite 只是支持 python2, 并且安装过程很麻烦，我在折腾大半天后都无法安装成功，实在有点沮丧。最后想起了伟大的 docker, 并且直接找到已经打包好的image. 数行命令即解决所有的安装问题，不得不说：docker, 你值得拥有。运行截图：\n7 爬虫拆分 本来爬取商品信息的爬虫和爬取评论的爬虫都是同一只爬虫，但是后来发现，再不使用代理 IP 的情况下，爬取到 150000 条商品信息的时候，需要输入验证码。\n但是爬取商品评论的爬虫并不存在被反爬策略限制的情况。所以我将爬虫拆分成两只爬虫，即使无法爬取商品信息的时候，还可以爬取商品的评论信息。\n8 小结 在爬取一天之后，爬虫成果：\n8.1 评论 8.2 评论总结 8.3 商品信息 商品信息加上评论数约 150+w.\n9 参考及致谢 https://github.com/noplay/scrapy-graphite https://github.com/gnemoug/distribute_crawler https://github.com/hopsoft/docker-graphite-statsd 10 项目源码 https://github.com/samrayleung/jd_spider\n公号同步更新，欢迎关注👻 ","wordCount":"2233","inLanguage":"zh","image":"https://ramsayleung.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2017-06-21T00:00:00+08:00","dateModified":"2022-02-24T15:46:43+08:00","author":{"@type":"Person","name":"Ramsay Leung"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ramsayleung.github.io/zh/post/2017/jd_spider/"},"publisher":{"@type":"Organization","name":"菠萝油与天光墟","logo":{"@type":"ImageObject","url":"https://ramsayleung.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ramsayleung.github.io/zh/ accesskey=h title="Home (Alt + H)"><img src=https://ramsayleung.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://ramsayleung.github.io/en/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li class=dropdown><a href=https://ramsayleung.github.io/zh/categories/ title="系列 "><span>系列 ▾</span></a><div class="menu-more-content dropdown-content"><a href=https://ramsayleung.github.io/zh/categories/%E5%BE%97%E5%A4%B1%E6%84%9F%E6%82%9F/ title=得失感悟><span>得失感悟
</span></a><a href=https://ramsayleung.github.io/zh/categories/%E6%97%85%E5%8A%A0%E7%BB%8F%E5%8E%86 title=旅加经历><span>旅加经历
</span></a><a href=https://ramsayleung.github.io/zh/categories/%E5%B7%A5%E4%BD%9C%E6%B5%81/ title=我的工作流><span>我的工作流
</span></a><a href=https://ramsayleung.github.io/zh/categories/%E6%B5%8B%E8%AF%95%E6%8A%80%E8%83%BD%E8%BF%9B%E9%98%B6/ title=测试技能进阶><span>测试技能进阶
</span></a><a href=https://ramsayleung.github.io/zh/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E8%BD%AF%E6%8A%80%E8%83%BD%E6%8C%87%E5%8C%97/ title=软件工程师的软技能指北><span>软件工程师的软技能指北
</span></a><a href=https://ramsayleung.github.io/zh/categories/%E5%9F%BA%E4%BA%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95%E7%9A%84telegram%E5%B9%BF%E5%91%8A%E6%8B%A6%E6%88%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/ title=基于贝叶斯算法的Telegram广告拦截机器人><span>基于贝叶斯算法的Telegram广告拦截机器人</span></a></div></li><li><a href=https://ramsayleung.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://ramsayleung.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://ramsayleung.github.io/zh/tags/ title=标签><span>标签</span></a></li><li><a href=https://ramsayleung.github.io/zh/about_me_zh/ title=关于><span>关于</span></a></li><li><a href=https://ramsayleung.github.io/zh/index.xml title=RSS><span>RSS</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ramsayleung.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://ramsayleung.github.io/zh/post/>Posts</a></div><h1 class="post-title entry-hint-parent">从京东"窃取"150+万条数据</h1><div class=post-description>An spider to crawl jindong item and comments</div><div class=post-quote><blockquote><p>简单即是美<br>Simple is beautiful</p></blockquote></div><hr class=post-separator><div class=post-meta><span title='2017-06-21 00:00:00 +0800 +0800'>六月 21, 2017</span>&nbsp;·&nbsp;5 分钟&nbsp;·&nbsp;2233 字&nbsp;|&nbsp;<a href=https://github.com/ramsayleung/ramsayleung.github.io/blob/master/content/post/2017/jd_spider.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#爬取策略><span class=section-num>1</span> 爬取策略</a></li><li><a href=#提取数据><span class=section-num>2</span> 提取数据</a></li><li><a href=#商品评论><span class=section-num>3</span> 商品评论</a></li><li><a href=#反爬虫策略><span class=section-num>4</span> 反爬虫策略</a><ul><li><a href=#禁用-cookie><span class=section-num>4.1</span> 禁用 cookie</a></li><li><a href=#轮转-user-agent><span class=section-num>4.2</span> 轮转 user-agent</a></li><li><a href=#伪装成搜索引擎><span class=section-num>4.3</span> 伪装成搜索引擎</a></li><li><a href=#代理-ip><span class=section-num>4.4</span> 代理 IP</a></li></ul></li><li><a href=#扩展成分布式爬虫><span class=section-num>5</span> 扩展成分布式爬虫</a></li><li><a href=#爬虫监控><span class=section-num>6</span> 爬虫监控</a><ul><li><a href=#scrapy-graphite><span class=section-num>6.1</span> scrapy-graphite</a></li><li><a href=#docker><span class=section-num>6.2</span> docker</a></li></ul></li><li><a href=#爬虫拆分><span class=section-num>7</span> 爬虫拆分</a></li><li><a href=#小结><span class=section-num>8</span> 小结</a><ul><li><a href=#评论><span class=section-num>8.1</span> 评论</a></li><li><a href=#评论总结><span class=section-num>8.2</span> 评论总结</a></li><li><a href=#商品信息><span class=section-num>8.3</span> 商品信息</a></li></ul></li><li><a href=#参考及致谢><span class=section-num>9</span> 参考及致谢</a></li><li><a href=#项目源码><span class=section-num>10</span> 项目源码</a></li></ul></nav></div></details></div><div class=post-content><p>我最近编写了两只京东商品和评论的分布式爬虫来进行数据分析，现在就来分享一下。</p><h2 id=爬取策略><span class=section-num>1</span> 爬取策略<a hidden class=anchor aria-hidden=true href=#爬取策略>#</a></h2><p>众所周知，爬虫比较难爬取的就是动态生成的网页，因为需要解析 JS, 其中比较典型的例子就是淘宝，天猫，京东，QQ 空间等。</p><p>所以在我爬取京东网站的时候，首先需要确定的就是爬取策略。因为我想要爬取的是商品的信息以及相应的评论，并没有爬取特定的商品的需求。所以在分析京东的网页的 url 的时候, 决定使用类似全站爬取的策略。 分析如图：</p><figure><a href=/ox-hugo/jd_analyze.png><input type=checkbox id=zoomCheck-0b0bf hidden>
<label for=zoomCheck-0b0bf><img class=zoomCheck loading=lazy src=/ox-hugo/jd_analyze.png></label></a></figure><p>可以看出，京东不同的商品类别是对应不同的子域名的，例如 <code>book</code> 对应的是图书， <code>mvd</code> 对应的是音像， <code>shouji</code> 对应的是手机等。</p><p>因为我使用的是获取 <code>&lt;a href></code> 标签里面的 url 值，然后迭代爬取的策略。所以要把爬取的 url 限定在域名为<code>jd.com</code> 范围内，不然就有可能会出现无限广度。</p><p>此外，有相当多的页面是不会包含商品信息的；例如： <code>help.jd.com</code>, <code>doc.jd.com</code> 等，因此使用 <code>jd.com</code> 这个域名范围实在太大了，所以把所需的子域名都添加到一个 list :</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>jd_subdomain</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;jiadian&#34;</span><span class=p>,</span> <span class=s2>&#34;shouji&#34;</span><span class=p>,</span> <span class=s2>&#34;wt&#34;</span><span class=p>,</span> <span class=s2>&#34;shuma&#34;</span><span class=p>,</span> <span class=s2>&#34;diannao&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=s2>&#34;bg&#34;</span><span class=p>,</span> <span class=s2>&#34;channel&#34;</span><span class=p>,</span> <span class=s2>&#34;jipiao&#34;</span><span class=p>,</span> <span class=s2>&#34;hotel&#34;</span><span class=p>,</span> <span class=s2>&#34;trip&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=s2>&#34;ish&#34;</span><span class=p>,</span> <span class=s2>&#34;book&#34;</span><span class=p>,</span> <span class=s2>&#34;e&#34;</span><span class=p>,</span> <span class=s2>&#34;health&#34;</span><span class=p>,</span> <span class=s2>&#34;baby&#34;</span><span class=p>,</span> <span class=s2>&#34;toy&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=s2>&#34;nong&#34;</span><span class=p>,</span> <span class=s2>&#34;jiu&#34;</span><span class=p>,</span> <span class=s2>&#34;fresh&#34;</span><span class=p>,</span> <span class=s2>&#34;china&#34;</span><span class=p>,</span> <span class=s2>&#34;che&#34;</span><span class=p>,</span> <span class=s2>&#34;list&#34;</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=提取数据><span class=section-num>2</span> 提取数据<a hidden class=anchor aria-hidden=true href=#提取数据>#</a></h2><p>在确定了爬取策略之后，爬虫就可以不断地进行工作了。那么爬虫怎么知道什么时候才是商品信息的页面呢？再来分析一下京东的商品页面：</p><figure><a href=/ox-hugo/jd_item_analyze.png><input type=checkbox id=zoomCheck-c8137 hidden>
<label for=zoomCheck-c8137><img class=zoomCheck loading=lazy src=/ox-hugo/jd_item_analyze.png></label></a></figure><p>从上面的信息可以看出，每个商品的页面都是以 <code>item.jd.com/xxxxxxx.html</code> 的形式存 在的；而 xxxxxxx 就是该商品的 sku-id. 所以只需对 url 进行解析，子域名为 <code>item</code> 即商品页面，就可以进行爬取。</p><p>页面提取使用 Xpath 即可，也无需赘言。不过，需要注 意的是对商品而言，非常重要的价格就不是可以通过爬取 HTML 页面得到的。</p><p>因为价格是经常变动的，所以是异步向后台请求的。对于这些异步请求的数据，打开控制台，然后刷新，就可以看到一堆的 JS 文件，然后寻找相应的请求带有 &ldquo;money 或者price&rdquo; 之类关 键字的 JS 文件，应该就能找到。</p><p>如果还没办法找出来的话，Firefox 上有一个 <a href=https://addons.mozilla.org/en-US/firefox/addon/user-agent-switcher/>user-agent-switcher</a> 的扩展，然后通过这个扩展把自己的浏览器伪装成 IE6, 相信所有
花俏的 JS 都会没了, 只剩下那些不可或缺的 JS, 这样结果应该一目了然了，这么看来 IE6 还是有用滴。最终找到的URL 如下
<code>https://p.3.cn/prices/mgets?callback=jQuery6646724&amp;type=1&amp;area=19_1601_3633_0.137875165&amp;pdtk=9D4RIAHY317A3bZnQNapD7ip5Dg%252F6NXiIXt90Ahk0if2Yyh39PZQCuDBlhN%252FxOch3MpwWpHICu4P%250AVcgcOm11GQ%253D%253D&amp;pduid=14966417675252009727775&amp;pdpin=%25E5%2585%2591%25E9%2587%2591%25E8%25BE%25B0%25E6%2589%258B&amp;pdbp=0&amp;skuIds=J_3356012&amp;ext=10000000&amp;source=item-pc</code></p><p>不得不说，URL 实在是太长了。</p><p>根据经验，大部分的参数应该都是没什么用的，应该可以去掉的，所以在浏览器就一个个参数去掉，然后试试请求是否成功，如果成功，说明此参数无关重要，最后简化成： <code>http://p.3.cn/prices/mgets?pduid={}&amp;skuIds=J_{}</code> sku_id 即商品页面的 URL中包含的数字，而 pduid 则是一随机整数而已，用
<code>random.randint(1, 100000000)</code> 函数解决。</p><h2 id=商品评论><span class=section-num>3</span> 商品评论<a hidden class=anchor aria-hidden=true href=#商品评论>#</a></h2><p>商品的评论也是以 sku-id 为参数通过异步的方式进行请求的，构造请求的方法跟价格类 似，也不需过多赘述。</p><p>只是想要吐嘈一下的是，京东的评论是只能一页页向后翻的，不能跳转。还有一点就是，即使某样商品有 10+w 条评论，最多也只是返回 100页的数据。
略坑</p><h2 id=反爬虫策略><span class=section-num>4</span> 反爬虫策略<a hidden class=anchor aria-hidden=true href=#反爬虫策略>#</a></h2><p>商品的爬取策略以及提取策略都确定了，一只爬虫就基本成型了。但是一般比较大型的网站都有反爬虫措施的。所以道高一尺，魔高一丈，爬虫也要有对应的反反爬虫策略</p><h3 id=禁用-cookie><span class=section-num>4.1</span> 禁用 cookie<a hidden class=anchor aria-hidden=true href=#禁用-cookie>#</a></h3><p>通过禁用 cookie, 服务器就无法根据 cookie 判断出爬虫是否访问过网站</p><h3 id=轮转-user-agent><span class=section-num>4.2</span> 轮转 user-agent<a hidden class=anchor aria-hidden=true href=#轮转-user-agent>#</a></h3><p>一般的爬虫都会使用浏览器的 user-agent 来模拟浏览器以欺骗服务器 (当然，如果你是一只什么 user-agent都不用耿直的小爬虫，我也无话可说).</p><p>为了提高突破反爬虫策略的成功率，可以定义多个 user-agent, 然后每次请求都随机选择 user-agent。</p><h3 id=伪装成搜索引擎><span class=section-num>4.3</span> 伪装成搜索引擎<a hidden class=anchor aria-hidden=true href=#伪装成搜索引擎>#</a></h3><p>要说最著名的爬虫是谁？肯定是搜索引擎，它本质上也是爬虫，而且是非常强大的爬虫。</p><p>而且这些爬虫可以光明正大地去爬取各式网站，相信各式网站也很乐意被它爬。</p><p>那么， 现在可以通过修改 user-agent 伪装成搜索引擎，然后再结合上面的轮转 user-agent,</p><p>伪装成各式搜索引擎：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&#39;Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)&#39;,
</span></span><span class=line><span class=cl>&#39;Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)&#39;,
</span></span><span class=line><span class=cl>&#39;Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)&#39;,
</span></span><span class=line><span class=cl>&#39;DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)&#39;,
</span></span><span class=line><span class=cl>&#39;Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)&#39;,
</span></span><span class=line><span class=cl>&#39;Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)&#39;,
</span></span><span class=line><span class=cl>&#39;ia_archiver (+http://www.alexa.com/site/help/webmasters; crawler@alexa.com)&#39;,
</span></span></code></pre></td></tr></table></div></div><h3 id=代理-ip><span class=section-num>4.4</span> 代理 IP<a hidden class=anchor aria-hidden=true href=#代理-ip>#</a></h3><p>虽说可以伪装成搜索引擎，但是因为 http 请求是建立在三次握手之上的，爬虫的 IP 还是会被记录下来的，如果同一个 IP 访问得太频繁，那基本就可以确定是一只爬虫了，然后就把它的 IP 封掉，温和一点的就会叫你输入验证码，不然就返回 403.</p><p>对待这种情况，就需要使用代理 IP 了。</p><p>只是代理 IP 都有不同程度的延迟，并且免费的 IP 大多不能用，所以这是不得而为之了</p><h2 id=扩展成分布式爬虫><span class=section-num>5</span> 扩展成分布式爬虫<a hidden class=anchor aria-hidden=true href=#扩展成分布式爬虫>#</a></h2><p>一台机器的爬虫可能爬取一个网站可能需要 100 天，而且带宽也到达瓶颈了，那么是否可以提高爬取效率呢？</p><p>那就用 100台机器，1天应该就能爬取完 (当然，现实并非如此美好).</p><p>这个就涉及到分布式的爬虫的问题。而不同的分布式爬虫有不同的实现方法，而我选择了 scrapy 和 redis 整合的 <a href=https://github.com/rolando/scrapy-redis>scrapy-redis</a> 来实现分布式，URL 的去重以及调度都有了相应的实现了，也无需额外的操心</p><h2 id=爬虫监控><span class=section-num>6</span> 爬虫监控<a hidden class=anchor aria-hidden=true href=#爬虫监控>#</a></h2><p>既然爬虫从单机变成了分布式，新的问题随之而来：如何监控分布式爬虫呢？在单机的时候，最简单的监控 &ndash; 直接将爬虫的日志信息输出到终端即可。</p><p>但是对于分布式爬虫，这样的做法显然不现实。我最终选择使用 <a href=https://graphiteapp.org>graphite</a> 这个监控工具。</p><h3 id=scrapy-graphite><span class=section-num>6.1</span> scrapy-graphite<a hidden class=anchor aria-hidden=true href=#scrapy-graphite>#</a></h3><p>参考 Github上 <a href=https://github.com/gnemoug/distribute_crawler>distributed_crawler</a> 的代码，将单机版本的 <a href=https://github.com/noplay/scrapy-graphite>scrapy-graphite</a> 扩展成基于分布式的 graphite 监控程序，并且实现对 python3 的支持。</p><h3 id=docker><span class=section-num>6.2</span> docker<a hidden class=anchor aria-hidden=true href=#docker>#</a></h3><p>但是 graphite 只是支持 python2, 并且安装过程很麻烦，我在折腾大半天后都无法安装成功，实在有点沮丧。最后想起了伟大的 docker, 并且直接找到已经打包好的image. 数行命令即解决所有的安装问题，不得不说：docker, 你值得拥有。运行截图：</p><figure><a href=/ox-hugo/jd_comment_graphite1.png><input type=checkbox id=zoomCheck-cd205 hidden>
<label for=zoomCheck-cd205><img class=zoomCheck loading=lazy src=/ox-hugo/jd_comment_graphite1.png></label></a></figure><figure><a href=/ox-hugo/jd_comment_graphite2.png><input type=checkbox id=zoomCheck-97ae9 hidden>
<label for=zoomCheck-97ae9><img class=zoomCheck loading=lazy src=/ox-hugo/jd_comment_graphite2.png></label></a></figure><h2 id=爬虫拆分><span class=section-num>7</span> 爬虫拆分<a hidden class=anchor aria-hidden=true href=#爬虫拆分>#</a></h2><p>本来爬取商品信息的爬虫和爬取评论的爬虫都是同一只爬虫，但是后来发现，再不使用代理 IP 的情况下，爬取到 150000 条商品信息的时候，需要输入验证码。</p><p>但是爬取商品评论的爬虫并不存在被反爬策略限制的情况。所以我将爬虫拆分成两只爬虫，即使无法爬取商品信息的时候，还可以爬取商品的评论信息。</p><h2 id=小结><span class=section-num>8</span> 小结<a hidden class=anchor aria-hidden=true href=#小结>#</a></h2><p>在爬取一天之后，爬虫成果：</p><h3 id=评论><span class=section-num>8.1</span> 评论<a hidden class=anchor aria-hidden=true href=#评论>#</a></h3><figure><a href=/ox-hugo/jd_comment.png><input type=checkbox id=zoomCheck-8be27 hidden>
<label for=zoomCheck-8be27><img class=zoomCheck loading=lazy src=/ox-hugo/jd_comment.png></label></a></figure><h3 id=评论总结><span class=section-num>8.2</span> 评论总结<a hidden class=anchor aria-hidden=true href=#评论总结>#</a></h3><figure><a href=/ox-hugo/jd_comment_summary.png><input type=checkbox id=zoomCheck-fc61f hidden>
<label for=zoomCheck-fc61f><img class=zoomCheck loading=lazy src=/ox-hugo/jd_comment_summary.png></label></a></figure><h3 id=商品信息><span class=section-num>8.3</span> 商品信息<a hidden class=anchor aria-hidden=true href=#商品信息>#</a></h3><figure><a href=/ox-hugo/jd_parameters.png><input type=checkbox id=zoomCheck-29f68 hidden>
<label for=zoomCheck-29f68><img class=zoomCheck loading=lazy src=/ox-hugo/jd_parameters.png></label></a></figure><p>商品信息加上评论数约 150+w.</p><h2 id=参考及致谢><span class=section-num>9</span> 参考及致谢<a hidden class=anchor aria-hidden=true href=#参考及致谢>#</a></h2><ul><li><a href=https://github.com/noplay/scrapy-graphite>https://github.com/noplay/scrapy-graphite</a></li><li><a href=https://github.com/gnemoug/distribute_crawler>https://github.com/gnemoug/distribute_crawler</a></li><li><a href=https://github.com/hopsoft/docker-graphite-statsd>https://github.com/hopsoft/docker-graphite-statsd</a></li></ul><h2 id=项目源码><span class=section-num>10</span> 项目源码<a hidden class=anchor aria-hidden=true href=#项目源码>#</a></h2><p><a href=https://github.com/samrayleung/jd_spider>https://github.com/samrayleung/jd_spider</a></p><div center class=qr-container><img src=/ox-hugo/qrcode_gh_e06d750e626f_1.jpg alt=qrcode_gh_e06d750e626f_1.jpg width=160px height=160px center=t class=qr-container>
公号同步更新，欢迎关注👻</div></div><footer class=post-footer><ul class=post-tags><li><a href=https://ramsayleung.github.io/zh/tags/python/>Python</a></li><li><a href=https://ramsayleung.github.io/zh/tags/crawler/>Crawler</a></li></ul><nav class=paginav><a class=prev href=https://ramsayleung.github.io/zh/post/2017/blog/><span class=title>« 上一页</span><br><span>写博客的动机</span>
</a><a class=next href=https://ramsayleung.github.io/zh/post/2017/tweak_eshell_prompt/><span class=title>下一页 »</span><br><span>Eshell提示符优化</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label='share 从京东"窃取"150+万条数据 on x' href="https://x.com/intent/tweet/?text=%e4%bb%8e%e4%ba%ac%e4%b8%9c%22%e7%aa%83%e5%8f%96%22150%2b%e4%b8%87%e6%9d%a1%e6%95%b0%e6%8d%ae&amp;url=https%3a%2f%2framsayleung.github.io%2fzh%2fpost%2f2017%2fjd_spider%2f&amp;hashtags=python%2ccrawler"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label='share 从京东"窃取"150+万条数据 on linkedin' href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2framsayleung.github.io%2fzh%2fpost%2f2017%2fjd_spider%2f&amp;title=%e4%bb%8e%e4%ba%ac%e4%b8%9c%22%e7%aa%83%e5%8f%96%22150%2b%e4%b8%87%e6%9d%a1%e6%95%b0%e6%8d%ae&amp;summary=%e4%bb%8e%e4%ba%ac%e4%b8%9c%22%e7%aa%83%e5%8f%96%22150%2b%e4%b8%87%e6%9d%a1%e6%95%b0%e6%8d%ae&amp;source=https%3a%2f%2framsayleung.github.io%2fzh%2fpost%2f2017%2fjd_spider%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label='share 从京东"窃取"150+万条数据 on reddit' href="https://reddit.com/submit?url=https%3a%2f%2framsayleung.github.io%2fzh%2fpost%2f2017%2fjd_spider%2f&title=%e4%bb%8e%e4%ba%ac%e4%b8%9c%22%e7%aa%83%e5%8f%96%22150%2b%e4%b8%87%e6%9d%a1%e6%95%b0%e6%8d%ae"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label='share 从京东"窃取"150+万条数据 on facebook' href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2framsayleung.github.io%2fzh%2fpost%2f2017%2fjd_spider%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label='share 从京东"窃取"150+万条数据 on whatsapp' href="https://api.whatsapp.com/send?text=%e4%bb%8e%e4%ba%ac%e4%b8%9c%22%e7%aa%83%e5%8f%96%22150%2b%e4%b8%87%e6%9d%a1%e6%95%b0%e6%8d%ae%20-%20https%3a%2f%2framsayleung.github.io%2fzh%2fpost%2f2017%2fjd_spider%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label='share 从京东"窃取"150+万条数据 on telegram' href="https://telegram.me/share/url?text=%e4%bb%8e%e4%ba%ac%e4%b8%9c%22%e7%aa%83%e5%8f%96%22150%2b%e4%b8%87%e6%9d%a1%e6%95%b0%e6%8d%ae&amp;url=https%3a%2f%2framsayleung.github.io%2fzh%2fpost%2f2017%2fjd_spider%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label='share 从京东"窃取"150+万条数据 on ycombinator' href="https://news.ycombinator.com/submitlink?t=%e4%bb%8e%e4%ba%ac%e4%b8%9c%22%e7%aa%83%e5%8f%96%22150%2b%e4%b8%87%e6%9d%a1%e6%95%b0%e6%8d%ae&u=https%3a%2f%2framsayleung.github.io%2fzh%2fpost%2f2017%2fjd_spider%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><section><h2>Comments</h2><div id=comments-giscus></div></section><script type=text/javascript>function getCurrentTheme(){return document.documentElement.getAttribute("data-theme")||document.body.classList.contains("dark")?"dark":"light"}function setGiscusTheme(e=!1){const s=e?"dark":"light";var n,t=document.querySelector(".giscus-frame");t&&(n=new URL(t.src),n.searchParams.set("theme",s),t.src=n.toString())}function loadComment(e=!1){const n="zh"=="zh",t=document.getElementById("comments-giscus");if(t!==null&&!t.hasAttribute("data-giscus-loaded")){console.log("Initial giscus load");const s=document.createElement("script");s.setAttribute("src","https://giscus.app/client.js"),s.setAttribute("data-repo","ramsayleung/comment"),s.setAttribute("data-repo-id","MDEwOlJlcG9zaXRvcnkzMDk2NjQ1NDk="),s.setAttribute("data-category","General"),s.setAttribute("data-category-id","DIC_kwDOEnUbJc4Cltnz"),s.setAttribute("data-mapping","pathname"),s.setAttribute("data-strict","0"),s.setAttribute("data-reactions-enabled","1"),s.setAttribute("data-emit-metadata","0"),s.setAttribute("data-input-position","bottom"),s.setAttribute("data-theme",e?"dark":"light"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("data-lang",n?"zh-CN":"en"),s.setAttribute("async","true"),t.appendChild(s),t.setAttribute("data-giscus-loaded","true")}}let currentTheme=getCurrentTheme();loadComment(currentTheme==="dark");const themeObserver=new MutationObserver(e=>{e.forEach(e=>{if(e.type==="attributes"&&e.attributeName==="class"){const e=getCurrentTheme();e!==currentTheme&&(currentTheme=e,setGiscusTheme(currentTheme==="dark"))}})});themeObserver.observe(document.body,{attributes:!0,attributeFilter:["class"]})</script></article></main><footer class=footer><span>See this site&rsquo;s source code <a href=https://github.com/ramsayleung/ramsayleung.github.io>here</a>, licensed under GPLv3 ·</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>