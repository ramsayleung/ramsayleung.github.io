<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>crawler on In Pursuit of Hubris</title><link>https://ramsayleung.github.io/tags/crawler/</link><description>Recent content in crawler on In Pursuit of Hubris</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Wed, 21 Jun 2017 00:00:00 +0800</lastBuildDate><atom:link href="https://ramsayleung.github.io/tags/crawler/index.xml" rel="self" type="application/rss+xml"/><item><title>从京东"窃取"150+万条数据</title><link>https://ramsayleung.github.io/post/2017/jd_spider/</link><pubDate>Wed, 21 Jun 2017 00:00:00 +0800</pubDate><guid>https://ramsayleung.github.io/post/2017/jd_spider/</guid><description>我最近编写了两只京东商品和评论的分布式爬虫来进行数据分析，现在就来分享一下。 1 爬取策略 众所周知，爬虫比较难爬取的就是动态生成的网页，因为需要</description></item><item><title>爬虫高效去重之布隆过滤器</title><link>https://ramsayleung.github.io/post/2017/bloom_filter/</link><pubDate>Sun, 09 Apr 2017 00:00:00 +0800</pubDate><guid>https://ramsayleung.github.io/post/2017/bloom_filter/</guid><description>笔者最近思考如何编写高效的爬虫; 而在编写高效爬虫的时候，有一个必需解决的问题就是： url 的去重，即如何判别 url 是否已经被爬取，如果被爬取，那就不要</description></item></channel></rss>