<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Crawler on 过河卒</title>
    <link>http://localhost:1313/zh/tags/crawler/</link>
    <description>Recent content in Crawler on 过河卒</description>
    <image>
      <title>过河卒</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.146.7</generator>
    <language>zh</language>
    <copyright>See this site&amp;rsquo;s source code here, licensed under GPLv3 ·</copyright>
    <lastBuildDate>Thu, 24 Feb 2022 15:46:43 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/zh/tags/crawler/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>从京东&#34;窃取&#34;150&#43;万条数据</title>
      <link>http://localhost:1313/zh/post/2017/jd_spider/</link>
      <pubDate>Wed, 21 Jun 2017 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/zh/post/2017/jd_spider/</guid>
      <description>An spider to crawl jindong item and comments</description>
      <content:encoded><![CDATA[<p>我最近编写了两只京东商品和评论的分布式爬虫来进行数据分析，现在就来分享一下。</p>
<h2 id="爬取策略"><span class="section-num">1</span> 爬取策略</h2>
<p>众所周知，爬虫比较难爬取的就是动态生成的网页，因为需要解析 JS, 其中比较典型的例子就是淘宝，天猫，京东，QQ 空间等。</p>
<p>所以在我爬取京东网站的时候，首先需要确定的就是爬取策略。因为我想要爬取的是商品的信息以及相应的评论，并没有爬取特定的商品的需求。所以在分析京东的网页的 url 的时候, 决定使用类似全站爬取的策略。 分析如图：</p>

<figure><a href="/ox-hugo/jd_analyze.png">
    
    
    <input type="checkbox" id="zoomCheck-0b0bf" hidden>
    <label for="zoomCheck-0b0bf">
    
    
    <img class="zoomCheck" loading="lazy" src="/ox-hugo/jd_analyze.png"/> 
    
    
    </label></a>
</figure>

<p>可以看出，京东不同的商品类别是对应不同的子域名的，例如 <code>book</code> 对应的是图书， <code>mvd</code> 对应的是音像， <code>shouji</code> 对应的是手机等。</p>
<p>因为我使用的是获取 <code>&lt;a href&gt;</code> 标签里面的 url 值，然后迭代爬取的策略。所以要把爬取的 url 限定在域名为<code>jd.com</code> 范围内，不然就有可能会出现无限广度。</p>
<p>此外，有相当多的页面是不会包含商品信息的；例如： <code>help.jd.com</code>, <code>doc.jd.com</code> 等，因此使用 <code>jd.com</code> 这个域名范围实在太大了，所以把所需的子域名都添加到一个 list :</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">jd_subdomain</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;jiadian&#34;</span><span class="p">,</span> <span class="s2">&#34;shouji&#34;</span><span class="p">,</span> <span class="s2">&#34;wt&#34;</span><span class="p">,</span> <span class="s2">&#34;shuma&#34;</span><span class="p">,</span> <span class="s2">&#34;diannao&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="s2">&#34;bg&#34;</span><span class="p">,</span> <span class="s2">&#34;channel&#34;</span><span class="p">,</span> <span class="s2">&#34;jipiao&#34;</span><span class="p">,</span> <span class="s2">&#34;hotel&#34;</span><span class="p">,</span> <span class="s2">&#34;trip&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="s2">&#34;ish&#34;</span><span class="p">,</span> <span class="s2">&#34;book&#34;</span><span class="p">,</span> <span class="s2">&#34;e&#34;</span><span class="p">,</span> <span class="s2">&#34;health&#34;</span><span class="p">,</span> <span class="s2">&#34;baby&#34;</span><span class="p">,</span> <span class="s2">&#34;toy&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="s2">&#34;nong&#34;</span><span class="p">,</span> <span class="s2">&#34;jiu&#34;</span><span class="p">,</span> <span class="s2">&#34;fresh&#34;</span><span class="p">,</span> <span class="s2">&#34;china&#34;</span><span class="p">,</span> <span class="s2">&#34;che&#34;</span><span class="p">,</span> <span class="s2">&#34;list&#34;</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="提取数据"><span class="section-num">2</span> 提取数据</h2>
<p>在确定了爬取策略之后，爬虫就可以不断地进行工作了。那么爬虫怎么知道什么时候才是商品信息的页面呢？再来分析一下京东的商品页面：</p>

<figure><a href="/ox-hugo/jd_item_analyze.png">
    
    
    <input type="checkbox" id="zoomCheck-c8137" hidden>
    <label for="zoomCheck-c8137">
    
    
    <img class="zoomCheck" loading="lazy" src="/ox-hugo/jd_item_analyze.png"/> 
    
    
    </label></a>
</figure>

<p>从上面的信息可以看出，每个商品的页面都是以 <code>item.jd.com/xxxxxxx.html</code> 的形式存 在的；而 xxxxxxx 就是该商品的 sku-id. 所以只需对 url 进行解析，子域名为 <code>item</code> 即商品页面，就可以进行爬取。</p>
<p>页面提取使用 Xpath 即可，也无需赘言。不过，需要注 意的是对商品而言，非常重要的价格就不是可以通过爬取 HTML 页面得到的。</p>
<p>因为价格是经常变动的，所以是异步向后台请求的。对于这些异步请求的数据，打开控制台，然后刷新，就可以看到一堆的 JS 文件，然后寻找相应的请求带有 &ldquo;money 或者price&rdquo; 之类关 键字的 JS 文件，应该就能找到。</p>
<p>如果还没办法找出来的话，Firefox 上有一个 <a href="https://addons.mozilla.org/en-US/firefox/addon/user-agent-switcher/">user-agent-switcher</a> 的扩展，然后通过这个扩展把自己的浏览器伪装成 IE6, 相信所有
花俏的 JS 都会没了, 只剩下那些不可或缺的 JS, 这样结果应该一目了然了，这么看来 IE6 还是有用滴。最终找到的URL 如下
<code>https://p.3.cn/prices/mgets?callback=jQuery6646724&amp;type=1&amp;area=19_1601_3633_0.137875165&amp;pdtk=9D4RIAHY317A3bZnQNapD7ip5Dg%252F6NXiIXt90Ahk0if2Yyh39PZQCuDBlhN%252FxOch3MpwWpHICu4P%250AVcgcOm11GQ%253D%253D&amp;pduid=14966417675252009727775&amp;pdpin=%25E5%2585%2591%25E9%2587%2591%25E8%25BE%25B0%25E6%2589%258B&amp;pdbp=0&amp;skuIds=J_3356012&amp;ext=10000000&amp;source=item-pc</code></p>
<p>不得不说，URL 实在是太长了。</p>
<p>根据经验，大部分的参数应该都是没什么用的，应该可以去掉的，所以在浏览器就一个个参数去掉，然后试试请求是否成功，如果成功，说明此参数无关重要，最后简化成： <code>http://p.3.cn/prices/mgets?pduid={}&amp;skuIds=J_{}</code> sku_id 即商品页面的 URL中包含的数字，而 pduid 则是一随机整数而已，用
<code>random.randint(1, 100000000)</code> 函数解决。</p>
<h2 id="商品评论"><span class="section-num">3</span> 商品评论</h2>
<p>商品的评论也是以 sku-id 为参数通过异步的方式进行请求的，构造请求的方法跟价格类 似，也不需过多赘述。</p>
<p>只是想要吐嘈一下的是，京东的评论是只能一页页向后翻的，不能跳转。还有一点就是，即使某样商品有 10+w 条评论，最多也只是返回 100页的数据。
略坑</p>
<h2 id="反爬虫策略"><span class="section-num">4</span> 反爬虫策略</h2>
<p>商品的爬取策略以及提取策略都确定了，一只爬虫就基本成型了。但是一般比较大型的网站都有反爬虫措施的。所以道高一尺，魔高一丈，爬虫也要有对应的反反爬虫策略</p>
<h3 id="禁用-cookie"><span class="section-num">4.1</span> 禁用 cookie</h3>
<p>通过禁用 cookie, 服务器就无法根据 cookie 判断出爬虫是否访问过网站</p>
<h3 id="轮转-user-agent"><span class="section-num">4.2</span> 轮转 user-agent</h3>
<p>一般的爬虫都会使用浏览器的 user-agent 来模拟浏览器以欺骗服务器 (当然，如果你是一只什么 user-agent都不用耿直的小爬虫，我也无话可说).</p>
<p>为了提高突破反爬虫策略的成功率，可以定义多个 user-agent, 然后每次请求都随机选择 user-agent。</p>
<h3 id="伪装成搜索引擎"><span class="section-num">4.3</span> 伪装成搜索引擎</h3>
<p>要说最著名的爬虫是谁？肯定是搜索引擎，它本质上也是爬虫，而且是非常强大的爬虫。</p>
<p>而且这些爬虫可以光明正大地去爬取各式网站，相信各式网站也很乐意被它爬。</p>
<p>那么， 现在可以通过修改 user-agent 伪装成搜索引擎，然后再结合上面的轮转 user-agent,</p>
<p>伪装成各式搜索引擎：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&#39;Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)&#39;,
</span></span><span class="line"><span class="cl">&#39;Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)&#39;,
</span></span><span class="line"><span class="cl">&#39;Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)&#39;,
</span></span><span class="line"><span class="cl">&#39;DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)&#39;,
</span></span><span class="line"><span class="cl">&#39;Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)&#39;,
</span></span><span class="line"><span class="cl">&#39;Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)&#39;,
</span></span><span class="line"><span class="cl">&#39;ia_archiver (+http://www.alexa.com/site/help/webmasters; crawler@alexa.com)&#39;,
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="代理-ip"><span class="section-num">4.4</span> 代理 IP</h3>
<p>虽说可以伪装成搜索引擎，但是因为 http 请求是建立在三次握手之上的，爬虫的 IP 还是会被记录下来的，如果同一个 IP 访问得太频繁，那基本就可以确定是一只爬虫了，然后就把它的 IP 封掉，温和一点的就会叫你输入验证码，不然就返回 403.</p>
<p>对待这种情况，就需要使用代理 IP 了。</p>
<p>只是代理 IP 都有不同程度的延迟，并且免费的 IP 大多不能用，所以这是不得而为之了</p>
<h2 id="扩展成分布式爬虫"><span class="section-num">5</span> 扩展成分布式爬虫</h2>
<p>一台机器的爬虫可能爬取一个网站可能需要 100 天，而且带宽也到达瓶颈了，那么是否可以提高爬取效率呢？</p>
<p>那就用 100台机器，1天应该就能爬取完 (当然，现实并非如此美好).</p>
<p>这个就涉及到分布式的爬虫的问题。而不同的分布式爬虫有不同的实现方法，而我选择了 scrapy 和 redis 整合的 <a href="https://github.com/rolando/scrapy-redis">scrapy-redis</a> 来实现分布式，URL 的去重以及调度都有了相应的实现了，也无需额外的操心</p>
<h2 id="爬虫监控"><span class="section-num">6</span> 爬虫监控</h2>
<p>既然爬虫从单机变成了分布式，新的问题随之而来：如何监控分布式爬虫呢？在单机的时候，最简单的监控 &ndash; 直接将爬虫的日志信息输出到终端即可。</p>
<p>但是对于分布式爬虫，这样的做法显然不现实。我最终选择使用 <a href="https://graphiteapp.org">graphite</a> 这个监控工具。</p>
<h3 id="scrapy-graphite"><span class="section-num">6.1</span> scrapy-graphite</h3>
<p>参考 Github上 <a href="https://github.com/gnemoug/distribute_crawler">distributed_crawler</a> 的代码，将单机版本的 <a href="https://github.com/noplay/scrapy-graphite">scrapy-graphite</a> 扩展成基于分布式的 graphite 监控程序，并且实现对 python3 的支持。</p>
<h3 id="docker"><span class="section-num">6.2</span> docker</h3>
<p>但是 graphite 只是支持 python2, 并且安装过程很麻烦，我在折腾大半天后都无法安装成功，实在有点沮丧。最后想起了伟大的 docker, 并且直接找到已经打包好的image. 数行命令即解决所有的安装问题，不得不说：docker, 你值得拥有。运行截图：</p>

<figure><a href="/ox-hugo/jd_comment_graphite1.png">
    
    
    <input type="checkbox" id="zoomCheck-cd205" hidden>
    <label for="zoomCheck-cd205">
    
    
    <img class="zoomCheck" loading="lazy" src="/ox-hugo/jd_comment_graphite1.png"/> 
    
    
    </label></a>
</figure>


<figure><a href="/ox-hugo/jd_comment_graphite2.png">
    
    
    <input type="checkbox" id="zoomCheck-97ae9" hidden>
    <label for="zoomCheck-97ae9">
    
    
    <img class="zoomCheck" loading="lazy" src="/ox-hugo/jd_comment_graphite2.png"/> 
    
    
    </label></a>
</figure>

<h2 id="爬虫拆分"><span class="section-num">7</span> 爬虫拆分</h2>
<p>本来爬取商品信息的爬虫和爬取评论的爬虫都是同一只爬虫，但是后来发现，再不使用代理 IP 的情况下，爬取到 150000 条商品信息的时候，需要输入验证码。</p>
<p>但是爬取商品评论的爬虫并不存在被反爬策略限制的情况。所以我将爬虫拆分成两只爬虫，即使无法爬取商品信息的时候，还可以爬取商品的评论信息。</p>
<h2 id="小结"><span class="section-num">8</span> 小结</h2>
<p>在爬取一天之后，爬虫成果：</p>
<h3 id="评论"><span class="section-num">8.1</span> 评论</h3>

<figure><a href="/ox-hugo/jd_comment.png">
    
    
    <input type="checkbox" id="zoomCheck-8be27" hidden>
    <label for="zoomCheck-8be27">
    
    
    <img class="zoomCheck" loading="lazy" src="/ox-hugo/jd_comment.png"/> 
    
    
    </label></a>
</figure>

<h3 id="评论总结"><span class="section-num">8.2</span> 评论总结</h3>

<figure><a href="/ox-hugo/jd_comment_summary.png">
    
    
    <input type="checkbox" id="zoomCheck-fc61f" hidden>
    <label for="zoomCheck-fc61f">
    
    
    <img class="zoomCheck" loading="lazy" src="/ox-hugo/jd_comment_summary.png"/> 
    
    
    </label></a>
</figure>

<h3 id="商品信息"><span class="section-num">8.3</span> 商品信息</h3>

<figure><a href="/ox-hugo/jd_parameters.png">
    
    
    <input type="checkbox" id="zoomCheck-29f68" hidden>
    <label for="zoomCheck-29f68">
    
    
    <img class="zoomCheck" loading="lazy" src="/ox-hugo/jd_parameters.png"/> 
    
    
    </label></a>
</figure>

<p>商品信息加上评论数约 150+w.</p>
<h2 id="参考及致谢"><span class="section-num">9</span> 参考及致谢</h2>
<ul>
<li><a href="https://github.com/noplay/scrapy-graphite">https://github.com/noplay/scrapy-graphite</a></li>
<li><a href="https://github.com/gnemoug/distribute_crawler">https://github.com/gnemoug/distribute_crawler</a></li>
<li><a href="https://github.com/hopsoft/docker-graphite-statsd">https://github.com/hopsoft/docker-graphite-statsd</a></li>
</ul>
<h2 id="项目源码"><span class="section-num">10</span> 项目源码</h2>
<p><a href="https://github.com/samrayleung/jd_spider">https://github.com/samrayleung/jd_spider</a></p>
<div center class="qr-container">
<img src="/ox-hugo/qrcode_gh_e06d750e626f_1.jpg" alt="qrcode_gh_e06d750e626f_1.jpg" width="160px" height="160px" center="t" class="qr-container" />
公号同步更新，欢迎关注👻
</div>
]]></content:encoded>
    </item>
    <item>
      <title>爬虫高效去重之布隆过滤器</title>
      <link>http://localhost:1313/zh/post/2017/bloom_filter/</link>
      <pubDate>Sun, 09 Apr 2017 00:00:00 +0800</pubDate>
      <guid>http://localhost:1313/zh/post/2017/bloom_filter/</guid>
      <description>an dscription about bloom filter</description>
      <content:encoded><![CDATA[<p>笔者最近思考如何编写高效的爬虫; 而在编写高效爬虫的时候，有一个必需解决的问题就是：
url 的去重，即如何判别 url 是否已经被爬取，如果被爬取，那就不要重复爬取。</p>
<p>一般如果需要爬取的网站不是非常庞大的话，使用Python 内置的 set 就可以实现去重了，但是使用 set 内存利用率不高，此外对于那些不像Python 那样用 hash 实现的 set 而言，时间复杂度是 log(N),实在难说高效。</p>
<h2 id="bloom-filter"><span class="section-num">1</span> Bloom Filter</h2>
<p>那么如何实现高效的去重呢？ 笔者查阅资料之后得知：使用布隆过滤器 (Bloom Filter).</p>
<p>布隆过滤器可以用于快速检索一个元素是否在一个集合中。布隆过滤器实际上是一个很长的二进制向量和一系列随机映射函数（Hash函数）。</p>
<p>而一般的判断一个元素是否在一个集合里面的做法是：用需要判断的元素和集合中的元素进行比较，一般的数据结构，例如链表，树，都是这么实现的。</p>
<p>缺点是：随着集合元素的增多，需要比较的元素也增多，检索速度就越来越慢。</p>
<p>而使用布隆过滤器判重可以实现常数级的时间复杂度(检索时间不随元素增长而增加).那么布隆过滤器又是怎样实现的呢</p>
<h3 id="布隆过滤器实现原理"><span class="section-num">1.1</span> 布隆过滤器实现原理</h3>
<p>一个Bloom Filter是基于一个m位的位向量（Bit Vector），这些位向量的初始值为0, 并且有一系列的 hash 函数，hash 函数值域为1-m.在下面例子中，是15位的位向量，初始值为0以空白表示，为1以颜色填充</p>

<figure><a href="/ox-hugo/bit_vector.png">
    
    
    <input type="checkbox" id="zoomCheck-a88b8" hidden>
    <label for="zoomCheck-a88b8">
    
    
    <img class="zoomCheck" loading="lazy" src="/ox-hugo/bit_vector.png"/> 
    
    
    </label></a>
</figure>

<p>现在有两个简单的 hash 函数：fnv,murmur.现在我输入一个字符串 &ldquo;whatever&rdquo; ,然后分别使用两个 hash 函数对 &ldquo;whatever&rdquo; 进行散列计算并且映射到上面的位向量。</p>

<figure><a href="/ox-hugo/whatever.png">
    
    
    <input type="checkbox" id="zoomCheck-395b7" hidden>
    <label for="zoomCheck-395b7">
    
    
    <img class="zoomCheck" loading="lazy" src="/ox-hugo/whatever.png"/> 
    
    
    </label></a>
</figure>

<p>可知，使用 fnv 函数计算出的 hash 值是11,使用 murmur 函数计算出的 hash 值是4. 然后映射到位向量上：</p>

<figure><a href="/ox-hugo/bit_vector1.png">
    
    
    <input type="checkbox" id="zoomCheck-05bbd" hidden>
    <label for="zoomCheck-05bbd">
    
    
    <img class="zoomCheck" loading="lazy" src="/ox-hugo/bit_vector1.png"/> 
    
    
    </label></a>
</figure>

<p>如果下一次，笔者要判断 <strong>whatever</strong> 是否在字符串中，只需使用 fnv 和 murmur 两个 hash 函数对 <strong>whatever</strong> 进行散列值计算，然后与位向量做 &ldquo;与运算&rdquo;，如果结果为0, 那么说明 <strong>whatever</strong> 是不在集合中的，因为同样的元素使用同一个 hash 函数产生的值每次都是相同的，不相同就说明不是同一个元素。</p>
<p>但是如果 &ldquo;与运算&rdquo; 的结果为1,是否可以说明 <strong>whatever</strong> 就在集合中呢？其实上是不能100% 确定的，因为 hash 函数存在散列冲突现象 (即两个散列值相同，但两个输入值是不同的), 所以布隆过滤器只能说&quot;我可以说这个元素我在集合中是看见过滴，只是我有一定的不确定性&quot;.</p>
<p>当你在分配的内存足够大之后，不确定性会变得很小很小。</p>
<p>你可以看到布隆过滤器可以有效利用内存实现常数级的判重任务，但是鱼和熊掌不可得兼，付出的代价就是一定的误判 (机率很小),所以本质上，布隆过滤器是 &ldquo;概率数据结构 (probabilistic data structure)&rdquo;.</p>
<p>这个就是布隆过滤器的基本原理。当然，位向量不会只是15位，hash函数也不会仅是两个简单的函数. 这只是简化枝节，为了清晰解述原理而已。</p>
<h2 id="python-bloomfilter"><span class="section-num">2</span> Python BloomFilter</h2>
<p>算法都是为了实际问题服务的，又回到爬虫这个话题上。在了解布隆过滤器原理之后，可以很容易地实现自己的布隆过滤器，但是想要实现一个高效健壮的布隆过滤器就需要比较多的功夫了，因为需要考虑的问题略多。</p>
<p>幸好，得益Python 强大的社区，已经有<a href="https://axiak.github.io/pybloomfiltermmap/">Python BloomFilter</a> 的库。一个文档中的简单例子：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pybloomfilter</span> <span class="kn">import</span> <span class="n">BloomFilter</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">bf</span> <span class="o">=</span> <span class="n">BloomFilter</span><span class="p">(</span><span class="mi">10000000</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;filter.bloom&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&#34;/usr/share/dict/words&#34;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">	<span class="n">bf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">rstrip</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span> <span class="s1">&#39;apple&#39;</span> <span class="ow">in</span> <span class="n">bf</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>结果为 <strong>True</strong></p>
<h2 id="总结"><span class="section-num">3</span> 总结</h2>
<p>原理就说得差不多了，要想对布隆过滤器有更深的认识，还需要更多的实战。多写，多思考。 Enjoy Python,Enjoy Crawler :)</p>
<h2 id="参考"><span class="section-num">4</span> 参考</h2>
<ul>
<li><a href="https://llimllib.github.io/bloomfilter-tutorial/">https://llimllib.github.io/bloomfilter-tutorial/</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bloom_filter">https://en.wikipedia.org/wiki/Bloom_filter</a></li>
</ul>
<div center class="qr-container">
<img src="/ox-hugo/qrcode_gh_e06d750e626f_1.jpg" alt="qrcode_gh_e06d750e626f_1.jpg" width="160px" height="160px" center="t" class="qr-container" />
公号同步更新，欢迎关注👻
</div>
]]></content:encoded>
    </item>
  </channel>
</rss>
