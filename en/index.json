[{"content":"1 Prologue A while ago, I took a flight from Canada back to Hong Kong - about 12 hours in total with Air Canada.\nInterestingly, the plane actually had WiFi:\nHowever, the WiFi had restrictions. For Aeroplan members who hadn\u0026rsquo;t paid, it only offered Free Texting, meaning you could only use messaging apps like WhatsApp, Snapchat, and WeChat to send text messages, but couldn\u0026rsquo;t access other websites.\nIf you wanted unlimited access to other websites, it would cost CAD $30.75:\nAnd if you wanted to watch videos on the plane, that would be CAD $39:\nI started wondering: for the Free Texting service, could I bypass the messaging app restriction and access other websites freely?\nEssentially, could I enjoy the benefits of the $30.75 paid service without actually paying the fee? After all, with such a long journey ahead, I needed something interesting to pass the 12 hours.\nSince I could use WeChat in flight, I could also call for help from the sky.\nCoincidentally, my roommate happens to be a security and networking expert who was on vacation at home. When I mentioned this idea, he thought it sounded fun and immediately agreed to collaborate. So we started working on it together across the Pacific.\n2 The Process After selecting the only available WiFi network acwifi.com on the plane, just like other login-required WiFi networks, it popped up a webpage from acwifi.com asking me to verify my Aeroplan membership. Once verified, I could access the internet.\nThere\u0026rsquo;s a classic software development interview question: what happens after you type a URL into the browser and press enter?\nFor example, if you type https://acwifi.com and only focus on the network request part, the general process is: DNS query -\u0026gt; TCP connection -\u0026gt; TLS handshake -\u0026gt; HTTP request and response.\nLet\u0026rsquo;s consider github.com as our target website we want to access. Now let\u0026rsquo;s see how we can break through the network restrictions and successfully access github.com.\n3 Approach 1: Disguise Domain Since acwifi.com is accessible but github.com is not, is it possible that the network has imposed restrictions on the DNS server, only resolving domain names within a whitelist (such as instant messaging domains)?\nIf this is the case, can I modify /etc/hosts to disguise my server as acwifi.com, so that all request traffic passes through my server before reaching the target website (github.com)? For example:\nThe general idea is that I modify the DNS record to bind our proxy server\u0026rsquo;s IP 137.184.231.87 to acwifi.com. Since the local /etc/hosts file takes precedence over the DNS server, I can then use a self-signed certificate to tell the browser that this IP is bound to this domain and that it should trust it.\nLet me first test this idea:\n1 2 3 4 5 6 7 8 9 10 \u0026gt; ping 137.184.231.87 PING 137.184.231.87 (137.184.231.87): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Request timeout for icmp_seq 2 Request timeout for icmp_seq 3 Request timeout for icmp_seq 4 ^C --- 137.184.231.87 ping statistics --- 6 packets transmitted, 0 packets received, 100.0% packet loss Unexpectedly, the IP was completely unreachable via ping, meaning the IP was likely blocked entirely.\nI tried other well-known IPs, like Cloudflare\u0026rsquo;s CDN IP, and they were also unreachable:\n1 2 3 4 5 6 7 8 9 10 \u0026gt; ping 172.67.133.121 PING 172.67.133.121 (172.67.133.121): 56 data bytes Request timeout for icmp_seq 0 Request timeout for icmp_seq 1 Request timeout for icmp_seq 2 Request timeout for icmp_seq 3 Request timeout for icmp_seq 4 ^C --- 172.67.133.121 ping statistics --- 6 packets transmitted, 0 packets received, 100.0% packet loss It seems this approach won\u0026rsquo;t work. This approach might only work if:\nThe DNS server only answers queries for a specific list of domain names (e.g., WhatsApp, Snapchat, WeChat), which means the firewall\u0026rsquo;s filtering mechanism was solely based on DNS resolution. The network allows connections to arbitrary IP addresses After all, if the IPs are directly blocked, no amount of disguise will help. This network likely maintains some IP whitelist (such as WhatsApp and WeChat\u0026rsquo;s egress IPs), and only IPs on the whitelist can be accessed.\nIf a ping to a specific IP times out, I wouldn\u0026rsquo;t say the IP is blocked. It could be that ICMP specifically is blocked, following some network rules on the firewall. This is pretty common in entreprise networks to not allow endpoint discovery. I could be missing something and happy to be corrected here, but I was surprised to read that. \u0026ndash; HackerNews top comment\nhttps://news.ycombinator.com/item?id=45536325\nActually, I did verified whether only ICMP was blocked, was it possible to create a connection through TLS:\n1 2 3 4 5 6 7 8 \u0026gt; curl -Lkv https://172.67.133.121 * Trying 172.67.133.121:443... * Connected to 172.67.133.121 (172.67.133.121) port 443 * ALPN: curl offers h2,http/1.1 * (304) (OUT), TLS handshake, Client hello (1): * LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to 172.67.133.121:443 * Closing connection curl: (35) LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to 172.67.133.121:443 However, it turned out that both ICMP and TLS were blocked\n4 Approach 2: DNS Port Masquerading When the first approach failed, my roommate suggested a second approach: try using DNS service as a breakthrough:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026gt; dig http418.org ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; http418.org ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 64160 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;http418.org.\tIN\tA ;; ANSWER SECTION: http418.org.\t300\tIN\tA\t172.67.133.121 http418.org.\t300\tIN\tA\t104.21.5.131 ;; Query time: 3288 msec ;; SERVER: 172.19.207.1#53(172.19.207.1) ;; WHEN: Sat Oct 04 14:18:24 PDT 2025 ;; MSG SIZE rcvd: 94 This is good news! It means there are still ways to reach external networks, and DNS is one of them.\nLooking at the record above, it shows our DNS query for http418.org was successful, meaning DNS requests work.\n4.1 Arbitrary DNS Servers My roommate then randomly picked another DNS server to see if the network had a whitelist for DNS servers:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026gt; dig @40.115.144.198 http418.org ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; @40.115.144.198 http418.org ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 58958 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1224 ;; QUESTION SECTION: ;http418.org.\tIN\tA ;; ANSWER SECTION: http418.org.\t275\tIN\tA\t104.21.5.131 http418.org.\t275\tIN\tA\t172.67.133.121 ;; Query time: 1169 msec ;; SERVER: 40.115.144.198#53(40.115.144.198) ;; WHEN: Sat Oct 04 14:24:25 PDT 2025 ;; MSG SIZE rcvd: 72 We can actually use arbitrary DNS servers - even better!\n4.2 TCP Queries The fact that arbitrary DNS servers can be queried successfully is excellent news. DNS typically uses UDP protocol, but would TCP-based DNS requests be blocked?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026gt; dig @40.115.144.198 http418.org +tcp ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; @40.115.144.198 http418.org +tcp ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 30355 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1224 ;; QUESTION SECTION: ;http418.org.\tIN\tA ;; ANSWER SECTION: http418.org.\t36\tIN\tA\t172.67.133.121 http418.org.\t36\tIN\tA\t104.21.5.131 ;; Query time: 4679 msec ;; SERVER: 40.115.144.198#53(40.115.144.198) ;; WHEN: Sat Oct 04 14:28:24 PDT 2025 ;; MSG SIZE rcvd: 72 DNS TCP queries also work! This indicates the plane network\u0026rsquo;s filtering policy is relatively lenient, standing a chance of our subsequent DNS tunneling approach.\n4.3 Proxy Service on Port 53 It seems the plane network restrictions aren\u0026rsquo;t completely airtight - we\u0026rsquo;ve found a \u0026ldquo;backdoor\u0026rdquo; in this wall.\nSo we had a clever idea: since the plane gateway doesn\u0026rsquo;t block DNS requests, theoretically we could disguise our proxy server as a DNS server, expose port 53 for DNS service, route all requests through the proxy server disguised as DNS requests, and thus bypass the restrictions.\nMy roommate spent about an hour setting up a proxy server exposing port 53 using xrayÂ 1, and sent me the configuration via WeChat:\nThe proxy server configuration my roommate set up with Xray included the following sample configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 { \u0026#34;outbounds\u0026#34;: [ { \u0026#34;tag\u0026#34;: \u0026#34;proxy\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;vless\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;vnext\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;our-proxy-server-domain\u0026#34;, \u0026#34;port\u0026#34;: 53, \u0026#34;users\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;some-uuid\u0026#34;, \u0026#34;flow\u0026#34;: \u0026#34;xtls-rprx-vision\u0026#34;, \u0026#34;encryption\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;level\u0026#34;: 0 } ] } ] }, \u0026#34;streamSettings\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;security\u0026#34;: \u0026#34;tls\u0026#34;, \u0026#34;tlsSettings\u0026#34;: { \u0026#34;allowInsecure\u0026#34;: false, \u0026#34;allowInsecureCiphers\u0026#34;: false, \u0026#34;alpn\u0026#34;: [ \u0026#34;h2\u0026#34; ] } } }, { \u0026#34;tag\u0026#34;: \u0026#34;direct\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;freedom\u0026#34; }, { \u0026#34;tag\u0026#34;: \u0026#34;block\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;blackhole\u0026#34; } ] } And I already had an xray client on my computer, so no additional software was needed to establish the connection.\nEverything was ready. The exciting moment arrived - pressing enter to access github.com:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 /Users/ramsayleung [ramsayleung@ramsayleungs-Laptop] [18:28] \u0026gt; curl -v github.com -x socks5://127.0.0.1:10810 * Trying 127.0.0.1:10810... * Connected to 127.0.0.1 (127.0.0.1) port 10810 * SOCKS5 connect to 172.19.1.1:80 (locally resolved) * SOCKS5 request granted. * Connected to 127.0.0.1 (127.0.0.1) port 10810 \u0026gt; GET / HTTP/1.1 \u0026gt; Host: github.com \u0026gt; User-Agent: curl/8.4.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 301 Moved Permanently \u0026lt; Content-Length: 0 \u0026lt; Location: https://github.com/ \u0026lt; * Connection #0 to host 127.0.0.1 left intact /Users/ramsayleung [ramsayleung@ramsayleungs-Laptop] [18:28] \u0026gt; curl -v github.com -x socks5://127.0.0.1:10810 * Trying 127.0.0.1:10810... * Connected to 127.0.0.1 (127.0.0.1) port 10810 * SOCKS5 connect to 172.19.1.1:80 (locally resolved) * SOCKS5 request granted. * Connected to 127.0.0.1 (127.0.0.1) port 10810 \u0026gt; GET / HTTP/1.1 \u0026gt; Host: github.com \u0026gt; User-Agent: curl/8.4.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 301 Moved Permanently \u0026lt; Content-Length: 0 \u0026lt; Location: https://github.com/ \u0026lt; * Connection #0 to host 127.0.0.1 left intact The request actually succeeded! github.com returned a successful result!\nThis means we\u0026rsquo;ve truly broken through the network restrictions and can access any website!\nWe hadn\u0026rsquo;t realized before that xray could be used in this clever way :)\nHere we exploited a simple cognitive bias: not all services using port 53 are DNS query requests.\n5 Ultimate Approach: DNS Tunnel If Approach 2 still didn\u0026rsquo;t work, we had one final trick up our sleeves.\nCurrently, the gateway only checks whether the port is 53 to determine if it\u0026rsquo;s a DNS request. But if the gateway were stricter and inspected the content of DNS request packets, it would discover that our requests are \u0026ldquo;disguised\u0026rdquo; as DNS queries rather than genuine DNS queries:\nSince disguised DNS requests would be blocked, we could embed all requests inside genuine DNS request packets, making them DNS TXT queries. We\u0026rsquo;d genuinely be querying DNS, just with some extra content inside:\nHowever, this ultimate approach requires a DNS Tunnel client to encapsulate all requests. I didn\u0026rsquo;t have such software on my computer, so this remained a theoretical ultimate solution that couldn\u0026rsquo;t be practically verified.\n6 Conclusion With the long journey ahead, my roommate and I spent about 4 hours remotely breaking through the network restrictions, having great fun in the process, proving that our problem-solving approach was indeed feasible.\nThe successful implementation of the solution was mainly thanks to my roommate, the networking expert, who provided remote technical and conceptual support.\nThe only downside was that although we broke through the network restrictions and could access any website, the plane\u0026rsquo;s bandwidth was extremely limited, making web browsing quite painful. So I didn\u0026rsquo;t spend much time browsing the web.\nFor the remaining hours, I rewatched the classic 80s time-travel movie: \u0026quot;Back to the Future\u0026quot; , which was absolutely fantastic.\nLast and not least, it\u0026rsquo;s the disclaimer:\nThis technical exploration is intended solely for educational and research purposes. We affirm our strict adherence to all relevant regulations and service terms throughout this project.\nDiscuss this post on HackerNews\nhttps://github.com/XTLS/Xray-core\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ramsayleung.github.io/en/post/2025/a_story_about_bypassing_air_canadas_in-flight_network_restrictions/","summary":"\u003ch2 id=\"prologue\"\u003e\u003cspan class=\"section-num\"\u003e1\u003c/span\u003e Prologue\u003c/h2\u003e\n\u003cp\u003eA while ago, I took a flight from Canada back to Hong Kong - about 12 hours in total with Air Canada.\u003c/p\u003e\n\u003cp\u003eInterestingly, the plane actually had WiFi:\u003c/p\u003e\n\n\u003cfigure\u003e\n    \n    \n    \u003cinput type=\"checkbox\" id=\"zoomCheck-a931d\" hidden\u003e\n    \u003clabel for=\"zoomCheck-a931d\"\u003e\n    \n    \n    \u003cimg class=\"zoomCheck\" loading=\"lazy\" src=\"/ox-hugo/acwifi-connect-2.png\"/\u003e \n    \n    \n    \u003c/label\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHowever, the WiFi had restrictions. For Aeroplan members who hadn\u0026rsquo;t paid, it only offered \u003ca href=\"https://www.aircanada.com/ca/en/aco/home/fly/onboard/in-flight-entertainment-and-connectivity.html#/\"\u003eFree Texting\u003c/a\u003e, meaning you could only use messaging apps like WhatsApp, Snapchat, and WeChat to send text messages, but couldn\u0026rsquo;t access other websites.\u003c/p\u003e\n\u003cp\u003eIf you wanted unlimited access to other websites, it would cost CAD $30.75:\u003c/p\u003e","title":"A Story About Bypassing Air Canada's In-flight Network Restrictions"},{"content":"ä¸­æçæ¬\n1 Preface I spent a weekend building a Telegram spam blocker bot based on Bayesian Algorithm @BayesSpamSniperBot (https://t.me/BayesSpamSniperBot). The project is open-sourced at: https://github.com/ramsayleung/bayes_spam_sniper\n1.1 Telegram Telegram is a popular instant messaging application, similar to Snapchat and WhatsApp, with over 1 billion users.\nIt supports many powerful features like cloud chat history storage, clients for Linux, Mac, Windows, Android, IOS, and Web (all open-source), Channel, and arguably the most powerful bot system I\u0026rsquo;ve ever seen.\n2 Origin I usually listen to podcasts while running and cooking. ãè½¯ä»¶é£äºäºå¿(A podcast in Chinese about history and story behind software)ã1 is one of my favorites, hosted by æ å¥Â 2. Because I enjoyed æ å¥\u0026rsquo;s show, I took the chance to join his Telegram channel.\næ å¥\u0026rsquo;s Telegram channel æ±çåæ Â 3 is primarily used for releasing podcast information. He once enabled the comment for channel, but it unexpectedly attracted a flood of crypto-related users posting spam, leading him to disable comments:\nAnother channel I subscribe, Ray TracingÂ 4, also complained about the crypto spam:\n3 Hackers \u0026amp; Painters Most common Telegram spam blocker bots are keyword-based, blocking messages by matching keywords, which can be easily bypassed by spammers.\nIf the messages get bypassed, it could only be deleted manually by administrator.\nThis reminded me of the situation Paul Graham described in his 2002 essay within \u0026ldquo;Hackers \u0026amp; Painters\u0026rdquo;:\nWhen email became popular, there was also a lot of spam. Common spam blockers were keyword matching + email address blacklists, but these were inefficient and easily circumvented.\nPaul Graham creatively used Bayesian Theorem to implement a spam blockerÂ 5, and the results were surprisingly effective.\nIsn\u0026rsquo;t this a similar problem for Telegram spam?\nCouldn\u0026rsquo;t I use a similar solution to tackle Telegram spam?\n3.1 Bayes\u0026rsquo; Theorem When it comes to probabilistic algorithms, the most classic example is the \u0026ldquo;coin toss\u0026rdquo; â a case of classical probability where each toss is an independent event, and the previous outcome doesn\u0026rsquo;t affect the next probability.\nHowever, many real-world scenarios aren\u0026rsquo;t infinitely repeatable like coin tosses, and events are often not independent.\nThis is where Bayes Theorem shows its unique value.\nIt used to update our degree of belief in a hypothesis given certain evidence.\nIn other words, the Bayes algorithm can dynamically adjust the estimated probability of an event occurring based on continuously emerging new evidence.\nSimply put, it\u0026rsquo;s like the human brain\u0026rsquo;s learning process: we start with a preliminary understanding, then revise our original view based on new information, thereby adjusting our next actions.\nPaul Graham used Bayes theorem to continuously classify new emails as spam or not based on emails already identified as spam or non-spam (ham).\nTo understand Bayes Theorem more intuitively, I recommend this clear and easy-to-understand videos:\nãBayes theorem, the geometry of changing beliefsã6 4 Architecture Design Telegram Bot supports two modes of interacting with Telegram servers:\nWebhook: Telegram servers actively callback a URL previously registered by the Bot whenever the Bot receives a new message. The Bot Server only needs to handle the callback messages.\nLong Polling: The Bot Server continuously polls the Telegram servers to check for new messages and processes them if any. This bot uses this mode.\n4.0.1 Message Analysis After the Bot Server receives a message, it dispatches it to a separate telegram_bot_worker for processing. Based on the pre-trained model, it judges whether it\u0026rsquo;s a spam. If it is, it calls the Bot API to delete the message.\n4.0.2 Ban and Train After the Bot Server receives a message, it dispatches it to a separate telegram_bot_worker for processing. The telegram_bot_worker calls the bot API to delete the message and ban the user, and inserts a training data record marked as spam.\nSaving the training data triggers a hook, creating a training message delivered to the training message queue. Another worker, classifier_trainer, subscribes to training messages and uses the new messages to retrain and update the model.\nUsing a queue and a background process (classifier_trainer) for training tasks, instead of directly using the telegram_bot_worker, primarily decouples the Bot request handling from model training. Otherwise, as the model size increases, training time would get longer and longer, leading to increasingly long response times.\nDecoupling makes it easy to scale.\n5 Why Rails whoever have seen my project source code might wonder, why was it implemented using Ruby on Rails?\nBecause I work with JVM languages (Java/Kotlin/Scala) and Rust, I\u0026rsquo;m quite familiar with Java/Rust. Initially, thinking model training might require high performance, my first prototypeÂ 7 was implemented in Rust, taking about half an hour.\nBut when I wanted to expand the prototype into a Telegram bot, I found I needed to handle a lot of logic related to bot interaction, mainly involving API and database operations, most of which were unrelated to the model. So, I thought of Ruby on Rails again.\nFor a single engineer building a product prototype, in my personal opinion, there\u0026rsquo;s really no framework more efficient than Ruby on Rails, so I switched to Ruby on Rails.\nNew features in Rails 8 move it further towards being a so-called \u0026ldquo;one-person full-stack framework,\u0026rdquo; with built-in support for Solid Queue via the relational database.\nThe queue and background process from the architecture design were implemented with just a few lines of code, without even needing extra configuration. If the queue doesn\u0026rsquo;t exist, the framework creates it automatically:\n1 2 3 4 5 6 7 8 class ClassifierTrainerJob \u0026lt; ApplicationJob # Job to train classifier asynchronously queue_as :training def perform(group_id, group_name) SpamClassifierService.rebuild_for_group(group_id, group_name) end end Thanks to Rails\u0026rsquo; powerful ORM framework and its built-in lifecycle hooks, the code to trigger the background process for retraining the model after inserting new training data is also just a few lines:\n1 2 3 4 5 6 7 8 9 10 class TrainedMessage \u0026lt; ApplicationRecord # Automatically train classifier after creating/updating a message after_create :retrain_classifier after_destroy :retrain_classifier def retrain_classifier # For efficiency, we could queue this as a background job ClassifierTrainerJob.perform_later(group_id, group_name) end end Empowered by Rails\u0026rsquo; various built-in powerful tools, I implemented the entire bot\u0026rsquo;s functionality in just one day.\nSeeing this, some friends might worry about performance, thinking Ruby\u0026rsquo;s performance isn\u0026rsquo;t great, and it\u0026rsquo;s a dynamic language, making it hard to maintain.\nMy view remains the same as in my previous blog post ãThoughts on a Decade of Programming(In Chinese)ã8:\nGet it running first.\nBuild a prototype and get it running. See if users are willing to use your product first.\nWhen running speed becomes a bottleneck, your business must be very successful, and you\u0026rsquo;ll surely have enough resources to hire a team of programmers to optimize the project into Rust/C++, or even assembly.\nWithout users, discussing performance is a pseudo-proposition.\nAs for the saying \u0026ldquo;dynamic languages are fast in the moment, but maintaining the code is a nightmare\u0026rdquo; I quite agree with that too.\nTherefore, I won\u0026rsquo;t consider dynamic languages when choosing a tech stack for a team; I would only use compiled languages, even strong-typed ones like Rust.\nBut right now, it\u0026rsquo;s just me building a prototype, so I use whatever I\u0026rsquo;m most proficient with.\n5.1 Vibe Coding? Concepts like Vibe Coding and AI programming are everywhere, overwhelming the discourse. You might naturally wonder if this project was generated by Vibe Coding.\nThe answer is, I tried for a few hours and then gave up entirely. I tried both Claude 4 and Gemini 2.5 Pro.\nI started with a Rust + Cloudflare Worker tech stack. Rust + Cloudflare Worker is a relatively niche field with limited training data. The code generated by Vibe Coding failed to compile.\nLater, I switched to Ruby on Rails, and the problems became even worse. Ruby is a dynamic language; its syntax is almost like English, and Rails has many \u0026ldquo;black magic\u0026rdquo; metaprogramming features.\nSo errors only appeared at runtime. The development time saved by code generation was entirely consumed by the debugging process.\nAnother issue is that code generated by Vibe Coding often lacks design. For example, it tightly coupled the Classifier and TrainedMessage classes, having the Classifier persist TrainedMessage instances.\nIt also directly performed synchronous model training within the telegram_bot_worker process upon receiving training data, waiting for training to finish before returning the command result, completely neglecting to decouple receiving training data from model training.\nOne can only say that Vibe Coding is quite suitable for strongly-typed, compiled languages like Rust â at least the generated code has to compile.\nAs for those claims of \u0026ldquo;making an APP without writing/changing a single line of code,\u0026rdquo; I can\u0026rsquo;t help but wonder:\nIs the code so good that it doesn\u0026rsquo;t need a single change? Or can the developer not identify the crux of the problem, and thus doesn\u0026rsquo;t change anything?\n6 Design Philosophy After developing the prototype and making the bot\u0026rsquo;s core functionality usable, many ideas popped into my head.\nI immediately rushed to add them to the bot, resulting in support for nearly ten commands, plus different modes for private chats and group chats.\nBut I felt something was off. Adding so many features felt like those all-in-one super apps common in China. I began to question:\nWould users really use all these features? Would any users use these features? Don\u0026rsquo;t too many features also create extra cognitive burden?\nMy favorite ad blocker, Ublock OriginÂ 9, is powerful and extremely effective at blocking, yet very simple and easy to use.\nRecalling the design philosophy mentioned in ãA Philosophy of Software Design(My thought about the book in Chinese)ã10, the interface should be simple and easy to use, even if the functionality underneath is complex and rich.\nSo, I first removed all commands I considered unrelated to the core functionality.\nFurthermore, considering that most users might not have a technical background and might not know how to use commands, I optimized the interface to use buttons as much as possible, allowing users to click directly, improving usability:\nI also wanted to support multiple languages (e.g., automatically switching to Chinese or English based on the user\u0026rsquo;s system language). This required decent Internationalization support.\nOver 60% of the code in the core service class telegram_botter.rb was introduced for such usability improvements.\nSimplicity for the user, complexity for the developer.\n6.1 How to Use Just two steps, and the bot works automatically.\nAdd the bot (@BayesSpamSniperBot) to your group Grant the bot admin permissions (Delete messages, Ban users) After these two steps, the bot will not only start working automatically, identifying spam within the group, deleting text messages, and banning users who send spam more than 3 times;\nIt will also become smarter with community usage (via /markspam and /feedspam).\nThe design philosophy of this bot is to minimize disruption to admins and users, provide simple operation commands, and maximize automation. Therefore, this bot only offers the following three commands (supporting auto-completion with \u0026ldquo;/\u0026rdquo;):\n6.1.1 /markspam Delete spam messages and ban the user. Requires admin permissions.\nReply /markspam to the message you want to ban, and the bot will automatically delete that message and ban the user.\n(Message has also been deleted) Unlike common group management bots, this command not only deletes the spam and bans the user but also, because this message is marked as spam by an admin with very high confidence, uses this spam ad as training data to update the model in real-time.\nSimilar messages will not only be identified next time, but all groups using this bot will benefit, as similar texts will also be marked as spam.\n6.1.2 /listspam View the list of spam messages. Requires admin permissions.\nView the list of spam messages and proactively mark false positive spam as normal.\n6.1.3 /listbanuser View the list of banned accounts. Requires admin permissions.\nView the list of banned users and proactively unban them.\n6.1.4 /feedspam Feed spam messages for training. No permissions required. Can be used in private chat or in-group.\nFeeding in private chat:\nFeeding in-group:\n7 Eating your own dog food In the software development field, there\u0026rsquo;s a saying: \u0026ldquo;Eating your own dog food,\u0026rdquo; which means you should use the things you develop yourself.\nSo I created my own channel for testing: è èæ²¹ä¸å¤©åå¢Â 11. Unfortunately, it has very few subscribers, which fails to attract many spammers. So everyone is welcome to subscribe or come in to post spam, to attract more spammers.\nIn my channel, everyone has the right to speak freely :-) (the only slight drawback is a limit on frequency ).\nSince no one was posting spam in my channel, and suffering from a lack of training data, I had to do it in the hard way: to join various crypto groups, NSFW groups, and actively seek out spam:\nThe screenshots are Chinese Telegram group contains a lot of spammers and spam:\nSince developing this bot, my perspective on spam has changed. I used to find spam annoying in other groups, but now I\u0026rsquo;m happy to see them in other groups, as they are valuable training data that I need to record quickly before they get deleted.\n7.1 The Ingenuity of Spam The algorithm\u0026rsquo;s effectiveness in other people\u0026rsquo;s stories is always surprisingly good, but when I run it myself, I always find various uncovered cases and unexpected surprises, just like life.\nAlthough keyword blocker is inefficient, the spam we actually see are those that have already bypassed keyword filters.\nFor example:\nå¨ å¸å æ³ èµ é±ï¼é£ ä½  ä¸å³ æ³¨ è¿ ä¸ª ç ç ç¤¾ åºï¼ççå¤ªå¯æäºï¼ç å¿ æ¨ èï¼æ¯ å¤© é½ æ å è´¹ ç­ ç¥ (Want to make money in the crypto circle? It\u0026rsquo;s a real pity if you don\u0026rsquo;t follow this ace community. Sincerely recommended, free strategies every day)\nOr\nè¿äººç®-ä»æç å-çº¦-æ¥åç¾¤ç»æºççETH500ç¹ï¼å¤§é¥¼5200ç¹ï¼ + @BTCETHl6666 (The contract reporting group linked in this person\u0026rsquo;s bio is pretty awesome. ETH to $500, BTC to $5200! + @BTCETHl6666)\nThe former uses spaces to bypass keywords, the latter uses punctuation.\nUnlike languages based on the Latin alphabet which naturally use spaces for word separation, Chinese requires word segmentation before statistical analysis with Bayes\u0026rsquo; theorem.\nthe fox jumped over the lazy dog\næä»¬çä¸­æå°±ä¸ä¸æ ·äº(Our Chinese is different)\n\u0026ldquo;æä»¬çä¸­æå°±ä¸ä¸æ ·äº\u0026rdquo; (Our Chinese is different) would be segmented into \u0026ldquo;æä»¬ | ç | ä¸­æ | å°± | ä¸ | ä¸æ · | äº\u0026rdquo; before word frequency can be counted.\nBut for the spam å¨ å¸å æ³ èµ é±ï¼é£ ä½  ä¸å³ æ³¨ è¿ ä¸ª ç ç ç¤¾ åºï¼ççå¤ªå¯æäºï¼ç å¿ æ¨ èï¼æ¯ å¤© é½ æ å è´¹ ç­ ç¥, the spaces not only affect keyword matching but also affect segmentation. The segmentation result for this sentence becomes(incorrect):\nå¨ | | å¸å | | æ³ | | èµ | | é± | ï¼ | é£ | | ä½  | | ä¸ | å³ | | æ³¨ | | è¿ | | ä¸ª | | ç | | ç | | ç¤¾ | | åº | ï¼ | çç | å¤ª | å¯æ | äº | ï¼ | ç | | å¿ | | æ¨ | | è | ï¼ | æ¯ | | å¤© | | é½ | | æ | | å | | è´¹ | | ç­ | | ç¥\nè¿äººç®-ä»æç å-çº¦-æ¥åç¾¤ç»æºççETH500ç¹ï¼å¤§é¥¼5200ç¹ï¼ + @BTCETHl6666 would be segmented into:\nè¿äººç® | - | ä»æ | ç | | å | - | çº¦ | - | æ¥å | ç¾¤ç» | æº | ç | ç | ETH500 | ç¹ | ï¼ | å¤§é¥¼ | 5200 | ç¹ | ï¼ | | + | | @ | BTCETHl6666\nUnsanitized training data would affect the model\u0026rsquo;s results, showing the importance of training data quality. Therefore, I performed corresponding preprocessing on the training data:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Step 1: Handle anti-spam separators # This still handles the cases like \u0026#34;å-çº¦\u0026#34; -\u0026gt; \u0026#34;åçº¦\u0026#34; previous = \u0026#34;\u0026#34; while previous != cleaned previous = cleaned.dup cleaned = cleaned.gsub(/([ä¸-é¾¯A-Za-z0-9])[^ä¸-é¾¯A-Za-z0-9]+([ä¸-é¾¯A-Za-z0-9])/, \u0026#39;\\1\\2\u0026#39;) end # Step 2: Handle anti-spam SPACES between Chinese characters # This specifically targets the \u0026#34;æ³ èµ é±\u0026#34; -\u0026gt; \u0026#34;æ³èµé±\u0026#34; case. # We run it in a loop to handle multiple spaces, e.g., \u0026#34;ç¤¾ åº\u0026#34; -\u0026gt; \u0026#34;ç¤¾åº\u0026#34; previous = \u0026#34;\u0026#34; while previous != cleaned previous = cleaned.dup # Find a Chinese char, followed by one or more spaces, then another Chinese char cleaned = cleaned.gsub(/([ä¸-é¾¯])(\\s+)([ä¸-é¾¯])/, \u0026#39;\\1\\3\u0026#39;) end # Step 3: Add strategic spaces # This helps jieba segment properly, e.g., \u0026#34;ç¤¾åºETH\u0026#34; -\u0026gt; \u0026#34;ç¤¾åº ETH\u0026#34; cleaned = cleaned.gsub(/([ä¸-é¾¯])([A-Za-z0-9])/, \u0026#39;\\1 \\2\u0026#39;) cleaned = cleaned.gsub(/([A-Za-z0-9])([ä¸-é¾¯])/, \u0026#39;\\1 \\2\u0026#39;) # Step 4: Remove excessive space cleaned = cleaned.gsub(/\\s+/, \u0026#39; \u0026#39;).strip After preprocessing, å¨ å¸å æ³ èµ é±ï¼é£ ä½  ä¸å³ æ³¨ è¿ ä¸ª ç ç ç¤¾ åºï¼ççå¤ªå¯æäºï¼ç å¿ æ¨ èï¼æ¯ å¤© é½ æ å è´¹ ç­ ç¥ becomes å¨å¸åæ³èµé±é£ä½ ä¸å³æ³¨è¿ä¸ªççç¤¾åºççå¤ªå¯æäºçå¿æ¨èæ¯å¤©é½æåè´¹ç­ç¥ (Note: legitimate commas are also removed here. I find the segmentation result acceptable compared to the impact of excessive punctuation). Its segmentation is:\nå¨ | å¸å | æ³ | èµé± | é£ | ä½  | ä¸ | å³æ³¨ | è¿ä¸ª | çç | ç¤¾åº | çç | å¤ª | å¯æ | äº | çå¿ | æ¨è | æ¯å¤© | é½ | æ | åè´¹ | ç­ç¥\nè¿äººç®-ä»æç å-çº¦-æ¥åç¾¤ç»æºççETH500ç¹ï¼å¤§é¥¼5200ç¹ï¼ + @BTCETHl6666 becomes è¿äººç®ä»æçåçº¦æ¥åç¾¤ç»æºçç ETH500 ç¹å¤§é¥¼ 5200 ç¹ï¼ + @BTCETHl6666. Its segmentation is:\nè¿ | äºº | ç®ä» | æ | ç | åçº¦ | æ¥å | ç¾¤ç» | æº | ç | ç | | ETH500 | | ç¹ | å¤§é¥¼ | | 5200 | | ç¹ | ï¼ | | + | | @ | BTCETHl6666\n7.1.1 Genius New Tricks Seeing many spam, I has to admire the creativity of spammers.\nBecause sending spam in messages gets caught by blockers, they innovatively came up with new tricks:\nThe messages contain normal text, but the profile picture and username are spam. This way, the spam blocker can\u0026rsquo;t work. Truly ingenious.\nFaced with such creative opponents, I adapted by building a training model for usernames. During detection, both the message text model and the username model are checked. If either one considers it spam, it gets blocked.\nOne could go further by doing OCR on profile pictures to extract text and add another model for profile pictures, but OCR is quite costly, so I\u0026rsquo;ll hold off for now.\n7.2 Optimization Without users, any optimization is unnecessary, as premature optimization is the root of all evil. Therefore, I focus on building the prototype first. But this doesn\u0026rsquo;t mean the prototype has no room for optimization.\nI have several optimization ideas in mind:\njieba\u0026rsquo;s segmentation might not be the best; other NLP algorithms could be used later for optimization. Retraining on every single training message is inefficient. A batching mechanism could be added, waiting for 5 minutes or accumulating 100 messages before processing. Currently, the entire model is computed in memory and persisted to the DB after calculation. A cache layer between memory and the database could optimize performance. The Bayesian algorithm might not be effective enough; a more complex machine learning model could be used. But these optimization points are all Good to have, not Must have. I\u0026rsquo;ll optimize them when actual problems arise later.\n8 Does it capture spam? Sending a message using transformed spam words:\nSuccessfully detected and automatically deleted:\nSome you might say this is just a demo showcase. Why are spam posted by others in my group still not being detected?\nBecause the Bayesian algorithm is fundamentally a probability-based algorithm. If it hasn\u0026rsquo;t encountered similar spam before, it cannot determine whether they are spam :(\nBe patient. All you need to do is use the /markspam command to delete the message and ban the user. This helps train the bot, and all users of this bot will benefit.\n9 Conclusion I thoroughly enjoyed this creative process: discovering a problem, having a spark of inspiration, building a prototype, and finally polishing it into a complete project.\nAlthough this is purely powered by passion â the code is open source, and I have to pay for the server out of my own pocket, with no material return.\nBut every time I see the bot successfully block an spam, that joy of creation and see it\u0026rsquo;s working is the best reward.\nProject repository: https://github.com/ramsayleung/bayes_spam_sniper Try it now: https://t.me/BayesSpamSniperBot https://podcasts.apple.com/us/podcast/%E8%BD%AF%E4%BB%B6%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/id1147186605\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://liuyandong.com/sample-page\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://t.me/huruanhuying\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://t.me/kaedeharakazuha17\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://paulgraham.com/spam.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.youtube.com/watch?v=HZGCoVF3YvM\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gist.github.com/ramsayleung/5848af0177a70a01d41f624e361b1b5d\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ramsayleung.github.io/zh/post/2024/%E7%BC%96%E7%A8%8B%E5%8D%81%E5%B9%B4%E7%9A%84%E6%84%9F%E6%82%9F/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ublockorigin.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ramsayleung.github.io/zh/post/2025/a_philosophy_of_software_design/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://t.me/pipeapplebun\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ramsayleung.github.io/en/post/2025/a_telegram_spam_blocker_bot_based_on_bayesian/","summary":"\u003cp\u003e\u003ca href=\"https://ramsayleung.github.io/zh/post/2025/%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AD%A6%E4%B9%A0%E7%9A%84telegram%E5%B9%BF%E5%91%8A%E6%8B%A6%E6%88%AA%E6%9C%BA%E5%99%A8%E4%BA%BA/\"\u003eä¸­æçæ¬\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"preface\"\u003e\u003cspan class=\"section-num\"\u003e1\u003c/span\u003e Preface\u003c/h2\u003e\n\u003cp\u003eI spent a weekend building a Telegram spam blocker bot based on Bayesian Algorithm \u003ccode\u003e@BayesSpamSniperBot\u003c/code\u003e (\u003ca href=\"https://t.me/BayesSpamSniperBot\"\u003ehttps://t.me/BayesSpamSniperBot\u003c/a\u003e). The project is open-sourced at: \u003ca href=\"https://github.com/ramsayleung/bayes_spam_sniper\"\u003ehttps://github.com/ramsayleung/bayes_spam_sniper\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"telegram\"\u003e\u003cspan class=\"section-num\"\u003e1.1\u003c/span\u003e Telegram\u003c/h3\u003e\n\u003cp\u003eTelegram is a popular instant messaging application, similar to Snapchat and WhatsApp, with over 1 billion users.\u003c/p\u003e\n\u003cp\u003eIt supports many powerful features like cloud chat history storage, clients for Linux, Mac, Windows, Android, IOS, and Web (all open-source), Channel, and arguably the most powerful bot system I\u0026rsquo;ve ever seen.\u003c/p\u003e","title":"A Telegram Spam Blocker Bot Based On Bayesian Algorithm"},{"content":"ä¸­æçæ¬\n1 Preface Malcolm Gladwell\u0026rsquo;s \u0026ldquo;10,000-hour rule\u0026rdquo; suggests that continuous investment of 10,000 hours of effort is sufficient to reach expert level in any field. Based on 20 hours of practice per week, this requires about 3 hours of daily investment, taking roughly ten years to achieve this goal.\nSince I wrote my first line of C code, more than ten years have passed. During this period, I have written over 300,000 lines of code, some of which, written at WeChat, have served more than 1 billion users.\nDespite having written so much code, I still dare not call myself an expert.\nHowever, years of being a \u0026ldquo;builder\u0026rdquo;, coding day after day, have allowed me to accumulate considerable insights.\n\u0026ldquo;Practice makes perfect\u0026rdquo; - these insights are both reflections on programming technology and experiences of professional life.\n2 Continuous Learning Although I started programming with C in university, my main language in college was Java, as Java is a very mature industrial language with rich frameworks, highly popular in enterprises, with a lot job opportunities.\nI started web development with Java Servlets, then learned the very popular JavaEE enterprise development framework SSH, namely Struts2Â 1+ SpringÂ 2+ HibernateÂ 3. Struts2 handled control logic, Spring provided decoupling, and Hibernate as ORM.\nBy the time I started job hunting, the SSH concept had changed - Struts2 was replaced by SpringMVCÂ 4, and SSH became SpringMVC + Spring + Hibernate.\nWhen I interned at Ant Group, I discovered that the team\u0026rsquo;s codebase didn\u0026rsquo;t use Hibernate for database, but rather IbatisÂ 5, which later switched to the newer MyBatisÂ 6.\nAnt Group internally didn\u0026rsquo;t use Spring/SpringMVC either, but rather their in-house SOFA frameworkÂ 7. The Spring community later felt that the Spring framework was too heavyweight and not conducive to rapid development, so they developed the lighter SpringBootÂ 8, while Ant internally launched the SOFA version called SOFABootÂ 9.\nAfter joining WeChat Pay, I initially wrote C++ using WeChat in-house RPC framework called Svrkit. Later, due to Big Data projects, I started using Spark + Python + Hive SQL.\nNow at AWS S3, because the business has extremely high performance and resource usage requirements, I\u0026rsquo;m using Rust again, while legacy business still uses Java. After going full circle, I\u0026rsquo;m back on the Java path.\nCounting them all, over these years, I\u0026rsquo;ve written production code in Java, C++, Python, Rust, Scala, Kotlin, and JavaScript/TypeScript.\nBeyond work, I\u0026rsquo;ve also learned Scheme while studying SICP, Emacs Lisp from using Emacs, Swift for indie development, Ruby for Ruby on Rails, Golang for load testing, and various frameworks and libraries for different languages.\nSince I started learning programming, I\u0026rsquo;ve learned at least half a dozen programming languages.\nI never define myself as a programmer of any specific language, like Java programmer or C++ programmer - I just call myself a Software Development Engineer. Languages are merely tools; as long as I keep learning, I\u0026rsquo;ll naturally learn new programming languages when encountering new scenarios.\nThe computer world changes rapidly - new frameworks might emerge every few months, and new languages become popular every few years. Continuous learning is essential for maintaining competitiveness..\n3 Independent Thinking WeChat used to have a tradition of giving out annual gifts to employees.\nIn 2022, the annual gift we received was indeed an aluminum plate with \u0026ldquo;Keep Independent Thinking 2022\u0026rdquo; written on it.\nThe creator of WeChat has always emphasized the importance of \u0026ldquo;independent thinking\u0026rdquo; for WeChat, believing that if he had to choose the most important quality, he would choose \u0026ldquo;independent thinking.\u0026rdquo;\nWhat superiors say isn\u0026rsquo;t necessarily right, what teachers say isn\u0026rsquo;t necessarily right, what academic institutions say isn\u0026rsquo;t necessarily right, what media says isn\u0026rsquo;t necessarily right, and loud voices certainly aren\u0026rsquo;t necessarily right - after all, being reasonable doesn\u0026rsquo;t require being loud.\nFor example, microservice architecture is very popular, and many companies are implementing microservices - does this mean monolithic architecture shouldn\u0026rsquo;t be used?\nShould startups or small teams adopt microservice architecture for new businesses? Or should they start with monolithic architecture and migrate to services as the business grows?\nDevelopment inevitably involves various decisions, such as technology selection. For your requirements, you might find a dozen components that \u0026ldquo;seemingly\u0026rdquo; meet the requirements, and you might search online for evaluations of each component, finding diverse opinions. You need to independently analyze each component, identify their pros and cons, and make decisions based on your team\u0026rsquo;s characteristics.\nRegarding independent thinking, my favorite quote is from the HBO miniseries \u0026ldquo;Chernobyl,\u0026rdquo; when scientist Valery Legasov asks the KGB to release his colleague Ulana Khomyuk, who was investigating the truth, saying he could guarantee she was no problem, the KGB chief responds:\nTrust, but verify.\nBecause the first step of independent thinking is: start questioning.\n4 Get It Running First This phrase has a well-known variant: \u0026ldquo;It\u0026rsquo;s not like it doesn\u0026rsquo;t work.\u0026rdquo;\nMany programmers are perfectionists, especially those who have read \u0026ldquo;Refactoring\u0026rdquo; and \u0026ldquo;Design Patterns,\u0026rdquo; who tend to spend a lot of time optimizing code and refactoring.\nI used to have similar impulses, always wanting to spend time optimizing code, but after working on many projects, I have a strong feeling that it\u0026rsquo;s better to get the MVP launch first and let users experience it early.\nIf no users are using it, even the best and most beautiful code is meaningless.\nSo when I often see community members asking what language and framework to use for side projects, whether PHP/Python/Ruby would be too slow, my view has always been: build a prototype first, find the first user first.\nWhen performance becomes a bottleneck, your business is already very success, and you\u0026rsquo;ll definitely have enough money to hire a dozen programmers to rewrite your project in Rust/C++, or assembly.\nIn this regard, I strongly agree with the my co-worker sitting next to me about code quality:\nmake it run, make it fast, make it beautiful.\nRecently attempting side projects, I have a profound realization that technology might be the least important aspect of business.\nBuilding a product from scratch and promoting it to users - users only care whether your product is useful and can solve their problems.\nThey don\u0026rsquo;t care whether you wrote it in C++/Java or JavaScript, nor do they care whether your code is elegant. Rather than obsessing over technology selection, it\u0026rsquo;s better to build the product first and let users try it.\n5 The Most Comfortable Tool is the Best I often see people in the community asking what\u0026rsquo;s the best language, best framework, best editor, best operating system.\n\u0026ldquo;Best\u0026rdquo; is quite a subjective conclusion, and there\u0026rsquo;s no \u0026ldquo;best\u0026rdquo; solution for all scenarios, but I often see community members arguing over which language is better.\nOr when someone shares about A, others reply that B/C/D is better, then arguments ensue.\nThis reminds me of the group identity phenomenon mentioned in \u0026ldquo;The Social Animal,\u0026rdquo; a famous social psychology work.\nWhen fans develop strong identification with a team, they view the team as part of their self-identity, leading them to:\nUse \u0026ldquo;we\u0026rdquo; instead of \u0026ldquo;they\u0026rdquo; to refer to the team View the team\u0026rsquo;s success as personal success React defensively to criticism of the team, viewing such criticism as attacks on themselves If someone asks me this question, I answer \u0026ldquo;the tool you\u0026rsquo;re most comfortable and familiar with is best.\u0026rdquo;\nEven if for fun, programming\u0026rsquo;s purpose is still to use computers to solve problems, and the best tool for solving problems is the one you\u0026rsquo;re most familiar with.\nUnless the tool you know isn\u0026rsquo;t suitable for your problem, then naturally you need a new tool - don\u0026rsquo;t force a square peg into a round hole.\nOf course, if it\u0026rsquo;s to satisfy curiosity and learn a new language, then choose what interests you.\nWhen I learned Rust in 2017, it was simply because I had no classes in senior year, plenty of time, and wanted to learn something interesting and new. Rust 1.0 had only been released for 2 years then - I never expected to find work with Rust.\nI can\u0026rsquo;t remember where I read this passage:\nI once asked myself similar questions:\nDo good things necessarily become popular? Not necessarily Are things I like necessarily good things? Not necessarily Will I spend time and energy on something that might not become popular but I like? Yes 6 Communicate More with People While programmers certainly work with machines, fundamentally we\u0026rsquo;re still solving human problems.\nWhen I first learned programming, I had a misconception that as long as I mastered the technology, I wouldn\u0026rsquo;t need to care about \u0026ldquo;interpersonal relationships.\u0026rdquo;\nTherefore, after entering the workplace, I held such views and acted accordingly. While I wasn\u0026rsquo;t cold to others, I inevitably was, as friends described: \u0026ldquo;aloof.\u0026rdquo;\nBut after working for a long time, I realized that \u0026ldquo;interpersonal relationships\u0026rdquo; are inevitable - which is called networking and connections.\nEven if my technical abilities are solid, I need to be seen by others. Having good relationships with colleagues and leaders allows for \u0026ldquo;win-win\u0026rdquo; when achievements are made.\nSo now I usually chat with colleagues whether there\u0026rsquo;s something specific or not, both to know each other better and learn more about the group, and to find potential optimization points from colleagues\u0026rsquo; complaints, practicing my philosophy of \u0026ldquo;Work hard and be nice to people.\u0026rdquo;\nAfter doing this job for a long time, I find out that software engineering is fundamentally human systems engineering.\n7 Code Isn\u0026rsquo;t Everything After writing programs for a while, it\u0026rsquo;s easy to have illusion that everything can be solved with code.\nWhen you\u0026rsquo;re holding a hammer, everything looks like a nail.\nThe reality I learned is that many things cannot be solved with code. Code is just a tool that can only be used in appropriate scenarios - avoid path dependency.\nKnowing how to write code isn\u0026rsquo;t useful, you also need to write articles, give presentations within the company, let others \u0026ldquo;see you.\u0026rdquo;\nProfessional ability in programming and project delivery is certainly important, but you also need soft skills in marketing yourself.\nChatting with the boss occasionally to increase communication and showing your visibility regularly might be more useful than launching ten projects.\n8 Work with Excellent People After years in the industry, having worked at Ant Group, WeChat, and AWS, and having worked with all kinds of colleagues, I have an increasingly strong insight:\nWork with excellent people\nNot only can you learn excellent qualities from them and improve technical abilities, you can learn best practices and engineering experience. During code reviews, you can learn better programming approaches, and when encountering problems, you have reliable teammates to help and guide you.\nYou\u0026rsquo;ll understand the uniqueness of systems developed by excellent programmers, know what simple and ergonomic systems look like, and develop your own technical taste.\nTaste and aesthetics are abstract concepts, but after using good systems, you naturally won\u0026rsquo;t be interested in those crude, poorly made systems that rely on boss endorsement for forced promotion.\nImproving technical taste, while enhancing our technical cognition, can in turn help us improve design capabilities.\nAnother benefit of working with excellent colleagues is building high-quality professional networks, which benefits career development and provides more options when changing jobs or switching tracks.\nAlthough startups also have excellent developers, on average, Big Tech have higher proportions of excellent programmers, as they offer more competitive salaries and benefits, naturally having higher recruitment standards.\nFor example, WeChat has a so-called interview committee(like a group of bar raiser). Besides interviewers from the hiring department, candidates must also pass interviews by committee interviewers to avoid lowering standards for quick hiring.\nSo I personally suggest that new graduates should go to big tech when possible, to gain experience.\nAlthough I left WeChat almost two years ago, I still miss the colleagues I worked with in the same group. They were truly technically excellent, super nice people, and willing to help.\n9 Health Is the Foundation of Everything After programming for so many years, I\u0026rsquo;ve developed a pile of occupational diseases.\nI had tenosynovitis since university, developed a lower back pain after working for a few years and my once thick, dark hair is now increasingly sparse.\nBecause Tencent headquarters had a free gym, I basically went to the company gym on workdays to take advantage of company benefits - 2 days of cardio running, 2 days of anaerobic equipment training, persisting for almost 3 years.\nI also started paying attention to my diet, trying to eat less oil and sugar and avoid alcohol.\nWhile fitness isn\u0026rsquo;t a cure-all, at least I feel charged to handle high-intensity work.\nOnly when I lose something do I learn to cherish it. Only when I start taking medication and going to hospital for follow-ups do I start paying attention to my body.\nAlthough programming is fun and supporting family is important, I still need to pay attention to my body. After all, health is the foundation of everything - if it breaks down, there are no other exciting stories.\n10 Summary Whether it\u0026rsquo;s programming or other skills, I feel they all follow the \u0026ldquo;Matthew Effect\u0026rdquo; - the more I learn, the more I understand, and the faster you\u0026rsquo;ll learn new things.\nAfter writing a lot of code, I realized that a programmer\u0026rsquo;s competitiveness isn\u0026rsquo;t writing code, nor is it any particular language or framework. The core competitiveness is the ability to solve problems through technology - why be constrained by any specific programming language or technology?\nI hope ten years of programming is just a starting point, and in ten years I can write another piece: \u0026ldquo;Reflections on Twenty Years of Programming.\u0026rdquo;\nhttps://struts.apache.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://spring.io/projects/spring-framework\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://hibernate.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.spring.io/spring-framework/reference/web/webmvc.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ibatis.apache.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://mybatis.org/mybatis-3/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/sofastack/sofa-rpc\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://spring.io/projects/spring-boot\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/sofastack/sofa-boot\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ramsayleung.github.io/en/post/2024/reflection_on_ten_years_of_programming/","summary":"\u003cp\u003e\u003ca href=\"https://ramsayleung.github.io/zh/post/2024/%E7%BC%96%E7%A8%8B%E5%8D%81%E5%B9%B4%E7%9A%84%E6%84%9F%E6%82%9F/\"\u003eä¸­æçæ¬\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"preface\"\u003e\u003cspan class=\"section-num\"\u003e1\u003c/span\u003e Preface\u003c/h2\u003e\n\u003cp\u003eMalcolm Gladwell\u0026rsquo;s \u0026ldquo;10,000-hour rule\u0026rdquo; suggests that continuous investment of 10,000 hours of effort is sufficient to reach expert level in any field.\nBased on 20 hours of practice per week, this requires about 3 hours of daily investment, taking roughly ten years to achieve this goal.\u003c/p\u003e\n\u003cp\u003eSince I wrote my first line of C code, more than ten years have passed.\nDuring this period, I have written over 300,000 lines of code, some of which, written at WeChat, have served more than 1 billion users.\u003c/p\u003e","title":"Reflections on Ten Years of Programming"},{"content":"Developers usually use git blame in GUI tools like GitHub Blame or using GitLens blame in VSCode: Even though GUI tools is intuitive, but the Git CLI has much more powerful tooling for finding something closer to the real story behind your code. There are many scenarios that CLI is valuable, the first is ignoring the whitespace changes. For example, if you formatted your C++ codebase with clang-format or Javascript codebase with prettier, you haven\u0026rsquo;t actually changed the codebase, but you\u0026rsquo;re the owner of tons of lines of code. The git blame -w option will ignore these type of whitespace changes. The other great option is -C which will look for code movement between files in a commit. For example, if you refactor a function from one file to another, the normal git blame will simply show you as the author in the new file, but the -C option will follow that movement and show the last person who actually change those lines of code. -C is extremely helpful when I need to find out the original author of some lines of code after file renames or refactors, to know more about the background and context behind this code According to the git blame doc, you could pass -C up to three times to ask Git try even harder: 1 2 3 4 5 -C[\u0026lt;num\u0026gt;] In addition to -M, detect lines moved or copied from other files that were modified in the same commit. This is useful when you reorganize your program and move code around across files. When this option is given twice, the command additionally looks for copies from other files in the commit that creates the file. When this option is given three times, the command additionally looks for copies from other files in any commit. (it\u0026rsquo;s a bit of odd design) Let\u0026rsquo;s take the access.rb file of ActiveModel module in Rails framework for example: 1 git blame activemodel/lib/active_model/access.rb Figure 1: Vanilla git blame\nOk, it looks like Jonathan Hefner wrote all of this code it appears, let\u0026rsquo;s look at the same code with git blame -w -C -C -C activemodel/lib/active_model/access.rb Figure 2: git blame -w -C -C -C\nNow we can see that Git has followed this code from file to file over the course of multiple renames, it turns out Jonathan Hefner is the most recent file renamer, Guillermo Iguaran is the original author. If we want to know the history about this file, it\u0026rsquo;s much better to ask Guillermo rather than Jonathan, which is beyond what the GUI blame or normal Git blame tool reveals ","permalink":"https://ramsayleung.github.io/en/post/2024/git_blame_with_following/","summary":"\u003cp\u003eDevelopers usually use \u003ccode\u003egit blame\u003c/code\u003e in GUI tools like GitHub Blame \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003e\n\u003cfigure\u003e\n    \n    \n    \u003cinput type=\"checkbox\" id=\"zoomCheck-68df0\" hidden\u003e\n    \u003clabel for=\"zoomCheck-68df0\"\u003e\n    \n    \n    \u003cimg class=\"zoomCheck\" loading=\"lazy\" src=\"/ox-hugo/github_blame.png\"/\u003e \n    \n    \n    \u003c/label\u003e\n\u003c/figure\u003e\n \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eor using GitLens blame in VSCode: \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003e\n\u003cfigure\u003e\n    \n    \n    \u003cinput type=\"checkbox\" id=\"zoomCheck-7f082\" hidden\u003e\n    \u003clabel for=\"zoomCheck-7f082\"\u003e\n    \n    \n    \u003cimg class=\"zoomCheck\" loading=\"lazy\" src=\"/ox-hugo/git_blame_git_lens_vscode.png\"/\u003e \n    \n    \n    \u003c/label\u003e\n\u003c/figure\u003e\n \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eEven though GUI tools is intuitive, but the Git CLI has much more powerful tooling for finding something closer to the real story behind your code. \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eThere are many scenarios that CLI is valuable, the first is ignoring the whitespace changes. \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eFor example, if you formatted your C++ codebase with \u003ccode\u003eclang-format\u003c/code\u003e or Javascript codebase with \u003ccode\u003eprettier\u003c/code\u003e, you haven\u0026rsquo;t actually changed the codebase, but you\u0026rsquo;re the owner of tons of lines of code. \u003cbr/\u003e\u003c/p\u003e","title":"TIL: Git Blame with Following"},{"content":"Every Git user will have probably been asked to set up their Git at the first time: 1 2 git config --global user.name \u0026#34;Ramsay Leung\u0026#34; git config --global user.email ramsayleung@gmail.com The above command will simply add the user.name and user.email value into your ~/.gitconfig file 1 2 3 4 5 6 7 8 \u0026gt; cat ~/.gitconfig [user] name = Ramsay Leung email = ramsayleung@gmail.com [core] quotepath = false [init] defaultBranch = master You could also specify --local argument to writes the config values to .git/config in whatever project you\u0026rsquo;re currently in. If you need to simultaneously contribute to your work and open source project on the same laptop, with different Git config values, e.g.(company email address for work-specific projects, personal email address for open source project), what should you do? You could definitely set up work-specific config as global config, then set up personal config with --local for every personal project separately. It works, but tedious and easy to mess-up. Fortunately, starting from Git version 2.13, Git supports conditional configuration includes, you are capable of setting up different configs for different repositories. If you add the following config to your global config file: 1 2 3 4 5 [includeIf \u0026#34;gitdir:~/projects/oss/\u0026#34;] path = ~/.gitconfig-oss [includeIf \u0026#34;gitdir:~/projects/work/\u0026#34;] path = ~/.gitconfig-work Then Git will look in the ~/.gitconfig-oss files for values only if the project you are currently working on matches ~/projects/oss/. Caution: If you forget to specify the \u0026ldquo;/\u0026rdquo; at the end of the git dir, e.g. \u0026ldquo;~/projects/oss\u0026rdquo;, Conditional Config won\u0026rsquo;t work! Therefore, you could have a \u0026ldquo;work\u0026rdquo; directory and work-specific config here and an \u0026ldquo;oss\u0026rdquo; directory with values for your open source projects, etc. Git also supports other filters more than gitdir, you could specify a branch name as an include filter with onbranch 1 2 3 4 ; include only if we are in a worktree where foo-branch is ; currently checked out [includeIf \u0026#34;onbranch:foo-branch\u0026#34;] path = foo.inc Check out the Git docs for more details ","permalink":"https://ramsayleung.github.io/en/post/2024/git_conditional_configs/","summary":"\u003cp\u003eEvery Git user will have probably been asked to set up their Git at the first time: \u003cbr/\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit config --global user.name \u003cspan class=\"s2\"\u003e\u0026#34;Ramsay Leung\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit config --global user.email ramsayleung@gmail.com\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eThe above command will simply add the \u003ccode\u003euser.name\u003c/code\u003e and \u003ccode\u003euser.email\u003c/code\u003e value into your \u003ccode\u003e~/.gitconfig\u003c/code\u003e file \u003cbr/\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e8\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u0026gt; cat ~/.gitconfig\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003euser\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nv\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e Ramsay Leung\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nv\"\u003eemail\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e ramsayleung@gmail.com\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003ecore\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nv\"\u003equotepath\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003efalse\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003einit\u003cspan class=\"o\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nv\"\u003edefaultBranch\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e master\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eYou could also specify \u003ccode\u003e--local\u003c/code\u003e argument to writes the config values to \u003ccode\u003e.git/config\u003c/code\u003e in whatever project you\u0026rsquo;re currently in. \u003cbr/\u003e\u003c/p\u003e","title":"TIL: Git Conditional Configs"},{"content":"1 Goodbye 2023 As I farewelled to 2023, a year marked by numerous changes and personal evolution, I find myself recollecting the multitude of experiences that unfolded. My 2023 journey was nothing short of fascinating and exciting, prompting me to revisit the year from various angles. After seeing hoards of posts in social media generated by Github Contributions Chart, I thought I could also build an APP to summarize my Github contribution for every year for friends to have fun. I spent my entire 4-days-new-year vocation to build this app named: Github Summary. This project led me through a series of first-time experiences: first time to try Tailwind Css framework, first time to use and deploy project on Vercel, first time to build project on nextjs, first time to develop a public project on React(yes, I\u0026rsquo;ve tried to learn React for hundreds of times, but never get a chance to use it in real project), etc. 2 Happy 2024 While I hoped I could have completed this project by the close of 2023 to share summaries with friends, life\u0026rsquo;s timeline had other plans. Now, as we step into 2024, I am thrilled to publish the GitHub Summary. It\u0026rsquo;s never too late to showcase creative work, and this project is poised to generate insightful summaries not just for the past year but for the adventures that await in 2024. Wishing everyone a Happy New Year! Feel free to explore GitHub Summary: https://github-summary.vercel.app/ ","permalink":"https://ramsayleung.github.io/en/post/2024/github_summary/","summary":"\u003ch2 id=\"goodbye-2023\"\u003e\u003cspan class=\"section-num\"\u003e1\u003c/span\u003e Goodbye 2023\u003c/h2\u003e\n\u003cp\u003eAs I farewelled to 2023, a year marked by numerous changes and personal evolution, I find myself recollecting the multitude of experiences that unfolded. \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eMy 2023 journey was nothing short of fascinating and exciting, prompting me to revisit the year from various angles. \u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003eAfter seeing hoards of posts in social media generated by \u003ca href=\"https://github.com/sallar/github-contributions-chart\"\u003eGithub Contributions Chart\u003c/a\u003e, I thought I could also build an APP to summarize my Github contribution for every year for friends to have fun. \u003cbr/\u003e\u003c/p\u003e","title":"Rewind your Github summary"},{"content":"1 Introduction 1.1 IaC Infrastructure as code(IaC) is the managing and provisioning of infrastructure through code instead of manual processes, for example, clicking button, adding or editing roles in AWS console.\n1.2 AWS CloudFormation AWS CloudFormation is the original IaC tool for AWS, released in 2011, which uses template files to automate and mange the setup of AWS resources.\n1.3 AWS CDK AWS Cloud Development Kit(CDK) is a product provided by AWS that makes it easier for developers to manage their infrastructure with familiar programming languages like TypeScript, Python, Java, etc.\nAnd, CDK is standing on the shoulder of Cloudformation, providing tools for developers by leveraging Cloudformation.\nA stack is a collection of AWS resources that you can manage as a single unit, like a box.\nFor instance, this box could include all the resources required to run an application or Lambda service, such as S3 Buckets (storage), Roles (authorization), Lambda Function (computing), API Gateway (access point), Alarm, Monitoring, etc.\n2 Problem I am currently working on a project which requires to set up two stacks, one stack( GlueStack ) for defining a list of AWS Glue tables and the other stack( ServiceStack ) for definition of Lambda service and associated resources.\nIn fact, S3 bucket names have to be globally unique within a partition, which means crossing the whole AWS customer base.\nYou are unable to create a S3 bucket with bucket name which is in use by another AWS customer or your own account.\nSo it\u0026rsquo;s safer to let CloudFormation generate a random bucket name for a developer when he need to initialize a S3 bucket.\nHowever, there is new a problem I face: since the S3 bucket name is randomly generated characters, if GlueStack need to read the bucket created by ServiceStack, how could I share the bucket name between two stacks?\nWhile these two stacks are isolated and separated, resources collection.\n3 Solution Fortunately, CDK offers a facility named CfnOutput to export a deployed resource, so that the consumer of the resource is able to Import required resource.\nDefine the required resource in ServiceStack (producer), for instance, a S3 bucket: 1 2 3 import { Bucket } from \u0026#39;aws-cdk-lib/aws-s3\u0026#39;; const s3Bucket = new Bucket(this, \u0026#39;MyBucketId\u0026#39;, {}); Export the resource by specifying the value and exportName: 1 2 3 4 5 6 7 import { CfnOutput } from \u0026#39;aws-cdk-lib\u0026#39;; // export the generated bucket name to other stack new CfnOutput(this, \u0026#39;exportRequiredS3Bucket\u0026#39;, { value: s3Bucket.bucketName, exportName: \u0026#39;exportRequiredS3Bucket\u0026#39;, }); Import the required resource in GlueStack (consumer): 1 2 3 import { Fn} from \u0026#39;aws-cdk-lib\u0026#39;; const requiredS3BucketName = Fn.importValue(\u0026#39;exportRequiredS3Bucket\u0026#39;); If we take a closer look at the synthesized CFN template for ServiceStack, we could find:\n1 2 3 4 5 6 7 8 9 \u0026#34;Outputs\u0026#34;: { \u0026#34;exportRequiredS3Bucket\u0026#34;: { \u0026#34;Value\u0026#34;: { \u0026#34;Ref\u0026#34;: \u0026#34;MyBucketId737FC949\u0026#34; }, \u0026#34;Export\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;exportRequiredS3Bucket\u0026#34; } } The synthesized CFN template for GlueStack:\n1 2 3 { \u0026#34;Fn::ImportValue\u0026#34;: \u0026#34;exportRequiredS3Bucket\u0026#34; } This is the way about how to share value between two stacks.\n4 Loose couping solution Updated on 2023-12-02\nPeople learn from mistake.\nAfter applying this practice in my project, I recently learn that it\u0026rsquo;s not good practice to share resource across stack.\nWith using export/import, I tightly couple my stacks with a commitment that I can never update that unless I remove that couping later on.\nIt means it will become a disaster1 whenever I need to update/delete the S3Bucket, CloudFormation will raise an error, complaining something like: \u0026ldquo;ServiceStack cannot be deleted as it\u0026rsquo;s in use by GlueStack\u0026rdquo;.\nA better practice I learnt is adding a loose couping between ServiceStack and GlueStack by sharing a constant variable:\nDefine a constant variable somewhere:\n1 2 3 export const Constants = { MyBucketName: \u0026#39;TestBucket\u0026#39; } Refine the definition of s3Bucket\n1 2 3 4 5 import { Bucket } from \u0026#39;aws-cdk-lib/aws-s3\u0026#39;; const s3Bucket = new Bucket(this, \u0026#39;MyBucketId\u0026#39;, { bucketName: Constants.MyBucketName, }); Refer the s3Bucket in GlueStack by MyBucketName instead of CDK exported reference\n1 const requiredS3BucketName = Constants.MyBucketName; Therefore, these two stacks are not directly coupled, but they are referencing the same constant variable.\nThen, CloudFormation won\u0026rsquo;t prevent you from updating the S3Bucket as there is not direct relation between these two stacks anymore.\nThis is the benefit of loose couping.\n5 Reference API Document: class CfnOutput (construct) AWS Cloud Development Kit (AWS CDK) v2 https://stackoverflow.com/questions/63350346/delete-resource-with-references\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ramsayleung.github.io/en/post/2023/how_to_share_resource_between_cdk_stacks/","summary":"\u003ch2 id=\"introduction\"\u003e\u003cspan class=\"section-num\"\u003e1\u003c/span\u003e Introduction\u003c/h2\u003e\n\u003ch3 id=\"iac\"\u003e\u003cspan class=\"section-num\"\u003e1.1\u003c/span\u003e IaC\u003c/h3\u003e\n\u003cp\u003eInfrastructure as code(IaC) is the managing and provisioning of infrastructure through code instead of manual processes, for example, clicking button, adding or editing roles in AWS console.\u003c/p\u003e\n\u003ch3 id=\"aws-cloudformation\"\u003e\u003cspan class=\"section-num\"\u003e1.2\u003c/span\u003e AWS CloudFormation\u003c/h3\u003e\n\u003cp\u003eAWS CloudFormation is the original IaC tool for AWS, released in 2011, which uses template files to automate and mange the setup of AWS resources.\u003c/p\u003e\n\u003ch3 id=\"aws-cdk\"\u003e\u003cspan class=\"section-num\"\u003e1.3\u003c/span\u003e AWS CDK\u003c/h3\u003e\n\u003cp\u003eAWS Cloud Development Kit(CDK) is a product provided by AWS that makes it easier for developers to manage their infrastructure with familiar programming languages like TypeScript, Python, Java, etc.\u003c/p\u003e","title":"How to share resource between CDK stacks"},{"content":"1 Definition In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertx v, u comes before v in the ordering.\nIt sounds pretty academic, but I am sure you are using topological sort unconsciously every single day.\n2 Application Many real world situations can be modeled as a graph with directed edges where some events must occur before others. Then a topological sort gives an order in which to perform these events, for instance:\n2.1 College class prerequisites You must take course b first if you want to take course a. For example, in your alma mater, the student must complete PHYS:1511(College Physics) or PHYS:1611(Introductory Physics I) before taking College Physics II.\nThe courses can be represented by vertices, and there is an edge from College Physics to College Physics II since PHYS:1511 must be finished before College Physics II can be enrolled.\n2.2 Job scheduling scheduling a sequence of jobs or tasks based on their dependencies. The jobs are represented by vertices, and there is an edge from x to y if job x must be completed before job y can be started.\nIn the context of a CI/CD pipeline, the relationships between jobs can be represented by directed graph(specifically speaking, by directed acyclic graph). For example, in a CI pipeline, build job should be finished before start test job and lint job.\n2.3 Program build dependencies You want to figure out in which order you should compile all the program\u0026rsquo;s dependencies so that you will never try and compile a dependency for which you haven\u0026rsquo;t first built all of its dependencies.\nA typical example is GNU Make: you specific your targets in a makefile, Make will parse makefile, and figure out which target should be built firstly. Supposing you have a makefile like this:\n1 2 3 4 5 6 7 8 9 10 # Makefile for analysis report output/figure_1.png: data/input_file_1.csv scripts/generate_histogram.py python scripts/generate_histogram.py -i data/input_file_1.csv -o output/figure_1.png output/figure_2.png: data/input_file_2.csv scripts/generate_histogram.py python scripts/generate_histogram.py -i data/input_file_2.csv -o output/figure_2.png output/report.pdf: report/report.tex output/figure_1.png output/figure_2.png cd report/ \u0026amp;\u0026amp; pdflatex report.tex \u0026amp;\u0026amp; mv report.pdf ../output/report.pdf Make will generate a DAG internally to figure out which target should be executed firstly with typological sort:\n3 Directed Acyclic Graph Back to the definition, we say that a topological ordering of a directed graph is a linear ordering of its vertices, but not all directed graphs have a topological ordering.\nA topological ordering is possible if and only if the graph has no directed cycles, that is, if it\u0026rsquo;s a directed acyclic graph(DAG).\nLet us see some examples:\nThe definition requires that only the directed acyclic graph has a topological ordering, but why? What happens if we are trying to find a topological ordering of a directed graph? Let\u0026rsquo;s take the figure 3 for an example.\nThe directed graph problem has no solution, this is the reason why directed cycle is forbidden\n4 Kahn\u0026rsquo;s Algorithm There are several algorithms for topological sorting, Kahn\u0026rsquo;s algorithm is one of them, based on breadth first search.\nThe intuition behind Kahn\u0026rsquo;s algorithm is pretty straightforward:\nTo repeatedly remove nodes without any dependencies from the graph and add them to the topological ordering\nAs nodes without dependencies are removed from the graph, the original nodes depend on the removed node should be free now.\nWe keep removing nodes without dependencies from the graph until all nodes are processed, or a cycle is detected.\nThe dependencies of one node are represented as in-degree of this node.\nLet\u0026rsquo;s take a quick example of how to find out a topological ordering of a given graph with Kahn\u0026rsquo;s algorithm.\nNow we should understand how Kahn\u0026rsquo;s algorithm works. Let\u0026rsquo;s have a look at a C++ implementation of Kahn\u0026rsquo;s algorithm:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #include \u0026lt;deque\u0026gt; #include \u0026lt;vector\u0026gt; // Kahn\u0026#39;s algorithm // `adj` is a directed acyclic graph represented as an adjacency list. std::vector\u0026lt;int\u0026gt; findTopologicalOrder(const std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp;adj) { int n = adj.size(); std::vector\u0026lt;int\u0026gt; in_degree(n, 0); for (int i = 0; i \u0026lt; n; i++) { for (const auto \u0026amp;to_vertex : adj[i]) { in_degree[to_vertex]++; } } // queue contains nodes with no incoming edges std::deque\u0026lt;int\u0026gt; queue; for (int i = 0; i \u0026lt; n; i++) { if (in_degree[i] == 0) { queue.push_back(i); } } std::vector\u0026lt;int\u0026gt; order(n, 0); int index = 0; while (queue.size() \u0026gt; 0) { int cur = queue.front(); queue.pop_front(); order[index++] = cur; for (const auto \u0026amp;next : adj[cur]) { if (--in_degree[next] == 0) { queue.push_back(next); } } } // there is no cycle if (n == index) { return order; } else { // return an empty list if there is a cycle return std::vector\u0026lt;int\u0026gt;{}; } } 5 Bonus When a pregnant woman takes calcium pills, she must make sure also that her diet is rich in vitamin D, since this vitamin makes the absorption of calcium possible.\nAfter reading the demonstration of topological ordering, you (and I) too should take a certain vitamin, metaphorically speaking, to help you absorb. The vitamin D I pick for you (and myself) is two leetcode problems, which involve with the most typical use case of topological ordering \u0026ndash; college class prerequisites:\nCourse Schedule Course Schedule II 6 Reference Topological Sort | Kahn\u0026rsquo;s Algorithm | Graph Theory Directed Acyclic Graph Hands-on Tutorial on Make Topological sorting ","permalink":"https://ramsayleung.github.io/en/post/2022/topological_sorting/","summary":"\u003ch2 id=\"definition\"\u003e\u003cspan class=\"section-num\"\u003e1\u003c/span\u003e Definition\u003c/h2\u003e\n\u003cp\u003eIn computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge \u003ccode\u003euv\u003c/code\u003e from vertex \u003ccode\u003eu\u003c/code\u003e to vertx \u003ccode\u003ev\u003c/code\u003e, \u003ccode\u003eu\u003c/code\u003e comes before \u003ccode\u003ev\u003c/code\u003e in the ordering.\u003c/p\u003e\n\u003cp\u003eIt sounds pretty academic, but I am sure you are using topological sort unconsciously every single day.\u003c/p\u003e\n\u003ch2 id=\"application\"\u003e\u003cspan class=\"section-num\"\u003e2\u003c/span\u003e Application\u003c/h2\u003e\n\u003cp\u003eMany real world situations can be modeled as a graph with directed edges where some events must occur before others. Then a topological sort gives an order in which to perform these events, for instance:\u003c/p\u003e","title":"Topological Sort"},{"content":"About me I\u0026rsquo;m Ramsay, a software engineer making a living by pressing keyboard, an amateur cook, an Emacs deadhead and Linux enthusiast.\nThe slogan of this site is In pursuit of Simplicity, because I prefer simple over complex\nProjects I\u0026rsquo;ve contributed multiple projects to open source community, I mainly focus on RSpotify right now due to the constrained energy.\nRSpotify: A Spotify Web API wrapper implemented in Rust. ","permalink":"https://ramsayleung.github.io/en/about_me/","summary":"\u003ch2 id=\"about-me\"\u003eAbout me\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m Ramsay, a software engineer making a living by pressing keyboard, an amateur cook, an Emacs deadhead and Linux enthusiast.\u003c/p\u003e\n\u003cp\u003eThe slogan of this site is \u003cstrong\u003e\u003cstrong\u003eIn pursuit of Simplicity\u003c/strong\u003e\u003c/strong\u003e, because I prefer simple over complex\u003c/p\u003e\n\u003ch2 id=\"projects\"\u003eProjects\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve contributed multiple projects to open source community, I mainly focus on RSpotify right now due to the constrained energy.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ramsayleung/rspotify\"\u003eRSpotify\u003c/a\u003e: A Spotify Web API wrapper implemented in Rust.\u003c/li\u003e\n\u003c/ul\u003e","title":"About Me"},{"content":"1 Preface I have been maintained a legacy distributed timer for months for my employer, then some important pay business are leveraging on it, with 1 billion tasks handled every day and 20k tasks added per second at most.\nEven though it\u0026rsquo;s old and full of black magic code, but it also also have insighted and well-designed code. Based on this old, running timer, I summarize and extract as this article, and it wont include any running code(perhaps pseudocode, and a lot of figures, as an adage says: A picture is worth a thousand words).\nif you are curious about the reason(I personally suggest to watch the TV series Silicon Valley, Richard has gave us a good example and answer)\n2 Design 2.1 Algorithm There are several algorithms in the world to implement timer, such as Red-Black Tree, Min-Heap and timer wheel. The most efficient and used algorithm is timer wheel algorithm, and it\u0026rsquo;s the algorithm we focus on.\nAs for timing wheel based timer, it can be modelled as two internal operations: per-tick bookkeeping and expiry processing.\nPer-tick bookkeeping: happens on every \u0026rsquo;tick\u0026rsquo; of the timer clock. If the unit of granularity for setting timers is T units of time (e.g. 1 second), then per-tick bookkeeping will happen every T units of time. It checks whether any outstanding timers have expired, and if so it removes them and invokes expiry processing. Expiry processing: is responsible for invoked the user-supplied callback (or other user requested action, depending on your model). 2.1.1 Simple Timing Wheels The simple timing wheel keeps a large timing wheel, the below timing wheel has 8 slots, and each slot is holding the task which is going to be expired. Supposing every slot presentes one second(one tick as a second), then the current slot is slot 1, if we want to add a task needed to be triggered 2s later, then this task will be inserted into slot 3.\nper-tick bookkeeping: O(1) What happen if we want to add a task needed to be launched 20s later, the answer is we have no way to do so since there are only 8 slots. So if we have a large period of timer task, we have to maintain a large timing wheel with tons of slots, which requires exponential amount of memory.\n2.1.2 Hashed Timing Wheel Hashed Timing Wheel is an improved simple timing wheel. As we mentioned before, it will consume large resources if timer period is comparatively large. Instead of using one slot per time unit, we could use a form of hashing instead. Construct a circular buffer with a fixed number of slots(such as 8 slots). If current slot is 0, we want store 3s later task, we could insert into slot 3, then if we want bookkeep 9s-later task, we could insert into slot 1(9 % 8 = 1)\nper-tick bookkeeping: O(1) - O(N) It\u0026rsquo;s a tradeoff strategy, We trade space with time.\n2.1.3 Hierarchical Timing Wheels Since simple timing wheels and hashed timing wheel come with drawback of time efficiency or space efficiency. Back to 1987, after studying a number of different approaches for the efficient management of timers, Varghese and Lauck posted a paper to introduce Hierarchical Timing Wheels\nJust make a long story short, I won\u0026rsquo;t dive deep into hierarchical timing wheels, you could easily understand it by a real life reference: the old water meter\nthe firse level wheel(seconds wheel) rotates one loop, triggering the second level(minutes wheel) ticks one slot, same for the third level(hour wheel). Therefore, we present a day(60*60*24 seconds) with 60+60+24 slots. If we want to present a month, we only need to a four level wheel(month wheel) with 30 slots.\nper-tick bookkeeping: O(1) 2.2 Per-tick bookkeeping After introducing timing wheel algorithm, let\u0026rsquo;s go back to the topic about designing a reliable distributed timer, it\u0026rsquo;s essential to decide how to store timer task. Taking implementation complexity and time, space trade off, we choose the Hashed Timing Wheel algorithm.\nThere are several internal components developed by my employer, one of them is named TableKV, a high-availability(99.999% ~ 99.9999%) NoSql service. TableKV supports 10m buckets(the terminology is table) at most, every table comes with full ACID properties of transactions support. You could simply replace TableKV with Redis as it provides the similar bucket functionality.\n2.2.1 Insert task into slot We are going to implement Hashed Timing Wheel algorithm with TableKV, supposing there are 10m buckets, and current time is 2021:08:05 11:17:33 +08=(the UNIX timestamp is =1628176653), there is a timer task which is going to be triggered 10s later with start_time = 1628176653 + 10 (or 100000010s later, start_time = 1628176653 + 10 + 100000000), these tasks both will be stored into bucket start_time % 100000000 = 28176663\n2.2.2 Pull task out from slot As clock tick-tacking to 2021:08:05 11:17:43 +08(1628176663), we need to pull tasks out from slot by calculating the bucket number: current_timestamp(1628176663) % 100000000 = 28176663. After locating the bucket number, we find all tasks in bucket 28176663 with start_time \u0026lt; current_timestamp=, then we get all expected expiry tasks.\n2.3 Global clock and lock As we mentioned before, when the clock tick-tacks to current_time, we fetch all expiry tasks. When our service is running on a distributed system, it\u0026rsquo;s universal that we will have multiple hosts(physical machines or dockers), with multiple current_times on its machine. There is no guarantee that all clocks of multiple hosts synchronized by the same Network Time Server, then all clocks might be subtly different. Which current_time is correct?\nIn order to get the correct time, it\u0026rsquo;s necessary to maintain a monotonic global clock(Of course, it\u0026rsquo;s not the only way to go, there are several ways to handle time and order). Since everything we care about clock is Unix timestamp, we could maintain a global system clock represented by Unix timestamp. All machines request the global clock every second to get the current time, fetching the expiry tasks later.\nWell, are we done? Not yet, a new issue breaks into our design: if all machines can fetch the expiry tasks, these tasks will be processed more than one time, which will cause essential problems. We also need a mutex lock to guarantee only one machine can fetch the expiry task. You can implement both global clock and mutex lock by a magnificent strategy: an Optimistic lock\nAll machines fetch global timestamp(timestamp A) with version All machines increase timestamp(timestamp B) and update version(optimistic locking), only one machine will success because of optimistic locking. Then the machine acquired mutex is authorized to fetch expiry tasks with timestamp A, the other machines failed to acquire mutex is suspended to wait for 1 seconds. Loop back to step 1 with timestamp B. We could encapsulate the role who keep acquiring lock and fetch expiry data as an individual component named scheduler.\n2.4 Expiry processing Expiry processing is responsible for invoked the user-supplied callback or other user requested action. In distributed computing, it\u0026rsquo;s common to execute a procedure by RPC(Remote Procedure Call). In our case, A RPC request is executed when timer task is expiry, from timer service to callback service. Thus, the caller(user) needs to explicitly tell the timer, which service should I execute with what kind of parameters data while the timer task is triggered.\nWe could pack and serialize this meta information and parameters data into binary data, and send it to the timer. When pulling data out from slot, the timer could reconstruct Request/Response/Client type and set it with user-defined data, the next step is a piece of cake, just executing it without saying.\nPerhaps there are many expiry tasks needed to triggered, in order to handle as many tasks as possible, you could create a thread pool, process pool, coroutine pool to execute RPC concurrently.\n2.5 Decoupling Supposing the callback service needs tons of operation, it takes a hundred of millisecond. Even though you have created a thread/process/coroutine pool to handle the timer task, it will inevitably hang, resulting in the decrease of throughout.\nAs for this heavyweight processing case, Message Queue is a great answer. Message queues can significantly simplify coding of decoupled services, while improving performance, reliability and scalability. It\u0026rsquo;s common to combine message queues with Pub/Sub messaging design pattern, timer could publish task data as message, and timer subscribes the same topic of message, using message queue as a buffer. Then in subscriber, the RPC client executes to request for callback service.\nAfter introducing message queue, we could outline the state machine of timer task:\nThanks to message queue, we are able to buffer, to retry or to batch work, and to smooth spiky workloads\n2.6 High availability guarantee 2.6.1 Missed expiry tasks A missed expiry of tasks may occur because of the scheduler process being shutdown or being crashed, or because of other unknown problems. One important job is how to locate these missed tasks and re-execute them. Since we are using global `current_timestamp` to fetch expiry data, we could have another scheduler to use `delay_10min_timestamp` to fetch missed expiry data.\nIn order to look for a needle in a haystack, we need to set a range(delay_10min - current time), and then to batch find cross buckets. After finding these missed tasks, the timer publishes them as a message to message queue. For other open source distributed timer projects like Quartz, which provides an instruction to handle missed(misfire) tasks: Misfire instructions\nIf your NoSql component doesn\u0026rsquo;t support find-cross-buckets feature, you could also find every bucket in the range one by one.\n2.6.2 Callback service error Since the distributed systems are shared-nothing systems, they communicate via message passing through a network(asynchronously or synchronously), but the network is unreliable. When invoking the user-supplied callback, the RPC request might fail if the network is cut off for a while or the callback service is temporarily down.\nRetries are a technique that helps us deal with transient errors, i.e. errors that are temporary and are likely to disappear soon. Retries help us achieve resiliency by allowing the system to send a request repeatedly until it gets an explicit response(success or fail). By leveraging message queue, you obtain the ability for retrying for free. In the meanwhile, the timer could handle the user-requested retries: It\u0026rsquo;s not the proper time to execute callback service, retry it later.\n3 Conclusion After a long way, we are finally here. The final full architecture would look like this:\nThe whole process:\nAdding a timer task, with specified meta info and task info Inserting task into bucket by hashed timing wheel algorithm(With task_state set to pending) Fetch_current scheduler tries to acquire lock and get global current time The Acquired lock scheduler fetches expiry tasks Return the expected data. \u0026amp; 7. Publishing task data as message to MQ with thread pool; And then set task_state to delivered Message subscriber pulls message from MQ Sending RPC request to callback service(set task_state to success or fail) Retry(If necessary) Wish you have fun and profit\n4 Reference Paper: Hashed and Hierarchical Timing Wheels: Efficient Data Structures for Implementing a Timer Facility Hashed and Hierarchical Timing Wheels ","permalink":"https://ramsayleung.github.io/en/post/2021/how_to_design_a_reliable_distributed_timer/","summary":"\u003ch2 id=\"preface\"\u003e\u003cspan class=\"section-num\"\u003e1\u003c/span\u003e Preface\u003c/h2\u003e\n\u003cp\u003eI have been maintained a legacy distributed timer for months for my employer, then some important pay business are leveraging on it, with 1 billion tasks handled every day and 20k tasks added per second at most.\u003c/p\u003e\n\u003cp\u003eEven though it\u0026rsquo;s old and full of black magic code, but it also also have insighted and well-designed code. Based on this old, running timer, I summarize and extract as this article, and it wont include any running code(perhaps pseudocode, and a lot of figures, as an adage says: \u003cem\u003eA picture is worth a thousand words\u003c/em\u003e).\u003c/p\u003e","title":"How To Design A Reliable Distributed Timer"},{"content":"Iterate through pagination in the Rest API\n1 Preface About 4 months ago, icewind1991 created an exciting PR that adding Stream/Iterator based versions of methods with paginated results, which makes enpoints in Rspotify more much ergonomic to use, and Mario completed this PR.\nIn order to know what this PR brought to us, we have to go back to the orignal story, the paginated results in Spotify\u0026rsquo;s Rest API.\n2 Orignal Story Taking the artist_albums as example, it gets Spotify catalog information about an artist\u0026rsquo;s albums.\nThe HTTP response body for this endpoint contains an array of simplified album object wrapped in a paging object and use limit field to control the number of album objects to return and offset field to set the index of the first album to return.\nSo designed endpoint in Rspotify looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 /// Paging object /// /// [Reference](https://developer.spotify.com/documentation/web-api/reference/#object-pagingobject) #[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct Page\u0026lt;T\u0026gt; { pub href: String, pub items: Vec\u0026lt;T\u0026gt;, pub limit: u32, pub next: Option\u0026lt;String\u0026gt;, pub offset: u32, pub previous: Option\u0026lt;String\u0026gt;, pub total: u32, } /// Get Spotify catalog information about an artist\u0026#39;s albums. /// /// Parameters: /// - artist_id - the artist ID, URI or URL /// - album_type - \u0026#39;album\u0026#39;, \u0026#39;single\u0026#39;, \u0026#39;appears_on\u0026#39;, \u0026#39;compilation\u0026#39; /// - market - limit the response to one particular country. /// - limit - the number of albums to return /// - offset - the index of the first album to return /// [Reference](https://developer.spotify.com/documentation/web-api/reference/#endpoint-get-an-artists-albums) pub fn artist_albums\u0026lt;\u0026#39;a\u0026gt;( \u0026amp;\u0026#39;a self, artist_id: \u0026amp;\u0026#39;a ArtistId, album_type: Option\u0026lt;\u0026amp;\u0026#39;a AlbumType\u0026gt;, market: Option\u0026lt;\u0026amp;\u0026#39;a Market\u0026gt;, ) -\u0026gt; ClientResult\u0026lt;Page\u0026lt;SimplifiedAlbum\u0026gt;\u0026gt;; Supposing that you fetched the first page of an artist\u0026rsquo;s ablums, then you would to get the data of the next page, you have to parse a URL:\n1 2 3 { \u0026#34;next\u0026#34;: \u0026#34;https://api.spotify.com/v1/browse/categories?offset=2\u0026amp;limit=20\u0026#34; } You have to parse the URL and extract limit and offset parameters, and recall the artist_albums endpoint with setting limit to 20 and offset to 2.\nWe have to manually fetch the data again and again until all datas have been consumed. It is not elegant, but works.\n3 Iterator Story Since we have the basic knowledge about the background, let\u0026rsquo;s jump to the iterator version of pagination endpoints.\nFirst of all, the iterator pattern allows us to perform some tasks on a sequence of items in turn. An iterator is responsible for the logic of itreating over each item and determining when the sequence has finished.\nIf you want to know about about Iterator, Jon Gjengset has covered a brilliant tutorial to demonstrate Iterators in Rust.\nAll iterators implement a trait named Iterator that is defined in the standard library. The definition of the trait looks like this:\n1 2 3 4 5 6 7 pub trait Iterator { type Item; fn next(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt;; // methods with default implementations elided } By implementing the Iterator trait on our own types, we could have iterators that do anything we want. Then working mechanism we want to iterate over paginated result will look like this:\nNow let\u0026rsquo;s dive deep into the code, we need to implement Iterator for our own types, the pseudocode looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 impl\u0026lt;T\u0026gt; Iterator for PageIterator\u0026lt;Request\u0026gt; { type Item = ClientResult\u0026lt;Page\u0026lt;T\u0026gt;\u0026gt;; fn next(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt; { match call endpoints with offset and limit { Ok(page) if page.items.is_empty() =\u0026gt; { we are done here None } Ok(page) =\u0026gt; { offset += page.items.len() as u32; Some(Ok(page)) } Err(e) =\u0026gt; Some(Err(e)), } } } In order to iterate paginated result from different endpoints, we need a generic type to represent different endpoints. The Fn trait comes to our mind, the function pointer that points to code, not data.\nThen the next version of pseudocode looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 impl\u0026lt;T, Request\u0026gt; Iterator for PageIterator\u0026lt;Request\u0026gt; where Request: Fn(u32, u32) -\u0026gt; ClientResult\u0026lt;Page\u0026lt;T\u0026gt;\u0026gt;, { type Item = ClientResult\u0026lt;Page\u0026lt;T\u0026gt;\u0026gt;; fn next(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt; { match (function_pointer)(offset and limit) { Ok(page) if page.items.is_empty() =\u0026gt; { we are done here None } Ok(page) =\u0026gt; { offset += page.items.len() as u32; Some(Ok(page)) } Err(e) =\u0026gt; Some(Err(e)), } } } Now, our iterator story has iterated to the end, the next item is that current full version code is here, check it if you are interested in :)\n4 Stream Story Are we done? Not yet. Let\u0026rsquo;s move our eyes to stream story.\nThe stream story is mostly similar with iterator story, except that iterator is synchronous, stream is asynchronous.\nThe Stream trait can yield multiple values before completing, similiar to the Iterator trait.\n1 2 3 4 5 6 7 8 9 10 trait Stream { /// The type of the value yielded by the stream. type Item; /// Attempt to resolve the next item in the stream. /// Returns `Poll::Pending` if not ready, `Poll::Ready(Some(x))` if a value /// is ready, and `Poll::Ready(None)` if the stream has completed. fn poll_next(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Option\u0026lt;Self::Item\u0026gt;\u0026gt;; } Since we have already known the iterator, let make the stream story short. We leverage the async-stream for using macro as Syntactic sugar to avoid clumsy type declaration and notation.\nWe use stream! macro to generate an anonymous type implementing the Stream trait, and the Item associated type is the type of the values yielded from the stream, which is ClientResult\u0026lt;T\u0026gt; in this case.\nThe stream full version is shorter and clearer:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 /// This is used to handle paginated requests automatically. pub fn paginate\u0026lt;T, Fut, Request\u0026gt;( req: Request, page_size: u32, ) -\u0026gt; impl Stream\u0026lt;Item = ClientResult\u0026lt;T\u0026gt;\u0026gt; where T: Unpin, Fut: Future\u0026lt;Output = ClientResult\u0026lt;Page\u0026lt;T\u0026gt;\u0026gt;\u0026gt;, Request: Fn(u32, u32) -\u0026gt; Fut, { use async_stream::stream; let mut offset = 0; stream! { loop { let page = req(page_size, offset).await?; offset += page.items.len() as u32; for item in page.items { yield Ok(item); } if page.next.is_none() { break; } } } } 5 Appendix Whew! It took more than I expected. Since iterators is the Rust features inspired by functional programming language ideas, which contributes to Rust\u0026rsquo;s capability to clearly express high-level ideas at low-level performance.\nIt\u0026rsquo;s good to leverage iterators wherever possible, now we can be thrilled to say that all endpoints don\u0026rsquo;t need to manuallly loop over anymore, they are all iterable and rusty.\nThanks Mario and icewind1991 again for their works :)\n","permalink":"https://ramsayleung.github.io/en/post/2021/iterate_through_pagination_api/","summary":"\u003cp\u003eIterate through pagination in the Rest API\u003c/p\u003e\n\u003ch2 id=\"preface\"\u003e\u003cspan class=\"section-num\"\u003e1\u003c/span\u003e Preface\u003c/h2\u003e\n\u003cp\u003eAbout 4 months ago, \u003ca href=\"https://github.com/icewind1991\"\u003eicewind1991\u003c/a\u003e created an exciting \u003ca href=\"https://github.com/ramsayleung/rspotify/pull/166\"\u003ePR\u003c/a\u003e that adding \u003ccode\u003eStream/Iterator\u003c/code\u003e based versions of methods with paginated results, which makes enpoints in \u003ca href=\"https://github.com/ramsayleung/rspotify\"\u003eRspotify\u003c/a\u003e more much ergonomic to use, and \u003ca href=\"https://github.com/marioortizmanero\"\u003eMario\u003c/a\u003e completed this PR.\u003c/p\u003e\n\u003cp\u003eIn order to know what this PR brought to us, we have to go back to the orignal story, the paginated results in Spotify\u0026rsquo;s Rest API.\u003c/p\u003e\n\u003ch2 id=\"orignal-story\"\u003e\u003cspan class=\"section-num\"\u003e2\u003c/span\u003e Orignal Story\u003c/h2\u003e\n\u003cp\u003eTaking the \u003ccode\u003eartist_albums\u003c/code\u003e as example, it gets Spotify catalog information about an artist\u0026rsquo;s albums.\u003c/p\u003e","title":"Let's make everything iterable"},{"content":"The lesson learned from refactoring rspotify\n1 Preface Recently, I and Mario are working on refactoring rspotify, trying to improve performance, documentation, error-handling, data model and reduce compile time, to make it easier to use. (For those who has never heard about rspotify, it is a Spotify HTTP SDK implemented in Rust).\nI am partly focusing on polishing the data model, based on the issue created by Koxiaet.\nSince rspotify is API client for Spotify, it has to handle the request and response from Spotify HTTP API.\nGenerally speaking, the data model is something about how to structure the response data, and used Serde to parse JSON response from HTTP API to Rust struct, and I have learnt a lot Serde tricks from refactoring.\n2 Serde Lesson 2.1 Deserialize JSON map to Vec based on its value. An actions object which contains a disallows object, allows to update the user interface based on which playback actions are available within the current context.\nThe response JSON data from HTTP API:\n1 2 3 4 5 6 7 { ... \u0026#34;disallows\u0026#34;: { \u0026#34;resuming\u0026#34;: true } ... } The original model representing actions was:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #[derive(Clone, Debug, Serialize, PartialEq, Eq)] pub struct Actions { pub disallows: HashMap\u0026lt;DisallowKey, bool\u0026gt; } #[derive(Clone, Serialize, Deserialize, Copy, PartialEq, Eq, Debug, Hash, ToString)] #[serde(rename_all = \u0026#34;snake_case\u0026#34;)] #[strum(serialize_all = \u0026#34;snake_case\u0026#34;)] pub enum DisallowKey { InterruptingPlayback, Pausing, Resuming, ... } And Koxiaet gave great advice about how to polish Actions:\nActions::disallows can be replaced with a Vec\u0026lt;DisallowKey\u0026gt; or HashSet\u0026lt;DisallowKey\u0026gt; by removing all entires whose value is false, which will result in a simpler API.\nTo be honest, I was not that familiar with Serde before, after digging in its official documentation for a while, it seems there is now a built-in way to convert JSON map to Vec\u0026lt;T\u0026gt; base on map\u0026rsquo;s value.\nAfter reading the Custom serialization from documentation, there was a simple solution came to my mind, so I wrote my first customized deserialize function.\nI created a dumb Actions struct inside the deserialize function, and converted HashMap to Vec by filtering its value.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #[derive(Clone, Debug, Serialize, PartialEq, Eq)] pub struct Actions { pub disallows: Vec\u0026lt;DisallowKey\u0026gt;, } impl\u0026lt;\u0026#39;de\u0026gt; Deserialize\u0026lt;\u0026#39;de\u0026gt; for Actions { fn deserialize\u0026lt;D\u0026gt;(deserializer: D) -\u0026gt; Result\u0026lt;Self, D::Error\u0026gt; where D: Deserializer\u0026lt;\u0026#39;de\u0026gt;, { #[derive(Deserialize)] struct OriginalActions { pub disallows: HashMap\u0026lt;DisallowKey, bool\u0026gt;, } let orignal_actions = OriginalActions::deserialize(deserializer)?; Ok(Actions { disallows: orignal_actions .disallows .into_iter() .filter(|(_, value)| *value) .map(|(key, _)| key) .collect(), }) } } The types should be familiar if you\u0026rsquo;ve used Serde before.\nIf you\u0026rsquo;re not used to Rust then the function signature will likely look a little strange. What it\u0026rsquo;s trying to tell is that d will be something that implements Serde\u0026rsquo;s Deserializer trait, and that any references to memory will live for the 'de lifetime.\n2.2 Deserialize Unix milliseconds timestamp to Datetime A currently playing object which contains information about currently playing item, and the timestamp field is an integer, representing the Unix millisecond timestamp when data was fetched.\nThe response JSON data from HTTP API:\n1 2 3 4 5 6 7 8 9 10 11 12 13 { ... \u0026#34;timestamp\u0026#34;: 1490252122574, \u0026#34;progress_ms\u0026#34;: 44272, \u0026#34;is_playing\u0026#34;: true, \u0026#34;currently_playing_type\u0026#34;: \u0026#34;track\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;disallows\u0026#34;: { \u0026#34;resuming\u0026#34;: true } } ... } The original model was:\n1 2 3 4 5 6 7 8 9 10 11 12 /// Currently playing object /// /// [Reference](https://developer.spotify.com/documentation/web-api/reference/player/get-the-users-currently-playing-track/) #[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct CurrentlyPlayingContext { pub timestamp: u64, pub progress_ms: Option\u0026lt;u32\u0026gt;, pub is_playing: bool, pub item: Option\u0026lt;PlayingItem\u0026gt;, pub currently_playing_type: CurrentlyPlayingType, pub actions: Actions, } As before, Koxiaet made a great point about timestamp and =progress_ms=(I will talk about it later):\nCurrentlyPlayingContext::timestamp should be a chrono::DateTime\u0026lt;Utc\u0026gt;, which could be easier to use.\nThe polished struct looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct CurrentlyPlayingContext { pub context: Option\u0026lt;Context\u0026gt;, #[serde( deserialize_with = \u0026#34;from_millisecond_timestamp\u0026#34;, serialize_with = \u0026#34;to_millisecond_timestamp\u0026#34; )] pub timestamp: DateTime\u0026lt;Utc\u0026gt;, pub progress_ms: Option\u0026lt;u32\u0026gt;, pub is_playing: bool, pub item: Option\u0026lt;PlayingItem\u0026gt;, pub currently_playing_type: CurrentlyPlayingType, pub actions: Actions, } Using the deserialize_with attribute tells Serde to use custom deserialization code for the timestamp field. The from_millisecond_timestamp code is:\n1 2 3 4 5 6 7 /// Deserialize Unix millisecond timestamp to `DateTime\u0026lt;Utc\u0026gt;` pub(in crate) fn from_millisecond_timestamp\u0026lt;\u0026#39;de, D\u0026gt;(d: D) -\u0026gt; Result\u0026lt;DateTime\u0026lt;Utc\u0026gt;, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { d.deserialize_u64(DateTimeVisitor) } The code calls d.deserialize_u64 passing in a struct. The passed in struct implements Serde\u0026rsquo;s Visitor, and look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // Vistor to help deserialize unix millisecond timestamp to `chrono::DateTime` struct DateTimeVisitor; impl\u0026lt;\u0026#39;de\u0026gt; de::Visitor\u0026lt;\u0026#39;de\u0026gt; for DateTimeVisitor { type Value = DateTime\u0026lt;Utc\u0026gt;; fn expecting(\u0026amp;self, formatter: \u0026amp;mut fmt::Formatter) -\u0026gt; fmt::Result { write!( formatter, \u0026#34;an unix millisecond timestamp represents DataTime\u0026lt;UTC\u0026gt;\u0026#34; ) } fn visit_u64\u0026lt;E\u0026gt;(self, v: u64) -\u0026gt; Result\u0026lt;Self::Value, E\u0026gt; where E: de::Error, { ... } } The struct DateTimeVisitor doesn\u0026rsquo;t have any fields, it just a type implemented the custom visitor which delegates to parse the u64.\nSince there is no way to construct DataTime directly from Unix millisecond timestamp, I have to figure out how to handle the construction. And it turns out that there is a way to construct DateTime from seconds and nanoseconds:\n1 2 3 use chrono::{DateTime, TimeZone, NaiveDateTime, Utc}; let dt = DateTime::\u0026lt;Utc\u0026gt;::from_utc(NaiveDateTime::from_timestamp(61, 0), Utc); Thus, what I need to do is just convert millisecond to second and nanosecond:\n1 2 3 4 5 6 7 8 9 10 11 12 13 fn visit_u64\u0026lt;E\u0026gt;(self, v: u64) -\u0026gt; Result\u0026lt;Self::Value, E\u0026gt; where E: de::Error, { let second = (v - v % 1000) / 1000; let nanosecond = ((v % 1000) * 1000000) as u32; // The maximum value of i64 is large enough to hold millisecond, so it would be safe to convert it i64 let dt = DateTime::\u0026lt;Utc\u0026gt;::from_utc( NaiveDateTime::from_timestamp(second as i64, nanosecond), Utc, ); Ok(dt) } The to_millisecond_timestamp function is similar to from_millisecond_timestamp, but it\u0026rsquo;s eaiser to implement, check this PR for more detail.\n2.3 Deserialize milliseconds to Duration The simplified episode object contains the simplified episode information, and the duration_ms field is an integer, which represents the episode length in milliseconds.\nThe response JSON data from HTTP API:\n1 2 3 4 5 6 7 8 { ... \u0026#34;audio_preview_url\u0026#34; : \u0026#34;https://p.scdn.co/mp3-preview/83bc7f2d40e850582a4ca118b33c256358de06ff\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;FÃ¶lj med Tobias Svanelid till Sveriges Ã¤ldsta tegelkyrka\u0026#34; \u0026#34;duration_ms\u0026#34; : 2685023, \u0026#34;explicit\u0026#34; : false, ... } The original model was\n1 2 3 4 5 6 7 #[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct SimplifiedEpisode { pub audio_preview_url: Option\u0026lt;String\u0026gt;, pub description: String, pub duration_ms: u32, ... } As before without saying, Koxiaet pointed out that\nSimplifiedEpisode::duration_ms should be replaced with a duration of type Duration, since a built-in Duration type works better than primitive type.\nSince I have worked with Serde\u0026rsquo;s custome deserialization, it\u0026rsquo;s not a hard job for me any more. I easily figure out how to deserialize u64 to Duration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct SimplifiedEpisode { pub audio_preview_url: Option\u0026lt;String\u0026gt;, pub description: String, #[serde( deserialize_with = \u0026#34;from_duration_ms\u0026#34;, serialize_with = \u0026#34;to_duration_ms\u0026#34;, rename = \u0026#34;duration_ms\u0026#34; )] pub duration: Duration, ... } /// Vistor to help deserialize duration represented as millisecond to `std::time::Duration` struct DurationVisitor; impl\u0026lt;\u0026#39;de\u0026gt; de::Visitor\u0026lt;\u0026#39;de\u0026gt; for DurationVisitor { type Value = Duration; fn expecting(\u0026amp;self, formatter: \u0026amp;mut fmt::Formatter) -\u0026gt; fmt::Result { write!(formatter, \u0026#34;a milliseconds represents std::time::Duration\u0026#34;) } fn visit_u64\u0026lt;E\u0026gt;(self, v: u64) -\u0026gt; Result\u0026lt;Self::Value, E\u0026gt; where E: de::Error, { Ok(Duration::from_millis(v)) } } /// Deserialize `std::time::Duration` from millisecond(represented as u64) pub(in crate) fn from_duration_ms\u0026lt;\u0026#39;de, D\u0026gt;(d: D) -\u0026gt; Result\u0026lt;Duration, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { d.deserialize_u64(DurationVisitor) } Now, the life is easier than before.\n2.4 Deserialize milliseconds to Option Let\u0026rsquo;s go back to CurrentlyPlayingContext model, since we have replaced millisecond (represents as u32) with Duration, it makes sense to replace all millisecond fields to Duration.\nBut hold on, it seems progress_ms field is a bit different.\nThe progress_ms field is either not present or a millisecond, the u32 handles the milliseconds, as its value might not be present in the response, it\u0026rsquo;s an Option\u0026lt;u32\u0026gt;, so it won\u0026rsquo;t work with from_duration_ms.\nThus, it\u0026rsquo;s necessary to figure out how to handle the Option type, and the answer is in the documentation, the deserialize_option function:\nHint that the Deserialize type is expecting an optional value.\nThis allows deserializers that encode an optional value as a nullable value to convert the null value into None and a regular value into Some(value).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct CurrentlyPlayingContext { pub context: Option\u0026lt;Context\u0026gt;, #[serde( deserialize_with = \u0026#34;from_millisecond_timestamp\u0026#34;, serialize_with = \u0026#34;to_millisecond_timestamp\u0026#34; )] pub timestamp: DateTime\u0026lt;Utc\u0026gt;, #[serde(default)] #[serde( deserialize_with = \u0026#34;from_option_duration_ms\u0026#34;, serialize_with = \u0026#34;to_option_duration_ms\u0026#34;, rename = \u0026#34;progress_ms\u0026#34; )] pub progress: Option\u0026lt;Duration\u0026gt;, } /// Deserialize `Option\u0026lt;std::time::Duration\u0026gt;` from millisecond(represented as u64) pub(in crate) fn from_option_duration_ms\u0026lt;\u0026#39;de, D\u0026gt;(d: D) -\u0026gt; Result\u0026lt;Option\u0026lt;Duration\u0026gt;, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { d.deserialize_option(OptionDurationVisitor) } As before, the OptionDurationVisitor is an empty struct implemented Visitor trait, but key point is in order to work with deserialize_option, the OptionDurationVisitor has to implement the visit_none and visit_some method:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 impl\u0026lt;\u0026#39;de\u0026gt; de::Visitor\u0026lt;\u0026#39;de\u0026gt; for OptionDurationVisitor { type Value = Option\u0026lt;Duration\u0026gt;; fn expecting(\u0026amp;self, formatter: \u0026amp;mut fmt::Formatter) -\u0026gt; fmt::Result { write!( formatter, \u0026#34;a optional milliseconds represents std::time::Duration\u0026#34; ) } fn visit_none\u0026lt;E\u0026gt;(self) -\u0026gt; Result\u0026lt;Self::Value, E\u0026gt; where E: de::Error, { Ok(None) } fn visit_some\u0026lt;D\u0026gt;(self, deserializer: D) -\u0026gt; Result\u0026lt;Self::Value, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { Ok(Some(deserializer.deserialize_u64(DurationVisitor)?)) } } The visit_none method return Ok(None) so the progress value in the struct will be None, and the visit_some delegates the parsing logic to DurationVisitor via the deserialize_u64 call, so deserializing Some(u64) works like the u64.\n2.5 Deserialize enum from number An AudioAnalysisSection model contains a mode field, which indicates the modality(major or minor) of a track, the type of scle from which its melodic content is derived. This field will contain a 0 for minor, a 1 for major, or a -1 for no result.\nThe response JSON data from HTTP API:\n1 2 3 4 5 6 { ... \u0026#34;mode\u0026#34;: 0, \u0026#34;mode_confidence\u0026#34;: 0.414, ... } The original struct representing AudioAnalysisSection was like this, since mode field was stored into a f32=(=f8 was a better choice for this case):\n1 2 3 4 5 6 7 #[derive(Clone, Debug, Serialize, Deserialize, PartialEq)] pub struct AudioAnalysisSection { ... pub mode: f32, pub mode_confidence: f32, ... } Koxiaet made a great point about mode field:\nAudioAnalysisSection::mode and AudioFeatures::mode are f32=s but should be =Option\u0026lt;Mode\u0026gt;=s where =enum Mode { Major, Minor } as it is more useful.\nIn this case, we don\u0026rsquo;t need the Opiton type and in order to deserialize enum from number, we firstly need to define a C-like enum:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 pub enum Modality { #[serde(rename = \u0026#34;0\u0026#34;)] Minor = 0, #[serde(rename = \u0026#34;1\u0026#34;)] Major = 1, #[serde(rename = \u0026#34;1\u0026#34;)] NoResult = -1, } pub struct AudioAnalysisSection { ... pub mode: Modality, pub mode_confidence: f32, ... } And then, what\u0026rsquo;s the next step? It seems serde doesn\u0026rsquo;t allow C-like enums to be formatted as integers rather that strings in JSON natively:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 working version: { ... \u0026#34;mode\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;mode_confidence\u0026#34;: 0.414, ... } failed version: { ... \u0026#34;mode\u0026#34;: 0, \u0026#34;mode_confidence\u0026#34;: 0.414, ... } Then the failed version is exactly what we want. I know that the serde\u0026rsquo;s official documentation has a solution for this case, the serde_repr crate provides alternative derive macros that derive the same Serialize and Deserialize traits but delegate to the underlying representation of a C-like enum.\nSince we are trying to reduce the compiled time of rspotify, so we are cautious about introducing new dependencies. So a custom-made serialize function would be a better choice, it just needs to match the number, and convert to a related enum value.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 /// Deserialize/Serialize `Modality` to integer(0, 1, -1). pub(in crate) mod modality { use super::enums::Modality; use serde::{de, Deserialize, Serializer}; pub fn deserialize\u0026lt;\u0026#39;de, D\u0026gt;(d: D) -\u0026gt; Result\u0026lt;Modality, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { let v = i8::deserialize(d)?; match v { 0 =\u0026gt; Ok(Modality::Minor), 1 =\u0026gt; Ok(Modality::Major), -1 =\u0026gt; Ok(Modality::NoResult), _ =\u0026gt; Err(de::Error::invalid_value( de::Unexpected::Signed(v.into()), \u0026amp;\u0026#34;valid value: 0, 1, -1\u0026#34;, )), } } pub fn serialize\u0026lt;S\u0026gt;(x: \u0026amp;Modality, s: S) -\u0026gt; Result\u0026lt;S::Ok, S::Error\u0026gt; where S: Serializer, { match x { Modality::Minor =\u0026gt; s.serialize_i8(0), Modality::Major =\u0026gt; s.serialize_i8(1), Modality::NoResult =\u0026gt; s.serialize_i8(-1), } } } 3 Move into module Update:\n2021-01-15\nfrom(to)_millisecond_timestamp have been moved into its module millisecond_timestamp and rename them to deserialize \u0026amp; serialize from(to)_duration_ms have been moved into its module duration_ms and rename them to deserialize \u0026amp; serialize from(to)_option_duration_ms have been moved into its module option_duration_ms and rename them to deserialize \u0026amp; serialize 4 Summary To be honest, it\u0026rsquo;s the first time I have needed some customized works, which took me some time to understand how does Serde works. Finally, all investments paid off, it works great now.\nSerde is such an awesome deserialize/serialize framework which I have learnt a lot of from and still have a lot of to learn from.\n5 Reference Deserializing optional datetimes with serde PR: Keep polishing the models PR: Refactor model PR: Deserialize enum from number ","permalink":"https://ramsayleung.github.io/en/post/2020/serde_lesson/","summary":"\u003cp\u003eThe lesson learned from refactoring rspotify\u003c/p\u003e\n\u003ch2 id=\"preface\"\u003e\u003cspan class=\"section-num\"\u003e1\u003c/span\u003e Preface\u003c/h2\u003e\n\u003cp\u003eRecently, I and \u003ca href=\"https://github.com/marioortizmanero\"\u003eMario\u003c/a\u003e are working on refactoring \u003ca href=\"https://github.com/ramsayleung/rspotify\"\u003e\u003ccode\u003erspotify\u003c/code\u003e\u003c/a\u003e, trying to improve performance, documentation, error-handling, data model and reduce compile time, to make it easier to use. (For those who has never heard about \u003ccode\u003erspotify\u003c/code\u003e, it is a Spotify HTTP SDK implemented in Rust).\u003c/p\u003e\n\u003cp\u003eI am partly focusing on polishing the data model, based on the \u003ca href=\"https://github.com/ramsayleung/rspotify/issues/127\"\u003eissue\u003c/a\u003e created by \u003ca href=\"https://github.com/Koxiaet\"\u003eKoxiaet\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSince \u003ccode\u003erspotify\u003c/code\u003e is API client for Spotify, it has to handle the request and response from Spotify HTTP API.\u003c/p\u003e","title":"Serde Tricks"},{"content":"1 Preface Today, I am exited to introduce you the v0.9 release I have been continued to work on it for the past few weeks that adds async/await support now!\n2 The road to async/await What is rspotify: \u0026gt; For those who has never heared about rspotify before, rspotify is a Spotify web Api wrapper implemented in Rust.\nWith async/await\u0026rsquo;s forthcoming stabilization and reqwest adds async/await support now, I think it\u0026rsquo;s time to let rspotify leverage power from async/await. To be honest, I was not familiar with async/await before, because of my Java background from where I just get used to multiple thread and sync stuff(Yes, I know Java has future either).\nAfter reading some good learning resources, such as Async book, Zero-cost Async IO, I started to step into the world of async/await. async/await is a way to write functions that can \u0026ldquo;pause\u0026rdquo;, return control to the runtime, ant then pick up from where they left off.\nI think perhaps the most important part of async/await is runtime, which defines how to schedule the functions.\nNow, by leveraging the async/await power of reqwest, rspotify could send HTTP request and handle response asynchronously.\nFuthermore, not only do I refactor the old blocking endpoint functions to async/await version, but also keep the old blocking endpoint functions with a new additional feature blocking, then other developers could choose API to their taste.\n3 Overview album example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 use rspotify::client::Spotify; use rspotify::oauth2::SpotifyClientCredentials; #[tokio::main] async fn main() { // Set client_id and client_secret in .env file or // export CLIENT_ID=\u0026#34;your client_id\u0026#34; // export CLIENT_SECRET=\u0026#34;secret\u0026#34; let client_credential = SpotifyClientCredentials::default().build(); // Or set client_id and client_secret explictly // let client_credential = SpotifyClientCredentials::default() // .client_id(\u0026#34;this-is-my-client-id\u0026#34;) // .client_secret(\u0026#34;this-is-my-client-secret\u0026#34;) // .build(); let spotify = Spotify::default() .client_credentials_manager(client_credential) .build(); let birdy_uri = \u0026#34;spotify:album:0sNOF9WDwhWunNAHPD3Baj\u0026#34;; let albums = spotify.album(birdy_uri).await; println!(\u0026#34;{:?}\u0026#34;, albums); } Just change the default API to async, and moving the previous synchronous API to blocking module.\nNotes that I think the v0.9 release of rspotify is going to be a huge break change because of the support for async/await, which definitely breaks backward compatibility.\nSo I decide to make an other break change into the next release, just refactoring the project structure to shorten the import path:\nbefore:\n1 2 use rspotify::spotify::client::Spotify; use rspotify::spotify::oauth2::SpotifyClientCredentials; after:\n1 2 use rspotify::client::Spotify; use rspotify::oauth2::SpotifyClientCredentials; the spotify module is unnecessary and inelegant, so I just remove it.\n4 Conclusion rspotify v0.9 is now available! There is documentation, examples and an issue tracker!\nPlease provide any feedback, as I would love to improve this library any way I can! Thanks @Alexander so much for actively participate in the refactor work for support async/await.\n","permalink":"https://ramsayleung.github.io/en/post/2020/async_await_for_rspotify/","summary":"\u003ch2 id=\"preface\"\u003e\u003cspan class=\"section-num\"\u003e1\u003c/span\u003e Preface\u003c/h2\u003e\n\u003cp\u003eToday, I am exited to introduce you the \u003ca href=\"https://github.com/ramsayleung/rspotify/releases/tag/v0.9\"\u003ev0.9\u003c/a\u003e release I have been continued to work on it for the past few weeks that\nadds \u003ccode\u003easync/await\u003c/code\u003e support now!\u003c/p\u003e\n\u003ch2 id=\"the-road-to-async-await\"\u003e\u003cspan class=\"section-num\"\u003e2\u003c/span\u003e The road to async/await\u003c/h2\u003e\n\u003cp\u003eWhat is rspotify: \u0026gt; For those who has never heared about rspotify\nbefore, \u003ca href=\"https://github.com/ramsayleung/rspotify\"\u003erspotify\u003c/a\u003e is a\nSpotify web Api wrapper implemented in Rust.\u003c/p\u003e\n\u003cp\u003eWith async/await\u0026rsquo;s forthcoming stabilization and reqwest adds\n\u003ccode\u003easync/await\u003c/code\u003e support now, I think it\u0026rsquo;s time to let rspotify leverage\npower from \u003ccode\u003easync/await\u003c/code\u003e. To be honest, I was not familiar with\n\u003ccode\u003easync/await\u003c/code\u003e before, because of my Java background from where I just\nget used to multiple thread and sync stuff(Yes, I know Java has \u003ccode\u003efuture\u003c/code\u003e\neither).\u003c/p\u003e","title":"rspotify has come to async/await"}]