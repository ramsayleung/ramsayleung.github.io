[{"content":"1 Goodbye 2023 As I farewelled to 2023, a year marked by numerous changes and personal evolution, I find myself recollecting the multitude of experiences that unfolded. My 2023 journey was nothing short of fascinating and exhilarating, prompting me to revisit the year from various angles. After seeing hoards of posts in social media generated by Github Contributions Chart, I thought I could also build an APP to summarize my Github contribution for every year for friends to have fun. I spent my entire 4-days-new-year vocation to build this app named: Github Summary. This project led me through a series of first-time experiences: first time to try Tailwind Css framework, first time to use and deploy project on Vercel, first time to build project on nextjs, first time to develop a public project on React(yes, I\u0026rsquo;ve tried to learn React for hundreds of times, but never get a chance to use it in real project), etc. 2 Happy 2024 While I hoped I could have completed this project by the close of 2023 to share summaries with friends, life\u0026rsquo;s timeline had other plans. Now, as we step into 2024, I am thrilled to publish the GitHub Summary. It\u0026rsquo;s never too late to showcase creative work, and this project is poised to generate insightful summaries not just for the past year but for the adventures that await in 2024. Wishing everyone a Happy New Year! Feel free to explore GitHub Summary: https://github-summary.vercel.app/ ","permalink":"https://ramsayleung.github.io/en/post/2024/github_summary/","summary":"1 Goodbye 2023 As I farewelled to 2023, a year marked by numerous changes and personal evolution, I find myself recollecting the multitude of experiences that unfolded. My 2023 journey was nothing short of fascinating and exhilarating, prompting me to revisit the year from various angles. After seeing hoards of posts in social media generated by Github Contributions Chart, I thought I could also build an APP to summarize my Github contribution for every year for friends to have fun.","title":"Rewind your Github summary"},{"content":"1 Introduction 1.1 IaC Infrastructure as code(IaC) is the managing and provisioning of infrastructure through code instead of manual processes, for example, clicking button, adding or editing roles in AWS console.\n1.2 AWS CloudFormation AWS CloudFormation is the original IaC tool for AWS, released in 2011, which uses template files to automate and mange the setup of AWS resources.\n1.3 AWS CDK AWS Cloud Development Kit(CDK) is a product provided by AWS that makes it easier for developers to manage their infrastructure with familiar programming languages like TypeScript, Python, Java, etc.\nAnd, CDK is standing on the shoulder of Cloudformation, providing tools for developers by leveraging Cloudformation.\nA stack is a collection of AWS resources that you can manage as a single unit, like a box.\nFor instance, this box could include all the resources required to run an application or Lambda service, such as S3 Buckets (storage), Roles (authorization), Lambda Function (computing), API Gateway (access point), Alarm, Monitoring, etc.\n2 Problem I am currently working on a project which requires to set up two stacks, one stack( GlueStack ) for defining a list of AWS Glue tables and the other stack( ServiceStack ) for definition of Lambda service and associated resources.\nIn fact, S3 bucket names have to be globally unique within a partition, which means crossing the whole AWS customer base.\nYou are unable to create a S3 bucket with bucket name which is in use by another AWS customer or your own account.\nSo it\u0026rsquo;s safer to let CloudFormation generate a random bucket name for a developer when he need to initialize a S3 bucket.\nHowever, there is new a problem I face: since the S3 bucket name is randomly generated characters, if GlueStack need to read the bucket created by ServiceStack, how could I share the bucket name between two stacks?\nWhile these two stacks are isolated and separated, resources collection.\n3 Solution Fortunately, CDK offers a facility named CfnOutput to export a deployed resource, so that the consumer of the resource is able to Import required resource.\nDefine the required resource in ServiceStack (producer), for instance, a S3 bucket: import { Bucket } from \u0026#39;aws-cdk-lib/aws-s3\u0026#39;; const s3Bucket = new Bucket(this, \u0026#39;MyBucketId\u0026#39;, {}); Export the resource by specifying the value and exportName: import { CfnOutput } from \u0026#39;aws-cdk-lib\u0026#39;; // export the generated bucket name to other stack new CfnOutput(this, \u0026#39;exportRequiredS3Bucket\u0026#39;, { value: s3Bucket.bucketName, exportName: \u0026#39;exportRequiredS3Bucket\u0026#39;, }); Import the required resource in GlueStack (consumer): import { Fn} from \u0026#39;aws-cdk-lib\u0026#39;; const requiredS3BucketName = Fn.importValue(\u0026#39;exportRequiredS3Bucket\u0026#39;); If we take a closer look at the synthesized CFN template for ServiceStack, we could find:\n\u0026#34;Outputs\u0026#34;: { \u0026#34;exportRequiredS3Bucket\u0026#34;: { \u0026#34;Value\u0026#34;: { \u0026#34;Ref\u0026#34;: \u0026#34;MyBucketId737FC949\u0026#34; }, \u0026#34;Export\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;exportRequiredS3Bucket\u0026#34; } } The synthesized CFN template for GlueStack:\n{ \u0026#34;Fn::ImportValue\u0026#34;: \u0026#34;exportRequiredS3Bucket\u0026#34; } This is the way about how to share value between two stacks.\n4 Loose couping solution Updated on 2023-12-02\nPeople learn from mistake.\nAfter applying this practice in my project, I recently learn that it\u0026rsquo;s not good practice to share resource across stack.\nWith using export/import, I tightly couple my stacks with a commitment that I can never update that unless I remove that couping later on.\nIt means it will become a disaster1 whenever I need to update/delete the S3Bucket, CloudFormation will raise an error, complaining something like: \u0026ldquo;ServiceStack cannot be deleted as it\u0026rsquo;s in use by GlueStack\u0026rdquo;.\nA better practice I learnt is adding a loose couping between ServiceStack and GlueStack by sharing a constant variable:\nDefine a constant variable somewhere:\nexport const Constants = { MyBucketName: \u0026#39;TestBucket\u0026#39; } Refine the definition of s3Bucket\nimport { Bucket } from \u0026#39;aws-cdk-lib/aws-s3\u0026#39;; const s3Bucket = new Bucket(this, \u0026#39;MyBucketId\u0026#39;, { bucketName: Constants.MyBucketName, }); Refer the s3Bucket in GlueStack by MyBucketName instead of CDK exported reference\nconst requiredS3BucketName = Constants.MyBucketName; Therefore, these two stacks are not directly coupled, but they are referencing the same constant variable.\nThen, CloudFormation won\u0026rsquo;t prevent you from updating the S3Bucket as there is not direct relation between these two stacks anymore.\nThis is the benefit of loose couping.\n5 Reference API Document: class CfnOutput (construct) AWS Cloud Development Kit (AWS CDK) v2 https://stackoverflow.com/questions/63350346/delete-resource-with-references\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ramsayleung.github.io/en/post/2023/how_to_share_resource_between_cdk_stacks/","summary":"1 Introduction 1.1 IaC Infrastructure as code(IaC) is the managing and provisioning of infrastructure through code instead of manual processes, for example, clicking button, adding or editing roles in AWS console.\n1.2 AWS CloudFormation AWS CloudFormation is the original IaC tool for AWS, released in 2011, which uses template files to automate and mange the setup of AWS resources.\n1.3 AWS CDK AWS Cloud Development Kit(CDK) is a product provided by AWS that makes it easier for developers to manage their infrastructure with familiar programming languages like TypeScript, Python, Java, etc.","title":"How to share resource between CDK stacks"},{"content":"1 Definition In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertx v, u comes before v in the ordering.\nIt sounds pretty academic, but I am sure you are using topological sort unconsciously every single day.\n2 Application Many real world situations can be modeled as a graph with directed edges where some events must occur before others. Then a topological sort gives an order in which to perform these events, for instance:\n2.1 College class prerequisites You must take course b first if you want to take course a. For example, in your alma mater, the student must complete PHYS:1511(College Physics) or PHYS:1611(Introductory Physics I) before taking College Physics II.\nThe courses can be represented by vertices, and there is an edge from College Physics to College Physics II since PHYS:1511 must be finished before College Physics II can be enrolled.\n2.2 Job scheduling scheduling a sequence of jobs or tasks based on their dependencies. The jobs are represented by vertices, and there is an edge from x to y if job x must be completed before job y can be started.\nIn the context of a CI/CD pipeline, the relationships between jobs can be represented by directed graph(specifically speaking, by directed acyclic graph). For example, in a CI pipeline, build job should be finished before start test job and lint job.\n2.3 Program build dependencies You want to figure out in which order you should compile all the program\u0026rsquo;s dependencies so that you will never try and compile a dependency for which you haven\u0026rsquo;t first built all of its dependencies.\nA typical example is GNU Make: you specific your targets in a makefile, Make will parse makefile, and figure out which target should be built firstly. Supposing you have a makefile like this:\n# Makefile for analysis report output/figure_1.png: data/input_file_1.csv scripts/generate_histogram.py python scripts/generate_histogram.py -i data/input_file_1.csv -o output/figure_1.png output/figure_2.png: data/input_file_2.csv scripts/generate_histogram.py python scripts/generate_histogram.py -i data/input_file_2.csv -o output/figure_2.png output/report.pdf: report/report.tex output/figure_1.png output/figure_2.png cd report/ \u0026amp;\u0026amp; pdflatex report.tex \u0026amp;\u0026amp; mv report.pdf ../output/report.pdf Make will generate a DAG internally to figure out which target should be executed firstly with typological sort:\n3 Directed Acyclic Graph Back to the definition, we say that a topological ordering of a directed graph is a linear ordering of its vertices, but not all directed graphs have a topological ordering.\nA topological ordering is possible if and only if the graph has no directed cycles, that is, if it\u0026rsquo;s a directed acyclic graph(DAG).\nLet us see some examples:\nThe definition requires that only the directed acyclic graph has a topological ordering, but why? What happens if we are trying to find a topological ordering of a directed graph? Let\u0026rsquo;s take the figure 3 for an example.\nThe directed graph problem has no solution, this is the reason why directed cycle is forbidden\n4 Kahn\u0026rsquo;s Algorithm There are several algorithms for topological sorting, Kahn\u0026rsquo;s algorithm is one of them, based on breadth first search.\nThe intuition behind Kahn\u0026rsquo;s algorithm is pretty straightforward:\nTo repeatedly remove nodes without any dependencies from the graph and add them to the topological ordering\nAs nodes without dependencies are removed from the graph, the original nodes depend on the removed node should be free now.\nWe keep removing nodes without dependencies from the graph until all nodes are processed, or a cycle is detected.\nThe dependencies of one node are represented as in-degree of this node.\nLet\u0026rsquo;s take a quick example of how to find out a topological ordering of a given graph with Kahn\u0026rsquo;s algorithm.\nNow we should understand how Kahn\u0026rsquo;s algorithm works. Let\u0026rsquo;s have a look at a C++ implementation of Kahn\u0026rsquo;s algorithm:\n#include \u0026lt;deque\u0026gt; #include \u0026lt;vector\u0026gt; // Kahn\u0026#39;s algorithm // `adj` is a directed acyclic graph represented as an adjacency list. std::vector\u0026lt;int\u0026gt; findTopologicalOrder(const std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp;adj) { int n = adj.size(); std::vector\u0026lt;int\u0026gt; in_degree(n, 0); for (int i = 0; i \u0026lt; n; i++) { for (const auto \u0026amp;to_vertex : adj[i]) { in_degree[to_vertex]++; } } // queue contains nodes with no incoming edges std::deque\u0026lt;int\u0026gt; queue; for (int i = 0; i \u0026lt; n; i++) { if (in_degree[i] == 0) { queue.push_back(i); } } std::vector\u0026lt;int\u0026gt; order(n, 0); int index = 0; while (queue.size() \u0026gt; 0) { int cur = queue.front(); queue.pop_front(); order[index++] = cur; for (const auto \u0026amp;next : adj[cur]) { if (--in_degree[next] == 0) { queue.push_back(next); } } } // there is no cycle if (n == index) { return order; } else { // return an empty list if there is a cycle return std::vector\u0026lt;int\u0026gt;{}; } } 5 Bonus When a pregnant woman takes calcium pills, she must make sure also that her diet is rich in vitamin D, since this vitamin makes the absorption of calcium possible.\nAfter reading the demonstration of topological ordering, you (and I) too should take a certain vitamin, metaphorically speaking, to help you absorb. The vitamin D I pick for you (and myself) is two leetcode problems, which involve with the most typical use case of topological ordering \u0026ndash; college class prerequisites:\nCourse Schedule Course Schedule II 6 Reference Topological Sort | Kahn\u0026rsquo;s Algorithm | Graph Theory Directed Acyclic Graph Hands-on Tutorial on Make Topological sorting ","permalink":"https://ramsayleung.github.io/en/post/2022/topological_sorting/","summary":"1 Definition In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertx v, u comes before v in the ordering.\nIt sounds pretty academic, but I am sure you are using topological sort unconsciously every single day.\n2 Application Many real world situations can be modeled as a graph with directed edges where some events must occur before others.","title":"Topological Sort"},{"content":"About me I\u0026rsquo;m Ramsay, I chase a lot of different things. The slogan of this site is In pursuit of Hubris, since:\nThere are three great virtues of a programmer; Laziness, Impatience and Hubris \u0026ndash; Larry Wall\nThe first programming language I learned is `Pascal` in a computer-hobbyist group when I was a middle school student. Back to time before, the only thing I could do with `Pascal` was trying to find the maximum or minimum number among the number array, or just calculate the sum of them, which is of less fun. I didn\u0026rsquo;t really start programming before I became a fresh student majared in software enginering in my university. I\u0026rsquo;ve build a lot of different stuffs for personal use or simply out of curiosity, you could find them at my Github project.\nNowaday, I am employed by Ant Financial(Alibaba group), aka Alipay, make a living by doing some Java stuff, working overtime every working day(whick also known as 996 work schedule). I am also interested in Rust/Python/C++, but find no way to build some industrial level softwares with these languages.\nUpdate: I have left Ant Financial, now I am working for Wechat Pay, just embracing the change, as what my ex-employer encourages to do.\nProjects rspotify: A Spotify Web API wrapper implemented in Rust. jd spiders: A dumb distributed crawler built on Scrapy to scrape goods data and comments data from a famous Chinese E-commerce company, implemented in Python. blog: A clean, elegant blog built on Rust/Javascript(which is also known as the source code of this blog) Book I love to reading books, the reading of all good books is like a comfortable conversation with the most brilliant people of the past centuries, it also brings me unknown friends. As for book, my taste is history, politics, novel and computer science. Right, I get used to write summary of them after reading(mostly in Chinese). Reading does feed me.\n","permalink":"https://ramsayleung.github.io/en/about_me/","summary":"About me I\u0026rsquo;m Ramsay, I chase a lot of different things. The slogan of this site is In pursuit of Hubris, since:\nThere are three great virtues of a programmer; Laziness, Impatience and Hubris \u0026ndash; Larry Wall\nThe first programming language I learned is `Pascal` in a computer-hobbyist group when I was a middle school student. Back to time before, the only thing I could do with `Pascal` was trying to find the maximum or minimum number among the number array, or just calculate the sum of them, which is of less fun.","title":"About Me"},{"content":"1 Preface I have been maintained a legacy distributed timer for months for my employer, then some important pay business are leveraging on it, with 1 billion tasks handled every day and 20k tasks added per second at most.\nEven though it\u0026rsquo;s old and full of black magic code, but it also also have insighted and well-designed code. Based on this old, running timer, I summarize and extract as this article, and it wont include any running code(perhaps pseudocode, and a lot of figures, as an adage says: A picture is worth a thousand words).\nif you are curious about the reason(I personally suggest to watch the TV series Silicon Valley, Richard has gave us a good example and answer)\n2 Design 2.1 Algorithm There are several algorithms in the world to implement timer, such as Red-Black Tree, Min-Heap and timer wheel. The most efficient and used algorithm is timer wheel algorithm, and it\u0026rsquo;s the algorithm we focus on.\nAs for timing wheel based timer, it can be modelled as two internal operations: per-tick bookkeeping and expiry processing.\nPer-tick bookkeeping: happens on every \u0026rsquo;tick\u0026rsquo; of the timer clock. If the unit of granularity for setting timers is T units of time (e.g. 1 second), then per-tick bookkeeping will happen every T units of time. It checks whether any outstanding timers have expired, and if so it removes them and invokes expiry processing. Expiry processing: is responsible for invoked the user-supplied callback (or other user requested action, depending on your model). 2.1.1 Simple Timing Wheels The simple timing wheel keeps a large timing wheel, the below timing wheel has 8 slots, and each slot is holding the task which is going to be expired. Supposing every slot presentes one second(one tick as a second), then the current slot is slot 1, if we want to add a task needed to be triggered 2s later, then this task will be inserted into slot 3.\nper-tick bookkeeping: O(1) What happen if we want to add a task needed to be launched 20s later, the answer is we have no way to do so since there are only 8 slots. So if we have a large period of timer task, we have to maintain a large timing wheel with tons of slots, which requires exponential amount of memory.\n2.1.2 Hashed Timing Wheel Hashed Timing Wheel is an improved simple timing wheel. As we mentioned before, it will consume large resources if timer period is comparatively large. Instead of using one slot per time unit, we could use a form of hashing instead. Construct a circular buffer with a fixed number of slots(such as 8 slots). If current slot is 0, we want store 3s later task, we could insert into slot 3, then if we want bookkeep 9s-later task, we could insert into slot 1(9 % 8 = 1)\nper-tick bookkeeping: O(1) - O(N) It\u0026rsquo;s a tradeoff strategy, We trade space with time.\n2.1.3 Hierarchical Timing Wheels Since simple timing wheels and hashed timing wheel come with drawback of time efficiency or space efficiency. Back to 1987, after studying a number of different approaches for the efficient management of timers, Varghese and Lauck posted a paper to introduce Hierarchical Timing Wheels\nJust make a long story short, I won\u0026rsquo;t dive deep into hierarchical timing wheels, you could easily understand it by a real life reference: the old water meter\nthe firse level wheel(seconds wheel) rotates one loop, triggering the second level(minutes wheel) ticks one slot, same for the third level(hour wheel). Therefore, we present a day(60*60*24 seconds) with 60+60+24 slots. If we want to present a month, we only need to a four level wheel(month wheel) with 30 slots.\nper-tick bookkeeping: O(1) 2.2 Per-tick bookkeeping After introducing timing wheel algorithm, let\u0026rsquo;s go back to the topic about designing a reliable distributed timer, it\u0026rsquo;s essential to decide how to store timer task. Taking implementation complexity and time, space trade off, we choose the Hashed Timing Wheel algorithm.\nThere are several internal components developed by my employer, one of them is named TableKV, a high-availability(99.999% ~ 99.9999%) NoSql service. TableKV supports 10m buckets(the terminology is table) at most, every table comes with full ACID properties of transactions support. You could simply replace TableKV with Redis as it provides the similar bucket functionality.\n2.2.1 Insert task into slot We are going to implement Hashed Timing Wheel algorithm with TableKV, supposing there are 10m buckets, and current time is 2021:08:05 11:17:33 +08=(the UNIX timestamp is =1628176653), there is a timer task which is going to be triggered 10s later with start_time = 1628176653 + 10 (or 100000010s later, start_time = 1628176653 + 10 + 100000000), these tasks both will be stored into bucket start_time % 100000000 = 28176663\n2.2.2 Pull task out from slot As clock tick-tacking to 2021:08:05 11:17:43 +08(1628176663), we need to pull tasks out from slot by calculating the bucket number: current_timestamp(1628176663) % 100000000 = 28176663. After locating the bucket number, we find all tasks in bucket 28176663 with start_time \u0026lt; current_timestamp=, then we get all expected expiry tasks.\n2.3 Global clock and lock As we mentioned before, when the clock tick-tacks to current_time, we fetch all expiry tasks. When our service is running on a distributed system, it\u0026rsquo;s universal that we will have multiple hosts(physical machines or dockers), with multiple current_times on its machine. There is no guarantee that all clocks of multiple hosts synchronized by the same Network Time Server, then all clocks might be subtly different. Which current_time is correct?\nIn order to get the correct time, it\u0026rsquo;s necessary to maintain a monotonic global clock(Of course, it\u0026rsquo;s not the only way to go, there are several ways to handle time and order). Since everything we care about clock is Unix timestamp, we could maintain a global system clock represented by Unix timestamp. All machines request the global clock every second to get the current time, fetching the expiry tasks later.\nWell, are we done? Not yet, a new issue breaks into our design: if all machines can fetch the expiry tasks, these tasks will be processed more than one time, which will cause essential problems. We also need a mutex lock to guarantee only one machine can fetch the expiry task. You can implement both global clock and mutex lock by a magnificent strategy: an Optimistic lock\nAll machines fetch global timestamp(timestamp A) with version All machines increase timestamp(timestamp B) and update version(optimistic locking), only one machine will success because of optimistic locking. Then the machine acquired mutex is authorized to fetch expiry tasks with timestamp A, the other machines failed to acquire mutex is suspended to wait for 1 seconds. Loop back to step 1 with timestamp B. We could encapsulate the role who keep acquiring lock and fetch expiry data as an individual component named scheduler.\n2.4 Expiry processing Expiry processing is responsible for invoked the user-supplied callback or other user requested action. In distributed computing, it\u0026rsquo;s common to execute a procedure by RPC(Remote Procedure Call). In our case, A RPC request is executed when timer task is expiry, from timer service to callback service. Thus, the caller(user) needs to explicitly tell the timer, which service should I execute with what kind of parameters data while the timer task is triggered.\nWe could pack and serialize this meta information and parameters data into binary data, and send it to the timer. When pulling data out from slot, the timer could reconstruct Request/Response/Client type and set it with user-defined data, the next step is a piece of cake, just executing it without saying.\nPerhaps there are many expiry tasks needed to triggered, in order to handle as many tasks as possible, you could create a thread pool, process pool, coroutine pool to execute RPC concurrently.\n2.5 Decoupling Supposing the callback service needs tons of operation, it takes a hundred of millisecond. Even though you have created a thread/process/coroutine pool to handle the timer task, it will inevitably hang, resulting in the decrease of throughout.\nAs for this heavyweight processing case, Message Queue is a great answer. Message queues can significantly simplify coding of decoupled services, while improving performance, reliability and scalability. It\u0026rsquo;s common to combine message queues with Pub/Sub messaging design pattern, timer could publish task data as message, and timer subscribes the same topic of message, using message queue as a buffer. Then in subscriber, the RPC client executes to request for callback service.\nAfter introducing message queue, we could outline the state machine of timer task:\nThanks to message queue, we are able to buffer, to retry or to batch work, and to smooth spiky workloads\n2.6 High availability guarantee 2.6.1 Missed expiry tasks A missed expiry of tasks may occur because of the scheduler process being shutdown or being crashed, or because of other unknown problems. One important job is how to locate these missed tasks and re-execute them. Since we are using global `current_timestamp` to fetch expiry data, we could have another scheduler to use `delay_10min_timestamp` to fetch missed expiry data.\nIn order to look for a needle in a haystack, we need to set a range(delay_10min - current time), and then to batch find cross buckets. After finding these missed tasks, the timer publishes them as a message to message queue. For other open source distributed timer projects like Quartz, which provides an instruction to handle missed(misfire) tasks: Misfire instructions\nIf your NoSql component doesn\u0026rsquo;t support find-cross-buckets feature, you could also find every bucket in the range one by one.\n2.6.2 Callback service error Since the distributed systems are shared-nothing systems, they communicate via message passing through a network(asynchronously or synchronously), but the network is unreliable. When invoking the user-supplied callback, the RPC request might fail if the network is cut off for a while or the callback service is temporarily down.\nRetries are a technique that helps us deal with transient errors, i.e. errors that are temporary and are likely to disappear soon. Retries help us achieve resiliency by allowing the system to send a request repeatedly until it gets an explicit response(success or fail). By leveraging message queue, you obtain the ability for retrying for free. In the meanwhile, the timer could handle the user-requested retries: It\u0026rsquo;s not the proper time to execute callback service, retry it later.\n3 Conclusion After a long way, we are finally here. The final full architecture would look like this:\nThe whole process:\nAdding a timer task, with specified meta info and task info Inserting task into bucket by hashed timing wheel algorithm(With task_state set to pending) Fetch_current scheduler tries to acquire lock and get global current time The Acquired lock scheduler fetches expiry tasks Return the expected data. \u0026amp; 7. Publishing task data as message to MQ with thread pool; And then set task_state to delivered Message subscriber pulls message from MQ Sending RPC request to callback service(set task_state to success or fail) Retry(If necessary) Wish you have fun and profit\n4 Reference Paper: Hashed and Hierarchical Timing Wheels: Efficient Data Structures for Implementing a Timer Facility Hashed and Hierarchical Timing Wheels ","permalink":"https://ramsayleung.github.io/en/post/2021/how_to_design_a_reliable_distributed_timer/","summary":"1 Preface I have been maintained a legacy distributed timer for months for my employer, then some important pay business are leveraging on it, with 1 billion tasks handled every day and 20k tasks added per second at most.\nEven though it\u0026rsquo;s old and full of black magic code, but it also also have insighted and well-designed code. Based on this old, running timer, I summarize and extract as this article, and it wont include any running code(perhaps pseudocode, and a lot of figures, as an adage says: A picture is worth a thousand words).","title":"How To Design A Reliable Distributed Timer"},{"content":"Iterate through pagination in the Rest API\n1 Preface About 4 months ago, icewind1991 created an exciting PR that adding Stream/Iterator based versions of methods with paginated results, which makes enpoints in Rspotify more much ergonomic to use, and Mario completed this PR.\nIn order to know what this PR brought to us, we have to go back to the orignal story, the paginated results in Spotify\u0026rsquo;s Rest API.\n2 Orignal Story Taking the artist_albums as example, it gets Spotify catalog information about an artist\u0026rsquo;s albums.\nThe HTTP response body for this endpoint contains an array of simplified album object wrapped in a paging object and use limit field to control the number of album objects to return and offset field to set the index of the first album to return.\nSo designed endpoint in Rspotify looks like this:\n/// Paging object /// /// [Reference](https://developer.spotify.com/documentation/web-api/reference/#object-pagingobject) #[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct Page\u0026lt;T\u0026gt; { pub href: String, pub items: Vec\u0026lt;T\u0026gt;, pub limit: u32, pub next: Option\u0026lt;String\u0026gt;, pub offset: u32, pub previous: Option\u0026lt;String\u0026gt;, pub total: u32, } /// Get Spotify catalog information about an artist\u0026#39;s albums. /// /// Parameters: /// - artist_id - the artist ID, URI or URL /// - album_type - \u0026#39;album\u0026#39;, \u0026#39;single\u0026#39;, \u0026#39;appears_on\u0026#39;, \u0026#39;compilation\u0026#39; /// - market - limit the response to one particular country. /// - limit - the number of albums to return /// - offset - the index of the first album to return /// [Reference](https://developer.spotify.com/documentation/web-api/reference/#endpoint-get-an-artists-albums) pub fn artist_albums\u0026lt;\u0026#39;a\u0026gt;( \u0026amp;\u0026#39;a self, artist_id: \u0026amp;\u0026#39;a ArtistId, album_type: Option\u0026lt;\u0026amp;\u0026#39;a AlbumType\u0026gt;, market: Option\u0026lt;\u0026amp;\u0026#39;a Market\u0026gt;, ) -\u0026gt; ClientResult\u0026lt;Page\u0026lt;SimplifiedAlbum\u0026gt;\u0026gt;; Supposing that you fetched the first page of an artist\u0026rsquo;s ablums, then you would to get the data of the next page, you have to parse a URL:\n{ \u0026#34;next\u0026#34;: \u0026#34;https://api.spotify.com/v1/browse/categories?offset=2\u0026amp;limit=20\u0026#34; } You have to parse the URL and extract limit and offset parameters, and recall the artist_albums endpoint with setting limit to 20 and offset to 2.\nWe have to manually fetch the data again and again until all datas have been consumed. It is not elegant, but works.\n3 Iterator Story Since we have the basic knowledge about the background, let\u0026rsquo;s jump to the iterator version of pagination endpoints.\nFirst of all, the iterator pattern allows us to perform some tasks on a sequence of items in turn. An iterator is responsible for the logic of itreating over each item and determining when the sequence has finished.\nIf you want to know about about Iterator, Jon Gjengset has covered a brilliant tutorial to demonstrate Iterators in Rust.\nAll iterators implement a trait named Iterator that is defined in the standard library. The definition of the trait looks like this:\npub trait Iterator { type Item; fn next(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt;; // methods with default implementations elided } By implementing the Iterator trait on our own types, we could have iterators that do anything we want. Then working mechanism we want to iterate over paginated result will look like this:\nNow let\u0026rsquo;s dive deep into the code, we need to implement Iterator for our own types, the pseudocode looks like:\nimpl\u0026lt;T\u0026gt; Iterator for PageIterator\u0026lt;Request\u0026gt; { type Item = ClientResult\u0026lt;Page\u0026lt;T\u0026gt;\u0026gt;; fn next(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt; { match call endpoints with offset and limit { Ok(page) if page.items.is_empty() =\u0026gt; { we are done here None } Ok(page) =\u0026gt; { offset += page.items.len() as u32; Some(Ok(page)) } Err(e) =\u0026gt; Some(Err(e)), } } } In order to iterate paginated result from different endpoints, we need a generic type to represent different endpoints. The Fn trait comes to our mind, the function pointer that points to code, not data.\nThen the next version of pseudocode looks like:\nimpl\u0026lt;T, Request\u0026gt; Iterator for PageIterator\u0026lt;Request\u0026gt; where Request: Fn(u32, u32) -\u0026gt; ClientResult\u0026lt;Page\u0026lt;T\u0026gt;\u0026gt;, { type Item = ClientResult\u0026lt;Page\u0026lt;T\u0026gt;\u0026gt;; fn next(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt; { match (function_pointer)(offset and limit) { Ok(page) if page.items.is_empty() =\u0026gt; { we are done here None } Ok(page) =\u0026gt; { offset += page.items.len() as u32; Some(Ok(page)) } Err(e) =\u0026gt; Some(Err(e)), } } } Now, our iterator story has iterated to the end, the next item is that current full version code is here, check it if you are interested in :)\n4 Stream Story Are we done? Not yet. Let\u0026rsquo;s move our eyes to stream story.\nThe stream story is mostly similar with iterator story, except that iterator is synchronous, stream is asynchronous.\nThe Stream trait can yield multiple values before completing, similiar to the Iterator trait.\ntrait Stream { /// The type of the value yielded by the stream. type Item; /// Attempt to resolve the next item in the stream. /// Returns `Poll::Pending` if not ready, `Poll::Ready(Some(x))` if a value /// is ready, and `Poll::Ready(None)` if the stream has completed. fn poll_next(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Option\u0026lt;Self::Item\u0026gt;\u0026gt;; } Since we have already known the iterator, let make the stream story short. We leverage the async-stream for using macro as Syntactic sugar to avoid clumsy type declaration and notation.\nWe use stream! macro to generate an anonymous type implementing the Stream trait, and the Item associated type is the type of the values yielded from the stream, which is ClientResult\u0026lt;T\u0026gt; in this case.\nThe stream full version is shorter and clearer:\n/// This is used to handle paginated requests automatically. pub fn paginate\u0026lt;T, Fut, Request\u0026gt;( req: Request, page_size: u32, ) -\u0026gt; impl Stream\u0026lt;Item = ClientResult\u0026lt;T\u0026gt;\u0026gt; where T: Unpin, Fut: Future\u0026lt;Output = ClientResult\u0026lt;Page\u0026lt;T\u0026gt;\u0026gt;\u0026gt;, Request: Fn(u32, u32) -\u0026gt; Fut, { use async_stream::stream; let mut offset = 0; stream! { loop { let page = req(page_size, offset).await?; offset += page.items.len() as u32; for item in page.items { yield Ok(item); } if page.next.is_none() { break; } } } } 5 Appendix Whew! It took more than I expected. Since iterators is the Rust features inspired by functional programming language ideas, which contributes to Rust\u0026rsquo;s capability to clearly express high-level ideas at low-level performance.\nIt\u0026rsquo;s good to leverage iterators wherever possible, now we can be thrilled to say that all endpoints don\u0026rsquo;t need to manuallly loop over anymore, they are all iterable and rusty.\nThanks Mario and icewind1991 again for their works :)\n","permalink":"https://ramsayleung.github.io/en/post/2021/iterate_through_pagination_api/","summary":"Iterate through pagination in the Rest API\n1 Preface About 4 months ago, icewind1991 created an exciting PR that adding Stream/Iterator based versions of methods with paginated results, which makes enpoints in Rspotify more much ergonomic to use, and Mario completed this PR.\nIn order to know what this PR brought to us, we have to go back to the orignal story, the paginated results in Spotify\u0026rsquo;s Rest API.\n2 Orignal Story Taking the artist_albums as example, it gets Spotify catalog information about an artist\u0026rsquo;s albums.","title":"Let's make everything iterable"},{"content":"The lesson learned from refactoring rspotify\n1 Preface Recently, I and Mario are working on refactoring rspotify, trying to improve performance, documentation, error-handling, data model and reduce compile time, to make it easier to use. (For those who has never heard about rspotify, it is a Spotify HTTP SDK implemented in Rust).\nI am partly focusing on polishing the data model, based on the issue created by Koxiaet.\nSince rspotify is API client for Spotify, it has to handle the request and response from Spotify HTTP API.\nGenerally speaking, the data model is something about how to structure the response data, and used Serde to parse JSON response from HTTP API to Rust struct, and I have learnt a lot Serde tricks from refactoring.\n2 Serde Lesson 2.1 Deserialize JSON map to Vec based on its value. An actions object which contains a disallows object, allows to update the user interface based on which playback actions are available within the current context.\nThe response JSON data from HTTP API:\n{ ... \u0026#34;disallows\u0026#34;: { \u0026#34;resuming\u0026#34;: true } ... } The original model representing actions was:\n#[derive(Clone, Debug, Serialize, PartialEq, Eq)] pub struct Actions { pub disallows: HashMap\u0026lt;DisallowKey, bool\u0026gt; } #[derive(Clone, Serialize, Deserialize, Copy, PartialEq, Eq, Debug, Hash, ToString)] #[serde(rename_all = \u0026#34;snake_case\u0026#34;)] #[strum(serialize_all = \u0026#34;snake_case\u0026#34;)] pub enum DisallowKey { InterruptingPlayback, Pausing, Resuming, ... } And Koxiaet gave great advice about how to polish Actions:\nActions::disallows can be replaced with a Vec\u0026lt;DisallowKey\u0026gt; or HashSet\u0026lt;DisallowKey\u0026gt; by removing all entires whose value is false, which will result in a simpler API.\nTo be honest, I was not that familiar with Serde before, after digging in its official documentation for a while, it seems there is now a built-in way to convert JSON map to Vec\u0026lt;T\u0026gt; base on map\u0026rsquo;s value.\nAfter reading the Custom serialization from documentation, there was a simple solution came to my mind, so I wrote my first customized deserialize function.\nI created a dumb Actions struct inside the deserialize function, and converted HashMap to Vec by filtering its value.\n#[derive(Clone, Debug, Serialize, PartialEq, Eq)] pub struct Actions { pub disallows: Vec\u0026lt;DisallowKey\u0026gt;, } impl\u0026lt;\u0026#39;de\u0026gt; Deserialize\u0026lt;\u0026#39;de\u0026gt; for Actions { fn deserialize\u0026lt;D\u0026gt;(deserializer: D) -\u0026gt; Result\u0026lt;Self, D::Error\u0026gt; where D: Deserializer\u0026lt;\u0026#39;de\u0026gt;, { #[derive(Deserialize)] struct OriginalActions { pub disallows: HashMap\u0026lt;DisallowKey, bool\u0026gt;, } let orignal_actions = OriginalActions::deserialize(deserializer)?; Ok(Actions { disallows: orignal_actions .disallows .into_iter() .filter(|(_, value)| *value) .map(|(key, _)| key) .collect(), }) } } The types should be familiar if you\u0026rsquo;ve used Serde before.\nIf you\u0026rsquo;re not used to Rust then the function signature will likely look a little strange. What it\u0026rsquo;s trying to tell is that d will be something that implements Serde\u0026rsquo;s Deserializer trait, and that any references to memory will live for the 'de lifetime.\n2.2 Deserialize Unix milliseconds timestamp to Datetime A currently playing object which contains information about currently playing item, and the timestamp field is an integer, representing the Unix millisecond timestamp when data was fetched.\nThe response JSON data from HTTP API:\n{ ... \u0026#34;timestamp\u0026#34;: 1490252122574, \u0026#34;progress_ms\u0026#34;: 44272, \u0026#34;is_playing\u0026#34;: true, \u0026#34;currently_playing_type\u0026#34;: \u0026#34;track\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;disallows\u0026#34;: { \u0026#34;resuming\u0026#34;: true } } ... } The original model was:\n/// Currently playing object /// /// [Reference](https://developer.spotify.com/documentation/web-api/reference/player/get-the-users-currently-playing-track/) #[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct CurrentlyPlayingContext { pub timestamp: u64, pub progress_ms: Option\u0026lt;u32\u0026gt;, pub is_playing: bool, pub item: Option\u0026lt;PlayingItem\u0026gt;, pub currently_playing_type: CurrentlyPlayingType, pub actions: Actions, } As before, Koxiaet made a great point about timestamp and =progress_ms=(I will talk about it later):\nCurrentlyPlayingContext::timestamp should be a chrono::DateTime\u0026lt;Utc\u0026gt;, which could be easier to use.\nThe polished struct looks like:\n#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct CurrentlyPlayingContext { pub context: Option\u0026lt;Context\u0026gt;, #[serde( deserialize_with = \u0026#34;from_millisecond_timestamp\u0026#34;, serialize_with = \u0026#34;to_millisecond_timestamp\u0026#34; )] pub timestamp: DateTime\u0026lt;Utc\u0026gt;, pub progress_ms: Option\u0026lt;u32\u0026gt;, pub is_playing: bool, pub item: Option\u0026lt;PlayingItem\u0026gt;, pub currently_playing_type: CurrentlyPlayingType, pub actions: Actions, } Using the deserialize_with attribute tells Serde to use custom deserialization code for the timestamp field. The from_millisecond_timestamp code is:\n/// Deserialize Unix millisecond timestamp to `DateTime\u0026lt;Utc\u0026gt;` pub(in crate) fn from_millisecond_timestamp\u0026lt;\u0026#39;de, D\u0026gt;(d: D) -\u0026gt; Result\u0026lt;DateTime\u0026lt;Utc\u0026gt;, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { d.deserialize_u64(DateTimeVisitor) } The code calls d.deserialize_u64 passing in a struct. The passed in struct implements Serde\u0026rsquo;s Visitor, and look like:\n// Vistor to help deserialize unix millisecond timestamp to `chrono::DateTime` struct DateTimeVisitor; impl\u0026lt;\u0026#39;de\u0026gt; de::Visitor\u0026lt;\u0026#39;de\u0026gt; for DateTimeVisitor { type Value = DateTime\u0026lt;Utc\u0026gt;; fn expecting(\u0026amp;self, formatter: \u0026amp;mut fmt::Formatter) -\u0026gt; fmt::Result { write!( formatter, \u0026#34;an unix millisecond timestamp represents DataTime\u0026lt;UTC\u0026gt;\u0026#34; ) } fn visit_u64\u0026lt;E\u0026gt;(self, v: u64) -\u0026gt; Result\u0026lt;Self::Value, E\u0026gt; where E: de::Error, { ... } } The struct DateTimeVisitor doesn\u0026rsquo;t have any fields, it just a type implemented the custom visitor which delegates to parse the u64.\nSince there is no way to construct DataTime directly from Unix millisecond timestamp, I have to figure out how to handle the construction. And it turns out that there is a way to construct DateTime from seconds and nanoseconds:\nuse chrono::{DateTime, TimeZone, NaiveDateTime, Utc}; let dt = DateTime::\u0026lt;Utc\u0026gt;::from_utc(NaiveDateTime::from_timestamp(61, 0), Utc); Thus, what I need to do is just convert millisecond to second and nanosecond:\nfn visit_u64\u0026lt;E\u0026gt;(self, v: u64) -\u0026gt; Result\u0026lt;Self::Value, E\u0026gt; where E: de::Error, { let second = (v - v % 1000) / 1000; let nanosecond = ((v % 1000) * 1000000) as u32; // The maximum value of i64 is large enough to hold millisecond, so it would be safe to convert it i64 let dt = DateTime::\u0026lt;Utc\u0026gt;::from_utc( NaiveDateTime::from_timestamp(second as i64, nanosecond), Utc, ); Ok(dt) } The to_millisecond_timestamp function is similar to from_millisecond_timestamp, but it\u0026rsquo;s eaiser to implement, check this PR for more detail.\n2.3 Deserialize milliseconds to Duration The simplified episode object contains the simplified episode information, and the duration_ms field is an integer, which represents the episode length in milliseconds.\nThe response JSON data from HTTP API:\n{ ... \u0026#34;audio_preview_url\u0026#34; : \u0026#34;https://p.scdn.co/mp3-preview/83bc7f2d40e850582a4ca118b33c256358de06ff\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;Följ med Tobias Svanelid till Sveriges äldsta tegelkyrka\u0026#34; \u0026#34;duration_ms\u0026#34; : 2685023, \u0026#34;explicit\u0026#34; : false, ... } The original model was\n#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct SimplifiedEpisode { pub audio_preview_url: Option\u0026lt;String\u0026gt;, pub description: String, pub duration_ms: u32, ... } As before without saying, Koxiaet pointed out that\nSimplifiedEpisode::duration_ms should be replaced with a duration of type Duration, since a built-in Duration type works better than primitive type.\nSince I have worked with Serde\u0026rsquo;s custome deserialization, it\u0026rsquo;s not a hard job for me any more. I easily figure out how to deserialize u64 to Duration:\n#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct SimplifiedEpisode { pub audio_preview_url: Option\u0026lt;String\u0026gt;, pub description: String, #[serde( deserialize_with = \u0026#34;from_duration_ms\u0026#34;, serialize_with = \u0026#34;to_duration_ms\u0026#34;, rename = \u0026#34;duration_ms\u0026#34; )] pub duration: Duration, ... } /// Vistor to help deserialize duration represented as millisecond to `std::time::Duration` struct DurationVisitor; impl\u0026lt;\u0026#39;de\u0026gt; de::Visitor\u0026lt;\u0026#39;de\u0026gt; for DurationVisitor { type Value = Duration; fn expecting(\u0026amp;self, formatter: \u0026amp;mut fmt::Formatter) -\u0026gt; fmt::Result { write!(formatter, \u0026#34;a milliseconds represents std::time::Duration\u0026#34;) } fn visit_u64\u0026lt;E\u0026gt;(self, v: u64) -\u0026gt; Result\u0026lt;Self::Value, E\u0026gt; where E: de::Error, { Ok(Duration::from_millis(v)) } } /// Deserialize `std::time::Duration` from millisecond(represented as u64) pub(in crate) fn from_duration_ms\u0026lt;\u0026#39;de, D\u0026gt;(d: D) -\u0026gt; Result\u0026lt;Duration, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { d.deserialize_u64(DurationVisitor) } Now, the life is easier than before.\n2.4 Deserialize milliseconds to Option Let\u0026rsquo;s go back to CurrentlyPlayingContext model, since we have replaced millisecond (represents as u32) with Duration, it makes sense to replace all millisecond fields to Duration.\nBut hold on, it seems progress_ms field is a bit different.\nThe progress_ms field is either not present or a millisecond, the u32 handles the milliseconds, as its value might not be present in the response, it\u0026rsquo;s an Option\u0026lt;u32\u0026gt;, so it won\u0026rsquo;t work with from_duration_ms.\nThus, it\u0026rsquo;s necessary to figure out how to handle the Option type, and the answer is in the documentation, the deserialize_option function:\nHint that the Deserialize type is expecting an optional value.\nThis allows deserializers that encode an optional value as a nullable value to convert the null value into None and a regular value into Some(value).\n#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)] pub struct CurrentlyPlayingContext { pub context: Option\u0026lt;Context\u0026gt;, #[serde( deserialize_with = \u0026#34;from_millisecond_timestamp\u0026#34;, serialize_with = \u0026#34;to_millisecond_timestamp\u0026#34; )] pub timestamp: DateTime\u0026lt;Utc\u0026gt;, #[serde(default)] #[serde( deserialize_with = \u0026#34;from_option_duration_ms\u0026#34;, serialize_with = \u0026#34;to_option_duration_ms\u0026#34;, rename = \u0026#34;progress_ms\u0026#34; )] pub progress: Option\u0026lt;Duration\u0026gt;, } /// Deserialize `Option\u0026lt;std::time::Duration\u0026gt;` from millisecond(represented as u64) pub(in crate) fn from_option_duration_ms\u0026lt;\u0026#39;de, D\u0026gt;(d: D) -\u0026gt; Result\u0026lt;Option\u0026lt;Duration\u0026gt;, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { d.deserialize_option(OptionDurationVisitor) } As before, the OptionDurationVisitor is an empty struct implemented Visitor trait, but key point is in order to work with deserialize_option, the OptionDurationVisitor has to implement the visit_none and visit_some method:\nimpl\u0026lt;\u0026#39;de\u0026gt; de::Visitor\u0026lt;\u0026#39;de\u0026gt; for OptionDurationVisitor { type Value = Option\u0026lt;Duration\u0026gt;; fn expecting(\u0026amp;self, formatter: \u0026amp;mut fmt::Formatter) -\u0026gt; fmt::Result { write!( formatter, \u0026#34;a optional milliseconds represents std::time::Duration\u0026#34; ) } fn visit_none\u0026lt;E\u0026gt;(self) -\u0026gt; Result\u0026lt;Self::Value, E\u0026gt; where E: de::Error, { Ok(None) } fn visit_some\u0026lt;D\u0026gt;(self, deserializer: D) -\u0026gt; Result\u0026lt;Self::Value, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { Ok(Some(deserializer.deserialize_u64(DurationVisitor)?)) } } The visit_none method return Ok(None) so the progress value in the struct will be None, and the visit_some delegates the parsing logic to DurationVisitor via the deserialize_u64 call, so deserializing Some(u64) works like the u64.\n2.5 Deserialize enum from number An AudioAnalysisSection model contains a mode field, which indicates the modality(major or minor) of a track, the type of scle from which its melodic content is derived. This field will contain a 0 for minor, a 1 for major, or a -1 for no result.\nThe response JSON data from HTTP API:\n{ ... \u0026#34;mode\u0026#34;: 0, \u0026#34;mode_confidence\u0026#34;: 0.414, ... } The original struct representing AudioAnalysisSection was like this, since mode field was stored into a f32=(=f8 was a better choice for this case):\n#[derive(Clone, Debug, Serialize, Deserialize, PartialEq)] pub struct AudioAnalysisSection { ... pub mode: f32, pub mode_confidence: f32, ... } Koxiaet made a great point about mode field:\nAudioAnalysisSection::mode and AudioFeatures::mode are f32=s but should be =Option\u0026lt;Mode\u0026gt;=s where =enum Mode { Major, Minor } as it is more useful.\nIn this case, we don\u0026rsquo;t need the Opiton type and in order to deserialize enum from number, we firstly need to define a C-like enum:\npub enum Modality { #[serde(rename = \u0026#34;0\u0026#34;)] Minor = 0, #[serde(rename = \u0026#34;1\u0026#34;)] Major = 1, #[serde(rename = \u0026#34;1\u0026#34;)] NoResult = -1, } pub struct AudioAnalysisSection { ... pub mode: Modality, pub mode_confidence: f32, ... } And then, what\u0026rsquo;s the next step? It seems serde doesn\u0026rsquo;t allow C-like enums to be formatted as integers rather that strings in JSON natively:\nworking version: { ... \u0026#34;mode\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;mode_confidence\u0026#34;: 0.414, ... } failed version: { ... \u0026#34;mode\u0026#34;: 0, \u0026#34;mode_confidence\u0026#34;: 0.414, ... } Then the failed version is exactly what we want. I know that the serde\u0026rsquo;s official documentation has a solution for this case, the serde_repr crate provides alternative derive macros that derive the same Serialize and Deserialize traits but delegate to the underlying representation of a C-like enum.\nSince we are trying to reduce the compiled time of rspotify, so we are cautious about introducing new dependencies. So a custom-made serialize function would be a better choice, it just needs to match the number, and convert to a related enum value.\n/// Deserialize/Serialize `Modality` to integer(0, 1, -1). pub(in crate) mod modality { use super::enums::Modality; use serde::{de, Deserialize, Serializer}; pub fn deserialize\u0026lt;\u0026#39;de, D\u0026gt;(d: D) -\u0026gt; Result\u0026lt;Modality, D::Error\u0026gt; where D: de::Deserializer\u0026lt;\u0026#39;de\u0026gt;, { let v = i8::deserialize(d)?; match v { 0 =\u0026gt; Ok(Modality::Minor), 1 =\u0026gt; Ok(Modality::Major), -1 =\u0026gt; Ok(Modality::NoResult), _ =\u0026gt; Err(de::Error::invalid_value( de::Unexpected::Signed(v.into()), \u0026amp;\u0026#34;valid value: 0, 1, -1\u0026#34;, )), } } pub fn serialize\u0026lt;S\u0026gt;(x: \u0026amp;Modality, s: S) -\u0026gt; Result\u0026lt;S::Ok, S::Error\u0026gt; where S: Serializer, { match x { Modality::Minor =\u0026gt; s.serialize_i8(0), Modality::Major =\u0026gt; s.serialize_i8(1), Modality::NoResult =\u0026gt; s.serialize_i8(-1), } } } 3 Move into module Update:\n2021-01-15\nfrom(to)_millisecond_timestamp have been moved into its module millisecond_timestamp and rename them to deserialize \u0026amp; serialize from(to)_duration_ms have been moved into its module duration_ms and rename them to deserialize \u0026amp; serialize from(to)_option_duration_ms have been moved into its module option_duration_ms and rename them to deserialize \u0026amp; serialize 4 Summary To be honest, it\u0026rsquo;s the first time I have needed some customized works, which took me some time to understand how does Serde works. Finally, all investments paid off, it works great now.\nSerde is such an awesome deserialize/serialize framework which I have learnt a lot of from and still have a lot of to learn from.\n5 Reference Deserializing optional datetimes with serde PR: Keep polishing the models PR: Refactor model PR: Deserialize enum from number ","permalink":"https://ramsayleung.github.io/en/post/2020/serde_lesson/","summary":"The lesson learned from refactoring rspotify\n1 Preface Recently, I and Mario are working on refactoring rspotify, trying to improve performance, documentation, error-handling, data model and reduce compile time, to make it easier to use. (For those who has never heard about rspotify, it is a Spotify HTTP SDK implemented in Rust).\nI am partly focusing on polishing the data model, based on the issue created by Koxiaet.\nSince rspotify is API client for Spotify, it has to handle the request and response from Spotify HTTP API.","title":"Serde Tricks"},{"content":"1 Preface Today, I am exited to introduce you the v0.9 release I have been continued to work on it for the past few weeks that adds async/await support now!\n2 The road to async/await What is rspotify: \u0026gt; For those who has never heared about rspotify before, rspotify is a Spotify web Api wrapper implemented in Rust.\nWith async/await\u0026rsquo;s forthcoming stabilization and reqwest adds async/await support now, I think it\u0026rsquo;s time to let rspotify leverage power from async/await. To be honest, I was not familiar with async/await before, because of my Java background from where I just get used to multiple thread and sync stuff(Yes, I know Java has future either).\nAfter reading some good learning resources, such as Async book, Zero-cost Async IO, I started to step into the world of async/await. async/await is a way to write functions that can \u0026ldquo;pause\u0026rdquo;, return control to the runtime, ant then pick up from where they left off.\nI think perhaps the most important part of async/await is runtime, which defines how to schedule the functions.\nNow, by leveraging the async/await power of reqwest, rspotify could send HTTP request and handle response asynchronously.\nFuthermore, not only do I refactor the old blocking endpoint functions to async/await version, but also keep the old blocking endpoint functions with a new additional feature blocking, then other developers could choose API to their taste.\n3 Overview album example:\nuse rspotify::client::Spotify; use rspotify::oauth2::SpotifyClientCredentials; #[tokio::main] async fn main() { // Set client_id and client_secret in .env file or // export CLIENT_ID=\u0026#34;your client_id\u0026#34; // export CLIENT_SECRET=\u0026#34;secret\u0026#34; let client_credential = SpotifyClientCredentials::default().build(); // Or set client_id and client_secret explictly // let client_credential = SpotifyClientCredentials::default() // .client_id(\u0026#34;this-is-my-client-id\u0026#34;) // .client_secret(\u0026#34;this-is-my-client-secret\u0026#34;) // .build(); let spotify = Spotify::default() .client_credentials_manager(client_credential) .build(); let birdy_uri = \u0026#34;spotify:album:0sNOF9WDwhWunNAHPD3Baj\u0026#34;; let albums = spotify.album(birdy_uri).await; println!(\u0026#34;{:?}\u0026#34;, albums); } Just change the default API to async, and moving the previous synchronous API to blocking module.\nNotes that I think the v0.9 release of rspotify is going to be a huge break change because of the support for async/await, which definitely breaks backward compatibility.\nSo I decide to make an other break change into the next release, just refactoring the project structure to shorten the import path:\nbefore:\nuse rspotify::spotify::client::Spotify; use rspotify::spotify::oauth2::SpotifyClientCredentials; after:\nuse rspotify::client::Spotify; use rspotify::oauth2::SpotifyClientCredentials; the spotify module is unnecessary and inelegant, so I just remove it.\n4 Conclusion rspotify v0.9 is now available! There is documentation, examples and an issue tracker!\nPlease provide any feedback, as I would love to improve this library any way I can! Thanks @Alexander so much for actively participate in the refactor work for support async/await.\n","permalink":"https://ramsayleung.github.io/en/post/2020/async_await_for_rspotify/","summary":"1 Preface Today, I am exited to introduce you the v0.9 release I have been continued to work on it for the past few weeks that adds async/await support now!\n2 The road to async/await What is rspotify: \u0026gt; For those who has never heared about rspotify before, rspotify is a Spotify web Api wrapper implemented in Rust.\nWith async/await\u0026rsquo;s forthcoming stabilization and reqwest adds async/await support now, I think it\u0026rsquo;s time to let rspotify leverage power from async/await.","title":"rspotify has come to async/await"}]