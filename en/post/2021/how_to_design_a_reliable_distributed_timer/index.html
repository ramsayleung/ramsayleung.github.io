<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>How To Design A Reliable Distributed Timer | In Pursuit of Hubris</title><meta name=keywords content="distributed_system,timer"><meta name=description content="1 Preface I have been maintained a legacy distributed timer for months for my employer, then some important pay business are leveraging on it, with 1 billion tasks handled every day and 20k tasks added per second at most.
Even though it&rsquo;s old and full of black magic code, but it also also have insighted and well-designed code. Based on this old, running timer, I summarize and extract as this article, and it wont include any running code(perhaps pseudocode, and a lot of figures, as an adage says: A picture is worth a thousand words)."><meta name=author content="Ramsay Leung"><link rel=canonical href=https://ramsayleung.github.io/en/post/2021/how_to_design_a_reliable_distributed_timer/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://ramsayleung.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ramsayleung.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ramsayleung.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ramsayleung.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ramsayleung.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ramsayleung.github.io/en/post/2021/how_to_design_a_reliable_distributed_timer/><link rel=alternate hreflang=zh href=https://ramsayleung.github.io/zh/post/2021/how_to_design_a_reliable_distributed_timer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-9MG65HQHEL"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-9MG65HQHEL",{anonymize_ip:!1})}</script><meta property="og:title" content="How To Design A Reliable Distributed Timer"><meta property="og:description" content="1 Preface I have been maintained a legacy distributed timer for months for my employer, then some important pay business are leveraging on it, with 1 billion tasks handled every day and 20k tasks added per second at most.
Even though it&rsquo;s old and full of black magic code, but it also also have insighted and well-designed code. Based on this old, running timer, I summarize and extract as this article, and it wont include any running code(perhaps pseudocode, and a lot of figures, as an adage says: A picture is worth a thousand words)."><meta property="og:type" content="article"><meta property="og:url" content="https://ramsayleung.github.io/en/post/2021/how_to_design_a_reliable_distributed_timer/"><meta property="og:image" content="https://ramsayleung.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="post"><meta property="article:published_time" content="2021-08-05T09:19:36+00:00"><meta property="article:modified_time" content="2022-02-24T14:01:37+08:00"><meta property="og:site_name" content="Ramsay's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ramsayleung.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="How To Design A Reliable Distributed Timer"><meta name=twitter:description content="1 Preface I have been maintained a legacy distributed timer for months for my employer, then some important pay business are leveraging on it, with 1 billion tasks handled every day and 20k tasks added per second at most.
Even though it&rsquo;s old and full of black magic code, but it also also have insighted and well-designed code. Based on this old, running timer, I summarize and extract as this article, and it wont include any running code(perhaps pseudocode, and a lot of figures, as an adage says: A picture is worth a thousand words)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"How To Design A Reliable Distributed Timer","item":"https://ramsayleung.github.io/en/post/2021/how_to_design_a_reliable_distributed_timer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"How To Design A Reliable Distributed Timer","name":"How To Design A Reliable Distributed Timer","description":"1 Preface I have been maintained a legacy distributed timer for months for my employer, then some important pay business are leveraging on it, with 1 billion tasks handled every day and 20k tasks added per second at most.\nEven though it\u0026rsquo;s old and full of black magic code, but it also also have insighted and well-designed code. Based on this old, running timer, I summarize and extract as this article, and it wont include any running code(perhaps pseudocode, and a lot of figures, as an adage says: A picture is worth a thousand words).","keywords":["distributed_system","timer"],"articleBody":"1 Preface I have been maintained a legacy distributed timer for months for my employer, then some important pay business are leveraging on it, with 1 billion tasks handled every day and 20k tasks added per second at most.\nEven though it’s old and full of black magic code, but it also also have insighted and well-designed code. Based on this old, running timer, I summarize and extract as this article, and it wont include any running code(perhaps pseudocode, and a lot of figures, as an adage says: A picture is worth a thousand words).\nif you are curious about the reason(I personally suggest to watch the TV series Silicon Valley, Richard has gave us a good example and answer)\n2 Design 2.1 Algorithm There are several algorithms in the world to implement timer, such as Red-Black Tree, Min-Heap and timer wheel. The most efficient and used algorithm is timer wheel algorithm, and it’s the algorithm we focus on.\nAs for timing wheel based timer, it can be modelled as two internal operations: per-tick bookkeeping and expiry processing.\nPer-tick bookkeeping: happens on every ’tick’ of the timer clock. If the unit of granularity for setting timers is T units of time (e.g. 1 second), then per-tick bookkeeping will happen every T units of time. It checks whether any outstanding timers have expired, and if so it removes them and invokes expiry processing. Expiry processing: is responsible for invoked the user-supplied callback (or other user requested action, depending on your model). 2.1.1 Simple Timing Wheels The simple timing wheel keeps a large timing wheel, the below timing wheel has 8 slots, and each slot is holding the task which is going to be expired. Supposing every slot presentes one second(one tick as a second), then the current slot is slot 1, if we want to add a task needed to be triggered 2s later, then this task will be inserted into slot 3.\nper-tick bookkeeping: O(1) What happen if we want to add a task needed to be launched 20s later, the answer is we have no way to do so since there are only 8 slots. So if we have a large period of timer task, we have to maintain a large timing wheel with tons of slots, which requires exponential amount of memory.\n2.1.2 Hashed Timing Wheel Hashed Timing Wheel is an improved simple timing wheel. As we mentioned before, it will consume large resources if timer period is comparatively large. Instead of using one slot per time unit, we could use a form of hashing instead. Construct a circular buffer with a fixed number of slots(such as 8 slots). If current slot is 0, we want store 3s later task, we could insert into slot 3, then if we want bookkeep 9s-later task, we could insert into slot 1(9 % 8 = 1)\nper-tick bookkeeping: O(1) - O(N) It’s a tradeoff strategy, We trade space with time.\n2.1.3 Hierarchical Timing Wheels Since simple timing wheels and hashed timing wheel come with drawback of time efficiency or space efficiency. Back to 1987, after studying a number of different approaches for the efficient management of timers, Varghese and Lauck posted a paper to introduce Hierarchical Timing Wheels\nJust make a long story short, I won’t dive deep into hierarchical timing wheels, you could easily understand it by a real life reference: the old water meter\nthe firse level wheel(seconds wheel) rotates one loop, triggering the second level(minutes wheel) ticks one slot, same for the third level(hour wheel). Therefore, we present a day(60*60*24 seconds) with 60+60+24 slots. If we want to present a month, we only need to a four level wheel(month wheel) with 30 slots.\nper-tick bookkeeping: O(1) 2.2 Per-tick bookkeeping After introducing timing wheel algorithm, let’s go back to the topic about designing a reliable distributed timer, it’s essential to decide how to store timer task. Taking implementation complexity and time, space trade off, we choose the Hashed Timing Wheel algorithm.\nThere are several internal components developed by my employer, one of them is named TableKV, a high-availability(99.999% ~ 99.9999%) NoSql service. TableKV supports 10m buckets(the terminology is table) at most, every table comes with full ACID properties of transactions support. You could simply replace TableKV with Redis as it provides the similar bucket functionality.\n2.2.1 Insert task into slot We are going to implement Hashed Timing Wheel algorithm with TableKV, supposing there are 10m buckets, and current time is 2021:08:05 11:17:33 +08=(the UNIX timestamp is =1628176653), there is a timer task which is going to be triggered 10s later with start_time = 1628176653 + 10 (or 100000010s later, start_time = 1628176653 + 10 + 100000000), these tasks both will be stored into bucket start_time % 100000000 = 28176663\n2.2.2 Pull task out from slot As clock tick-tacking to 2021:08:05 11:17:43 +08(1628176663), we need to pull tasks out from slot by calculating the bucket number: current_timestamp(1628176663) % 100000000 = 28176663. After locating the bucket number, we find all tasks in bucket 28176663 with start_time \u003c current_timestamp=, then we get all expected expiry tasks.\n2.3 Global clock and lock As we mentioned before, when the clock tick-tacks to current_time, we fetch all expiry tasks. When our service is running on a distributed system, it’s universal that we will have multiple hosts(physical machines or dockers), with multiple current_times on its machine. There is no guarantee that all clocks of multiple hosts synchronized by the same Network Time Server, then all clocks might be subtly different. Which current_time is correct?\nIn order to get the correct time, it’s necessary to maintain a monotonic global clock(Of course, it’s not the only way to go, there are several ways to handle time and order). Since everything we care about clock is Unix timestamp, we could maintain a global system clock represented by Unix timestamp. All machines request the global clock every second to get the current time, fetching the expiry tasks later.\nWell, are we done? Not yet, a new issue breaks into our design: if all machines can fetch the expiry tasks, these tasks will be processed more than one time, which will cause essential problems. We also need a mutex lock to guarantee only one machine can fetch the expiry task. You can implement both global clock and mutex lock by a magnificent strategy: an Optimistic lock\nAll machines fetch global timestamp(timestamp A) with version All machines increase timestamp(timestamp B) and update version(optimistic locking), only one machine will success because of optimistic locking. Then the machine acquired mutex is authorized to fetch expiry tasks with timestamp A, the other machines failed to acquire mutex is suspended to wait for 1 seconds. Loop back to step 1 with timestamp B. We could encapsulate the role who keep acquiring lock and fetch expiry data as an individual component named scheduler.\n2.4 Expiry processing Expiry processing is responsible for invoked the user-supplied callback or other user requested action. In distributed computing, it’s common to execute a procedure by RPC(Remote Procedure Call). In our case, A RPC request is executed when timer task is expiry, from timer service to callback service. Thus, the caller(user) needs to explicitly tell the timer, which service should I execute with what kind of parameters data while the timer task is triggered.\nWe could pack and serialize this meta information and parameters data into binary data, and send it to the timer. When pulling data out from slot, the timer could reconstruct Request/Response/Client type and set it with user-defined data, the next step is a piece of cake, just executing it without saying.\nPerhaps there are many expiry tasks needed to triggered, in order to handle as many tasks as possible, you could create a thread pool, process pool, coroutine pool to execute RPC concurrently.\n2.5 Decoupling Supposing the callback service needs tons of operation, it takes a hundred of millisecond. Even though you have created a thread/process/coroutine pool to handle the timer task, it will inevitably hang, resulting in the decrease of throughout.\nAs for this heavyweight processing case, Message Queue is a great answer. Message queues can significantly simplify coding of decoupled services, while improving performance, reliability and scalability. It’s common to combine message queues with Pub/Sub messaging design pattern, timer could publish task data as message, and timer subscribes the same topic of message, using message queue as a buffer. Then in subscriber, the RPC client executes to request for callback service.\nAfter introducing message queue, we could outline the state machine of timer task:\nThanks to message queue, we are able to buffer, to retry or to batch work, and to smooth spiky workloads\n2.6 High availability guarantee 2.6.1 Missed expiry tasks A missed expiry of tasks may occur because of the scheduler process being shutdown or being crashed, or because of other unknown problems. One important job is how to locate these missed tasks and re-execute them. Since we are using global `current_timestamp` to fetch expiry data, we could have another scheduler to use `delay_10min_timestamp` to fetch missed expiry data.\nIn order to look for a needle in a haystack, we need to set a range(delay_10min - current time), and then to batch find cross buckets. After finding these missed tasks, the timer publishes them as a message to message queue. For other open source distributed timer projects like Quartz, which provides an instruction to handle missed(misfire) tasks: Misfire instructions\nIf your NoSql component doesn’t support find-cross-buckets feature, you could also find every bucket in the range one by one.\n2.6.2 Callback service error Since the distributed systems are shared-nothing systems, they communicate via message passing through a network(asynchronously or synchronously), but the network is unreliable. When invoking the user-supplied callback, the RPC request might fail if the network is cut off for a while or the callback service is temporarily down.\nRetries are a technique that helps us deal with transient errors, i.e. errors that are temporary and are likely to disappear soon. Retries help us achieve resiliency by allowing the system to send a request repeatedly until it gets an explicit response(success or fail). By leveraging message queue, you obtain the ability for retrying for free. In the meanwhile, the timer could handle the user-requested retries: It’s not the proper time to execute callback service, retry it later.\n3 Conclusion After a long way, we are finally here. The final full architecture would look like this:\nThe whole process:\nAdding a timer task, with specified meta info and task info Inserting task into bucket by hashed timing wheel algorithm(With task_state set to pending) Fetch_current scheduler tries to acquire lock and get global current time The Acquired lock scheduler fetches expiry tasks Return the expected data. \u0026 7. Publishing task data as message to MQ with thread pool; And then set task_state to delivered Message subscriber pulls message from MQ Sending RPC request to callback service(set task_state to success or fail) Retry(If necessary) Wish you have fun and profit\n4 Reference Paper: Hashed and Hierarchical Timing Wheels: Efficient Data Structures for Implementing a Timer Facility Hashed and Hierarchical Timing Wheels ","wordCount":"1845","inLanguage":"en","datePublished":"2021-08-05T09:19:36Z","dateModified":"2022-02-24T14:01:37+08:00","author":{"@type":"Person","name":"Ramsay Leung"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ramsayleung.github.io/en/post/2021/how_to_design_a_reliable_distributed_timer/"},"publisher":{"@type":"Organization","name":"In Pursuit of Hubris","logo":{"@type":"ImageObject","url":"https://ramsayleung.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ramsayleung.github.io/en/ accesskey=h title="Home (Alt + H)"><img src=https://ramsayleung.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://ramsayleung.github.io/zh/ title=中文 aria-label=中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://ramsayleung.github.io/en/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://ramsayleung.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://ramsayleung.github.io/en/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://ramsayleung.github.io/en/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://ramsayleung.github.io/en/about_me title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ramsayleung.github.io/en/>Home</a></div><h1 class=post-title>How To Design A Reliable Distributed Timer</h1><div class=post-meta><span title='2021-08-05 09:19:36 +0000 UTC'>August 5, 2021</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1845 words&nbsp;·&nbsp;Ramsay Leung&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://ramsayleung.github.io/zh/post/2021/how_to_design_a_reliable_distributed_timer/>Zh</a></li></ul>&nbsp;|&nbsp;<a href=https://github.com/ramsayleung/ramsayleung.github.io/blob/master/content/post/2021/how_to_design_a_reliable_distributed_timer.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#preface><span class=section-num>1</span> Preface</a></li><li><a href=#design><span class=section-num>2</span> Design</a><ul><li><a href=#algorithm><span class=section-num>2.1</span> Algorithm</a></li><li><a href=#per-tick-bookkeeping><span class=section-num>2.2</span> Per-tick bookkeeping</a></li><li><a href=#global-clock-and-lock><span class=section-num>2.3</span> Global clock and lock</a></li><li><a href=#expiry-processing><span class=section-num>2.4</span> Expiry processing</a></li><li><a href=#decoupling><span class=section-num>2.5</span> Decoupling</a></li><li><a href=#high-availability-guarantee><span class=section-num>2.6</span> High availability guarantee</a></li></ul></li><li><a href=#conclusion><span class=section-num>3</span> Conclusion</a></li><li><a href=#reference><span class=section-num>4</span> Reference</a></li></ul></nav></div></details></div><div class=post-content><h2 id=preface><span class=section-num>1</span> Preface<a hidden class=anchor aria-hidden=true href=#preface>#</a></h2><p>I have been maintained a legacy distributed timer for months for my employer, then some important pay business are leveraging on it, with 1 billion tasks handled every day and 20k tasks added per second at most.</p><p>Even though it&rsquo;s old and full of black magic code, but it also also have insighted and well-designed code. Based on this old, running timer, I summarize and extract as this article, and it wont include any running code(perhaps pseudocode, and a lot of figures, as an adage says: <em>A picture is worth a thousand words</em>).</p><p>if you are curious about the reason(I personally suggest to watch the TV series <a href=https://www.imdb.com/title/tt2575988/>Silicon Valley</a>, Richard has gave us a good example and answer)</p><h2 id=design><span class=section-num>2</span> Design<a hidden class=anchor aria-hidden=true href=#design>#</a></h2><h3 id=algorithm><span class=section-num>2.1</span> Algorithm<a hidden class=anchor aria-hidden=true href=#algorithm>#</a></h3><p>There are several algorithms in the world to implement timer, such as Red-Black Tree, Min-Heap and timer wheel. The most efficient and used algorithm is timer wheel algorithm, and it&rsquo;s the algorithm we focus on.</p><p>As for timing wheel based timer, it can be modelled as two internal operations: <em>per-tick bookkeeping</em> and <em>expiry processing</em>.</p><ul><li>Per-tick bookkeeping: happens on every &rsquo;tick&rsquo; of the timer clock. If the unit of granularity for setting timers is T units of time (e.g. 1 second), then per-tick bookkeeping will happen every T units of time. It checks whether any outstanding timers have expired, and if so it removes them and invokes expiry processing.</li><li>Expiry processing: is responsible for invoked the user-supplied callback (or other user requested action, depending on your model).</li></ul><h4 id=simple-timing-wheels><span class=section-num>2.1.1</span> Simple Timing Wheels<a hidden class=anchor aria-hidden=true href=#simple-timing-wheels>#</a></h4><p>The simple timing wheel keeps a large timing wheel, the below timing wheel has 8 slots, and each slot is holding the task which is going to be expired. Supposing every slot presentes one second(one tick as a second), then the current slot is <code>slot 1</code>, if we want to add a task needed to be triggered 2s later, then this task will be inserted into <code>slot 3</code>.</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806103302.png></figure><ul><li>per-tick bookkeeping: O(1)</li></ul><p>What happen if we want to add a task needed to be launched 20s later, the answer is we have no way to do so since there are only 8 slots. So if we have a large period of timer task, we have to maintain a large timing wheel with tons of slots, which requires exponential amount of memory.</p><h4 id=hashed-timing-wheel><span class=section-num>2.1.2</span> Hashed Timing Wheel<a hidden class=anchor aria-hidden=true href=#hashed-timing-wheel>#</a></h4><p>Hashed Timing Wheel is an improved simple timing wheel. As we mentioned before, it will consume large resources if timer period is comparatively large. Instead of using one slot per time unit, we could use a form of hashing instead. Construct a circular buffer with a fixed number of slots(such as 8 slots). If current slot is 0, we want store <code>3s later</code> task, we could insert into <code>slot 3</code>, then if we want bookkeep <code>9s-later</code> task, we could insert into <code>slot 1(9 % 8 = 1)</code></p><ul><li>per-tick bookkeeping: O(1) - O(N)</li></ul><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806103527.png></figure><p>It&rsquo;s a tradeoff strategy, We trade space with time.</p><h4 id=hierarchical-timing-wheels><span class=section-num>2.1.3</span> Hierarchical Timing Wheels<a hidden class=anchor aria-hidden=true href=#hierarchical-timing-wheels>#</a></h4><p>Since simple timing wheels and hashed timing wheel come with drawback of time efficiency or space efficiency. Back to 1987, after studying a number of different approaches for the efficient management of timers, Varghese and Lauck posted a paper to introduce <a href=http://www.cs.columbia.edu/~nahum/w6998/papers/sosp87-timing-wheels.pdf>Hierarchical Timing Wheels</a></p><p>Just make a long story short, I won&rsquo;t dive deep into hierarchical timing wheels, you could easily understand it by a real life reference: the old water meter</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210805205803.png></figure><p>the firse level wheel(seconds wheel) rotates one loop, triggering the second level(minutes wheel) ticks one slot, same for the third level(hour wheel). Therefore, we present a day(60*60*24 seconds) with 60+60+24 slots. If we want to present a month, we only need to a four level wheel(month wheel) with 30 slots.</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806103357.png></figure><ul><li>per-tick bookkeeping: O(1)</li></ul><h3 id=per-tick-bookkeeping><span class=section-num>2.2</span> Per-tick bookkeeping<a hidden class=anchor aria-hidden=true href=#per-tick-bookkeeping>#</a></h3><p>After introducing timing wheel algorithm, let&rsquo;s go back to the topic about designing a reliable distributed timer, it&rsquo;s essential to decide how to store timer task. Taking implementation complexity and time, space trade off, we choose the Hashed Timing Wheel algorithm.</p><p>There are several internal components developed by my employer, one of them is named <code>TableKV</code>, a high-availability(99.999% ~ 99.9999%) NoSql service. <code>TableKV</code> supports 10m buckets(the terminology is <code>table</code>) at most, every <code>table</code> comes with full ACID properties of transactions support. You could simply replace <code>TableKV</code> with <code>Redis</code> as it provides the similar bucket functionality.</p><h4 id=insert-task-into-slot><span class=section-num>2.2.1</span> Insert task into slot<a hidden class=anchor aria-hidden=true href=#insert-task-into-slot>#</a></h4><p>We are going to implement Hashed Timing Wheel algorithm with <code>TableKV</code>, supposing there are 10m buckets, and current time is <code>2021:08:05 11:17:33 +08=(the UNIX timestamp is =1628176653</code>), there is a timer task which is going to be triggered 10s later with <code>start_time = 1628176653 + 10</code> (or 100000010s later, <code>start_time = 1628176653 + 10 + 100000000</code>), these tasks both will be stored into bucket <code>start_time % 100000000 = 28176663</code></p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806195209.png></figure><h4 id=pull-task-out-from-slot><span class=section-num>2.2.2</span> Pull task out from slot<a hidden class=anchor aria-hidden=true href=#pull-task-out-from-slot>#</a></h4><p>As clock tick-tacking to <code>2021:08:05 11:17:43 +08(1628176663)</code>, we need to pull tasks out from slot by calculating the bucket number: <code>current_timestamp(1628176663) % 100000000 = 28176663</code>. After locating the bucket number, we find all tasks in <code>bucket 28176663</code> with <code>start_time &lt;</code> current_timestamp=, then we get all expected expiry tasks.</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806195227.png></figure><h3 id=global-clock-and-lock><span class=section-num>2.3</span> Global clock and lock<a hidden class=anchor aria-hidden=true href=#global-clock-and-lock>#</a></h3><p>As we mentioned before, when the clock tick-tacks to <strong>current_time</strong>, we fetch all expiry tasks. When our service is running on a distributed system, it&rsquo;s universal that we will have multiple hosts(physical machines or dockers), with multiple <code>current_times</code> on its machine. There is no guarantee that all clocks of multiple hosts synchronized by the same Network Time Server, then all clocks might be subtly different. Which current_time is correct?</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806105745.png></figure><p>In order to get the correct time, it&rsquo;s necessary to maintain a monotonic global clock(Of course, it&rsquo;s not the only way to go, there are several ways to handle <a href=http://book.mixu.net/distsys/time.html>time and order</a>). Since everything we care about clock is Unix timestamp, we could maintain a global system clock represented by Unix timestamp. All machines request the global clock every second to get the current time, fetching the expiry tasks later.</p><p>Well, are we done? Not yet, a new issue breaks into our design: if all machines can fetch the expiry tasks, these tasks will be processed more than one time, which will cause essential problems. We also need a mutex lock to guarantee only one machine can fetch the expiry task. You can implement both global clock and mutex lock by a magnificent strategy: an <a href=https://en.wikipedia.org/wiki/Optimistic_locking>Optimistic lock</a></p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806114120.png></figure><ol><li>All machines fetch global timestamp(timestamp A) with version</li><li>All machines increase timestamp(timestamp B) and update version(optimistic locking), only one machine will success because of optimistic locking.</li><li>Then the machine acquired mutex is authorized to fetch expiry tasks with timestamp A, the other machines failed to acquire mutex is suspended to wait for 1 seconds.</li><li>Loop back to step 1 with timestamp B.</li></ol><p>We could encapsulate the role who keep acquiring lock and fetch expiry data as an individual component named <strong>scheduler</strong>.</p><h3 id=expiry-processing><span class=section-num>2.4</span> Expiry processing<a hidden class=anchor aria-hidden=true href=#expiry-processing>#</a></h3><p>Expiry processing is responsible for invoked the user-supplied callback or other user requested action. In distributed computing, it&rsquo;s common to execute a procedure by RPC(Remote Procedure Call). In our case, A RPC request is executed when timer task is expiry, from timer service to callback service. Thus, the caller(user) needs to explicitly tell the timer, which service should I execute with what kind of parameters data while the timer task is triggered.</p><p>We could pack and serialize this meta information and parameters data into binary data, and send it to the timer. When pulling data out from slot, the timer could reconstruct Request/Response/Client type and set it with user-defined data, the next step is a piece of cake, just executing it without saying.</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806142855.png></figure><p>Perhaps there are many expiry tasks needed to triggered, in order to handle as many tasks as possible, you could create a thread pool, process pool, coroutine pool to execute RPC concurrently.</p><h3 id=decoupling><span class=section-num>2.5</span> Decoupling<a hidden class=anchor aria-hidden=true href=#decoupling>#</a></h3><p>Supposing the callback service needs tons of operation, it takes a hundred of millisecond. Even though you have created a thread/process/coroutine pool to handle the timer task, it will inevitably hang, resulting in the decrease of throughout.</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806152334.png></figure><p>As for this heavyweight processing case, Message Queue is a great answer. Message queues can significantly simplify coding of decoupled services, while improving performance, reliability and scalability. It&rsquo;s common to combine message queues with Pub/Sub messaging design pattern, timer could publish task data as message, and timer subscribes the same topic of message, using message queue as a buffer. Then in subscriber, the RPC client executes to request for callback service.</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806161123.png></figure><p>After introducing message queue, we could outline the state machine of timer task:</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806203653.png></figure><p>Thanks to message queue, we are able to buffer, to retry or to batch work, and to smooth spiky workloads</p><h3 id=high-availability-guarantee><span class=section-num>2.6</span> High availability guarantee<a hidden class=anchor aria-hidden=true href=#high-availability-guarantee>#</a></h3><h4 id=missed-expiry-tasks><span class=section-num>2.6.1</span> Missed expiry tasks<a hidden class=anchor aria-hidden=true href=#missed-expiry-tasks>#</a></h4><p>A missed expiry of tasks may occur because of the scheduler process being shutdown or being crashed, or because of other unknown problems. One important job is how to locate these missed tasks and re-execute them. Since we are using global `current_timestamp` to fetch expiry data, we could have another scheduler to use `delay_10min_timestamp` to fetch missed expiry data.</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806202724.png></figure><p>In order to look for a needle in a haystack, we need to set a range(delay_10min - current time), and then to batch find cross buckets. After finding these missed tasks, the timer publishes them as a message to message queue. For other open source distributed timer projects like Quartz, which provides an instruction to handle missed(misfire) tasks: <a href=http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/tutorial-lesson-04.html>Misfire instructions</a></p><p>If your NoSql component doesn&rsquo;t support find-cross-buckets feature, you could also find every bucket in the range one by one.</p><h4 id=callback-service-error><span class=section-num>2.6.2</span> Callback service error<a hidden class=anchor aria-hidden=true href=#callback-service-error>#</a></h4><p>Since the distributed systems are shared-nothing systems, they communicate via message passing through a network(asynchronously or synchronously), but the network is unreliable. When invoking the user-supplied callback, the RPC request might fail if the network is cut off for a while or the callback service is temporarily down.</p><p>Retries are a technique that helps us deal with transient errors, i.e. errors that are temporary and are likely to disappear soon. Retries help us achieve resiliency by allowing the system to send a request repeatedly until it gets an explicit response(success or fail). By leveraging message queue, you obtain the ability for retrying for free. In the meanwhile, the timer could handle the user-requested retries: It&rsquo;s not the proper time to execute callback service, retry it later.</p><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806195302.png></figure><h2 id=conclusion><span class=section-num>3</span> Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>After a long way, we are finally here. The final full architecture would look like this:</p><p>The whole process:</p><ol><li>Adding a timer task, with specified meta info and task info</li><li>Inserting task into bucket by hashed timing wheel algorithm(With <code>task_state</code> set to <code>pending</code>)</li><li>Fetch_current scheduler tries to acquire lock and get global current time</li><li>The Acquired lock scheduler fetches expiry tasks</li><li>Return the expected data.</li><li>& 7. Publishing task data as message to MQ with thread pool; And then set <code>task_state</code> to <code>delivered</code></li><li>Message subscriber pulls message from MQ</li><li>Sending RPC request to callback service(set <code>task_state</code> to <code>success</code> or <code>fail</code>)</li><li>Retry(If necessary)</li></ol><figure><img loading=lazy src=https://raw.githubusercontent.com/ramsayleung/images/master/20210806202140.png></figure><p>Wish you have fun and profit</p><h2 id=reference><span class=section-num>4</span> Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li><a href=http://www.cs.columbia.edu/~nahum/w6998/papers/ton97-timing-wheels.pdf>Paper: Hashed and Hierarchical Timing Wheels: Efficient Data Structures for Implementing a Timer Facility</a></li><li><a href=https://blog.acolyer.org/2015/11/23/hashed-and-hierarchical-timing-wheels/>Hashed and Hierarchical Timing Wheels</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://ramsayleung.github.io/en/tags/distributed_system/>distributed_system</a></li><li><a href=https://ramsayleung.github.io/en/tags/timer/>timer</a></li></ul><nav class=paginav><a class=prev href=https://ramsayleung.github.io/en/post/2022/topological_sorting/><span class=title>« Prev</span><br><span>Topological Sort</span></a>
<a class=next href=https://ramsayleung.github.io/en/post/2021/iterate_through_pagination_api/><span class=title>Next »</span><br><span>Let's make everything iterable</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share How To Design A Reliable Distributed Timer on x" href="https://x.com/intent/tweet/?text=How%20To%20Design%20A%20Reliable%20Distributed%20Timer&url=https%3a%2f%2framsayleung.github.io%2fen%2fpost%2f2021%2fhow_to_design_a_reliable_distributed_timer%2f&hashtags=distributed_system%2ctimer"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How To Design A Reliable Distributed Timer on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2framsayleung.github.io%2fen%2fpost%2f2021%2fhow_to_design_a_reliable_distributed_timer%2f&title=How%20To%20Design%20A%20Reliable%20Distributed%20Timer&summary=How%20To%20Design%20A%20Reliable%20Distributed%20Timer&source=https%3a%2f%2framsayleung.github.io%2fen%2fpost%2f2021%2fhow_to_design_a_reliable_distributed_timer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How To Design A Reliable Distributed Timer on reddit" href="https://reddit.com/submit?url=https%3a%2f%2framsayleung.github.io%2fen%2fpost%2f2021%2fhow_to_design_a_reliable_distributed_timer%2f&title=How%20To%20Design%20A%20Reliable%20Distributed%20Timer"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How To Design A Reliable Distributed Timer on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2framsayleung.github.io%2fen%2fpost%2f2021%2fhow_to_design_a_reliable_distributed_timer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How To Design A Reliable Distributed Timer on whatsapp" href="https://api.whatsapp.com/send?text=How%20To%20Design%20A%20Reliable%20Distributed%20Timer%20-%20https%3a%2f%2framsayleung.github.io%2fen%2fpost%2f2021%2fhow_to_design_a_reliable_distributed_timer%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How To Design A Reliable Distributed Timer on telegram" href="https://telegram.me/share/url?text=How%20To%20Design%20A%20Reliable%20Distributed%20Timer&url=https%3a%2f%2framsayleung.github.io%2fen%2fpost%2f2021%2fhow_to_design_a_reliable_distributed_timer%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How To Design A Reliable Distributed Timer on ycombinator" href="https://news.ycombinator.com/submitlink?t=How%20To%20Design%20A%20Reliable%20Distributed%20Timer&u=https%3a%2f%2framsayleung.github.io%2fen%2fpost%2f2021%2fhow_to_design_a_reliable_distributed_timer%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=ramsayleung/comment issue-term=title theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>See this site&rsquo;s source code <a href=https://github.com/ramsayleung/ramsayleung.github.io>here</a>, licensed under GPLv3 ·</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>