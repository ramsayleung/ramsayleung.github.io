<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>从京东"窃取"150+万条数据 - In Pursuit of Hubris</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="Ramsay Leung"><meta name=description content="An spider to crawl jindong item and comments"><meta name=keywords content="Blog,Software,Enginering">
<meta name=generator content="Hugo 0.92.2 with theme even">
<link rel=canonical href=https://ramsayleung.github.io/post/2017/jd_spider/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.b5a744db6de49a86cadafb3b70f555ab443f83c307a483402259e94726b045ff.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="从京东&#34;窃取&#34;150+万条数据">
<meta property="og:description" content="An spider to crawl jindong item and comments">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ramsayleung.github.io/post/2017/jd_spider/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2017-06-21T00:00:00+08:00">
<meta property="article:modified_time" content="2022-02-24T15:46:43+08:00">
<meta itemprop=name content="从京东&#34;窃取&#34;150+万条数据">
<meta itemprop=description content="An spider to crawl jindong item and comments"><meta itemprop=datePublished content="2017-06-21T00:00:00+08:00">
<meta itemprop=dateModified content="2022-02-24T15:46:43+08:00">
<meta itemprop=wordCount content="2221">
<meta itemprop=keywords content="python,crawler,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="从京东&#34;窃取&#34;150+万条数据">
<meta name=twitter:description content="An spider to crawl jindong item and comments"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>In Pursuit of Hubris</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/about_me>
<li class=mobile-menu-item>About</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>In Pursuit of Hubris</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/about_me>About</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>从京东"窃取"150+万条数据</h1>
<div class=post-meta>
<span class=post-time> 2017-06-21 </span>
<div class=post-category>
<a href=/categories/python/> python </a>
</div>
<span id=busuanzi_container_page_pv class=more-meta> <span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> times read </span>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li><a href=#爬取策略><span class=section-num>1</span> 爬取策略</a></li>
<li><a href=#提取数据><span class=section-num>2</span> 提取数据</a></li>
<li><a href=#商品评论><span class=section-num>3</span> 商品评论</a></li>
<li><a href=#反爬虫策略><span class=section-num>4</span> 反爬虫策略</a>
<ul>
<li><a href=#禁用-cookie><span class=section-num>4.1</span> 禁用 cookie</a></li>
<li><a href=#轮转-user-agent><span class=section-num>4.2</span> 轮转 user-agent</a></li>
<li><a href=#伪装成搜索引擎><span class=section-num>4.3</span> 伪装成搜索引擎</a></li>
<li><a href=#代理-ip><span class=section-num>4.4</span> 代理 IP</a></li>
</ul>
</li>
<li><a href=#扩展成分布式爬虫><span class=section-num>5</span> 扩展成分布式爬虫</a></li>
<li><a href=#爬虫监控><span class=section-num>6</span> 爬虫监控</a>
<ul>
<li><a href=#scrapy-graphite><span class=section-num>6.1</span> scrapy-graphite</a></li>
<li><a href=#docker><span class=section-num>6.2</span> docker</a></li>
</ul>
</li>
<li><a href=#爬虫拆分><span class=section-num>7</span> 爬虫拆分</a></li>
<li><a href=#小结><span class=section-num>8</span> 小结</a>
<ul>
<li><a href=#评论><span class=section-num>8.1</span> 评论</a></li>
<li><a href=#评论总结><span class=section-num>8.2</span> 评论总结</a></li>
<li><a href=#商品信息><span class=section-num>8.3</span> 商品信息</a></li>
</ul>
</li>
<li><a href=#参考及致谢><span class=section-num>9</span> 参考及致谢</a></li>
<li><a href=#项目源码><span class=section-num>10</span> 项目源码</a></li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<p>我最近编写了两只京东商品和评论的分布式爬虫来进行数据分析，现在就来分享一下。</p>
<h2 id=爬取策略><span class=section-num>1</span> 爬取策略</h2>
<p>众所周知，爬虫比较难爬取的就是动态生成的网页，因为需要解析 JS, 其中比较典型的例子就是淘宝，天猫，京东，QQ 空间等。</p>
<p>所以在我爬取京东网站的时候，首先需要确定的就是爬取策略。因为我想要爬取的是商品的信息以及相应的评论，并没有爬取特定的商品的需求。所以在分析京东的网页的 url 的时候, 决定使用类似全站爬取的策略。 分析如图：</p>
<figure><a href=/ox-hugo/jd_analyze.png><img src=/ox-hugo/jd_analyze.png></a>
</figure>
<p>可以看出，京东不同的商品类别是对应不同的子域名的，例如 <code>book</code> 对应的是图书， <code>mvd</code> 对应的是音像， <code>shouji</code> 对应的是手机等。</p>
<p>因为我使用的是获取 <code>&lt;a href></code> 标签里面的 url 值，然后迭代爬取的策略。所以要把爬取的 url 限定在域名为<code>jd.com</code> 范围内，不然就有可能会出现无限广度。</p>
<p>此外，有相当多的页面是不会包含商品信息的；例如： <code>help.jd.com</code>, <code>doc.jd.com</code> 等，因此使用 <code>jd.com</code> 这个域名范围实在太大了，所以把所需的子域名都添加到一个 list :</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>jd_subdomain</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;jiadian&#34;</span><span class=p>,</span> <span class=s2>&#34;shouji&#34;</span><span class=p>,</span> <span class=s2>&#34;wt&#34;</span><span class=p>,</span> <span class=s2>&#34;shuma&#34;</span><span class=p>,</span> <span class=s2>&#34;diannao&#34;</span><span class=p>,</span>
		<span class=s2>&#34;bg&#34;</span><span class=p>,</span> <span class=s2>&#34;channel&#34;</span><span class=p>,</span> <span class=s2>&#34;jipiao&#34;</span><span class=p>,</span> <span class=s2>&#34;hotel&#34;</span><span class=p>,</span> <span class=s2>&#34;trip&#34;</span><span class=p>,</span>
		<span class=s2>&#34;ish&#34;</span><span class=p>,</span> <span class=s2>&#34;book&#34;</span><span class=p>,</span> <span class=s2>&#34;e&#34;</span><span class=p>,</span> <span class=s2>&#34;health&#34;</span><span class=p>,</span> <span class=s2>&#34;baby&#34;</span><span class=p>,</span> <span class=s2>&#34;toy&#34;</span><span class=p>,</span>
		<span class=s2>&#34;nong&#34;</span><span class=p>,</span> <span class=s2>&#34;jiu&#34;</span><span class=p>,</span> <span class=s2>&#34;fresh&#34;</span><span class=p>,</span> <span class=s2>&#34;china&#34;</span><span class=p>,</span> <span class=s2>&#34;che&#34;</span><span class=p>,</span> <span class=s2>&#34;list&#34;</span><span class=p>]</span>
</code></pre></td></tr></table>
</div>
</div><h2 id=提取数据><span class=section-num>2</span> 提取数据</h2>
<p>在确定了爬取策略之后，爬虫就可以不断地进行工作了。那么爬虫怎么知道什么时候才是商品信息的页面呢？再来分析一下京东的商品页面：</p>
<figure><a href=/ox-hugo/jd_item_analyze.png><img src=/ox-hugo/jd_item_analyze.png></a>
</figure>
<p>从上面的信息可以看出，每个商品的页面都是以 <code>item.jd.com/xxxxxxx.html</code> 的形式存 在的；而 xxxxxxx 就是该商品的 sku-id. 所以只需对 url 进行解析，子域名为 <code>item</code> 即商品页面，就可以进行爬取。</p>
<p>页面提取使用 Xpath 即可，也无需赘言。不过，需要注 意的是对商品而言，非常重要的价格就不是可以通过爬取 HTML 页面得到的。</p>
<p>因为价格是经常变动的，所以是异步向后台请求的。对于这些异步请求的数据，打开控制台，然后刷新，就可以看到一堆的 JS 文件，然后寻找相应的请求带有 &ldquo;money 或者price&rdquo; 之类关 键字的 JS 文件，应该就能找到。</p>
<p>如果还没办法找出来的话，Firefox 上有一个 <a href=https://addons.mozilla.org/en-US/firefox/addon/user-agent-switcher/>user-agent-switcher</a> 的扩展，然后通过这个扩展把自己的浏览器伪装成 IE6, 相信所有
花俏的 JS 都会没了, 只剩下那些不可或缺的 JS, 这样结果应该一目了然了，这么看来 IE6 还是有用滴。最终找到的URL 如下
<code>https://p.3.cn/prices/mgets?callback=jQuery6646724&type=1&area=19_1601_3633_0.137875165&pdtk=9D4RIAHY317A3bZnQNapD7ip5Dg%252F6NXiIXt90Ahk0if2Yyh39PZQCuDBlhN%252FxOch3MpwWpHICu4P%250AVcgcOm11GQ%253D%253D&pduid=14966417675252009727775&pdpin=%25E5%2585%2591%25E9%2587%2591%25E8%25BE%25B0%25E6%2589%258B&pdbp=0&skuIds=J_3356012&ext=10000000&source=item-pc</code></p>
<p>不得不说，URL 实在是太长了。</p>
<p>根据经验，大部分的参数应该都是没什么用的，应该可以去掉的，所以在浏览器就一个个参数去掉，然后试试请求是否成功，如果成功，说明此参数无关重要，最后简化成： <code>http://p.3.cn/prices/mgets?pduid={}&skuIds=J_{}</code> sku_id 即商品页面的 URL中包含的数字，而 pduid 则是一随机整数而已，用
<code>random.randint(1, 100000000)</code> 函数解决。</p>
<h2 id=商品评论><span class=section-num>3</span> 商品评论</h2>
<p>商品的评论也是以 sku-id 为参数通过异步的方式进行请求的，构造请求的方法跟价格类 似，也不需过多赘述。</p>
<p>只是想要吐嘈一下的是，京东的评论是只能一页页向后翻的，不能跳转。还有一点就是，即使某样商品有 10+w 条评论，最多也只是返回 100页的数据。
略坑</p>
<h2 id=反爬虫策略><span class=section-num>4</span> 反爬虫策略</h2>
<p>商品的爬取策略以及提取策略都确定了，一只爬虫就基本成型了。但是一般比较大型的网站都有反爬虫措施的。所以道高一尺，魔高一丈，爬虫也要有对应的反反爬虫策略</p>
<h3 id=禁用-cookie><span class=section-num>4.1</span> 禁用 cookie</h3>
<p>通过禁用 cookie, 服务器就无法根据 cookie 判断出爬虫是否访问过网站</p>
<h3 id=轮转-user-agent><span class=section-num>4.2</span> 轮转 user-agent</h3>
<p>一般的爬虫都会使用浏览器的 user-agent 来模拟浏览器以欺骗服务器 (当然，如果你是一只什么 user-agent都不用耿直的小爬虫，我也无话可说).</p>
<p>为了提高突破反爬虫策略的成功率，可以定义多个 user-agent, 然后每次请求都随机选择 user-agent。</p>
<h3 id=伪装成搜索引擎><span class=section-num>4.3</span> 伪装成搜索引擎</h3>
<p>要说最著名的爬虫是谁？肯定是搜索引擎，它本质上也是爬虫，而且是非常强大的爬虫。</p>
<p>而且这些爬虫可以光明正大地去爬取各式网站，相信各式网站也很乐意被它爬。</p>
<p>那么， 现在可以通过修改 user-agent 伪装成搜索引擎，然后再结合上面的轮转 user-agent,</p>
<p>伪装成各式搜索引擎：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback>&#39;Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)&#39;,
&#39;Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)&#39;,
&#39;Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)&#39;,
&#39;DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)&#39;,
&#39;Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)&#39;,
&#39;Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)&#39;,
&#39;ia_archiver (+http://www.alexa.com/site/help/webmasters; crawler@alexa.com)&#39;,
</code></pre></td></tr></table>
</div>
</div><h3 id=代理-ip><span class=section-num>4.4</span> 代理 IP</h3>
<p>虽说可以伪装成搜索引擎，但是因为 http 请求是建立在三次握手之上的，爬虫的 IP 还是会被记录下来的，如果同一个 IP 访问得太频繁，那基本就可以确定是一只爬虫了，然后就把它的 IP 封掉，温和一点的就会叫你输入验证码，不然就返回 403.</p>
<p>对待这种情况，就需要使用代理 IP 了。</p>
<p>只是代理 IP 都有不同程度的延迟，并且免费的 IP 大多不能用，所以这是不得而为之了</p>
<h2 id=扩展成分布式爬虫><span class=section-num>5</span> 扩展成分布式爬虫</h2>
<p>一台机器的爬虫可能爬取一个网站可能需要 100 天，而且带宽也到达瓶颈了，那么是否可以提高爬取效率呢？</p>
<p>那就用 100台机器，1天应该就能爬取完 (当然，现实并非如此美好).</p>
<p>这个就涉及到分布式的爬虫的问题。而不同的分布式爬虫有不同的实现方法，而我选择了 scrapy 和 redis 整合的 <a href=https://github.com/rolando/scrapy-redis>scrapy-redis</a> 来实现分布式，URL 的去重以及调度都有了相应的实现了，也无需额外的操心</p>
<h2 id=爬虫监控><span class=section-num>6</span> 爬虫监控</h2>
<p>既然爬虫从单机变成了分布式，新的问题随之而来：如何监控分布式爬虫呢？在单机的时候，最简单的监控 &ndash; 直接将爬虫的日志信息输出到终端即可。</p>
<p>但是对于分布式爬虫，这样的做法显然不现实。我最终选择使用 <a href=https://graphiteapp.org>graphite</a> 这个监控工具。</p>
<h3 id=scrapy-graphite><span class=section-num>6.1</span> scrapy-graphite</h3>
<p>参考 Github上 <a href=https://github.com/gnemoug/distribute_crawler>distributed_crawler</a> 的代码，将单机版本的 <a href=https://github.com/noplay/scrapy-graphite>scrapy-graphite</a> 扩展成基于分布式的 graphite 监控程序，并且实现对 python3 的支持。</p>
<h3 id=docker><span class=section-num>6.2</span> docker</h3>
<p>但是 graphite 只是支持 python2, 并且安装过程很麻烦，我在折腾大半天后都无法安装成功，实在有点沮丧。最后想起了伟大的 docker, 并且直接找到已经打包好的image. 数行命令即解决所有的安装问题，不得不说：docker, 你值得拥有。运行截图：</p>
<figure><a href=/ox-hugo/jd_comment_graphite1.png><img src=/ox-hugo/jd_comment_graphite1.png></a>
</figure>
<figure><a href=/ox-hugo/jd_comment_graphite2.png><img src=/ox-hugo/jd_comment_graphite2.png></a>
</figure>
<h2 id=爬虫拆分><span class=section-num>7</span> 爬虫拆分</h2>
<p>本来爬取商品信息的爬虫和爬取评论的爬虫都是同一只爬虫，但是后来发现，再不使用代理 IP 的情况下，爬取到 150000 条商品信息的时候，需要输入验证码。</p>
<p>但是爬取商品评论的爬虫并不存在被反爬策略限制的情况。所以我将爬虫拆分成两只爬虫，即使无法爬取商品信息的时候，还可以爬取商品的评论信息。</p>
<h2 id=小结><span class=section-num>8</span> 小结</h2>
<p>在爬取一天之后，爬虫成果：</p>
<h3 id=评论><span class=section-num>8.1</span> 评论</h3>
<figure><a href=/ox-hugo/jd_comment.png><img src=/ox-hugo/jd_comment.png></a>
</figure>
<h3 id=评论总结><span class=section-num>8.2</span> 评论总结</h3>
<figure><a href=/ox-hugo/jd_comment_summary.png><img src=/ox-hugo/jd_comment_summary.png></a>
</figure>
<h3 id=商品信息><span class=section-num>8.3</span> 商品信息</h3>
<figure><a href=/ox-hugo/jd_parameters.png><img src=/ox-hugo/jd_parameters.png></a>
</figure>
<p>商品信息加上评论数约 150+w.</p>
<h2 id=参考及致谢><span class=section-num>9</span> 参考及致谢</h2>
<ul>
<li><a href=https://github.com/noplay/scrapy-graphite>https://github.com/noplay/scrapy-graphite</a></li>
<li><a href=https://github.com/gnemoug/distribute_crawler>https://github.com/gnemoug/distribute_crawler</a></li>
<li><a href=https://github.com/hopsoft/docker-graphite-statsd>https://github.com/hopsoft/docker-graphite-statsd</a></li>
</ul>
<h2 id=项目源码><span class=section-num>10</span> 项目源码</h2>
<p><a href=https://github.com/samrayleung/jd_spider>https://github.com/samrayleung/jd_spider</a></p>
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>Ramsay Leung</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
2022-02-24
</span>
</p>
</div>
<footer class=post-footer>
<div class=post-tags>
<a href=/tags/python/>python</a>
<a href=/tags/crawler/>crawler</a>
</div>
<nav class=post-nav>
<a class=prev href=/post/2017/python_with_sqlite3/>
<i class="iconfont icon-left"></i>
<span class="prev-text nav-default">用python 来操控 sqlite3</span>
<span class="prev-text nav-mobile">Prev</span>
</a>
<a class=next href=/post/2017/tweak_eshell_prompt/>
<span class="next-text nav-default">Eshell提示符优化</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i>
</a>
</nav>
</footer>
</article>
</div>
<div id=gitalk-container></div>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css crossorigin=anonymous>
<script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js crossorigin=anonymous></script>
<script type=text/javascript>var gitalk=new Gitalk({id:'2017-06-21 00:00:00 \u002b0800 \u002b0800',title:'从京东\u0022窃取\u0022150\u002b万条数据',clientID:'3c034c97f0926fafd2d6',clientSecret:'192051927d267ce83eb2ef10955890b7db2720ad',repo:'comment',owner:'ramsayleung',admin:['ramsayleung'],body:decodeURI(location.href)});gitalk.render('gitalk-container')</script>
<noscript>Please enable JavaScript to view the <a href=https://github.com/gitalk/gitalk>comments powered by gitalk.</a></noscript>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:ramsayleung@email.com class="iconfont icon-email" title=email></a>
<a href=https://stackoverflow.com/users/5738112/ramsay class="iconfont icon-stack-overflow" title=stack-overflow></a>
<a href=http://localhost:1313 class="iconfont icon-twitter" title=twitter></a>
<a href=https://github.com/ramsayleung class="iconfont icon-github" title=github></a>
<a href=https://ramsayleung.github.io/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<div class=busuanzi-footer>
<span id=busuanzi_container_site_pv> site pv: <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv> site uv: <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> </span>
</div>
<span class=copyright-year>
&copy;
2017 -
2022<span class=heart><i class="iconfont icon-heart"></i></span><span>Ramsay Leung</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
</body>
</html>